{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regularization and Optimization of Neural Networks - Lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Recall from the last lab that we had a training accuracy close to 90% and a test set accuracy close to 76%.\n",
    "\n",
    "As with our previous machine learning work, we should be asking a couple of questions:\n",
    "- Is there a high bias? yes/no\n",
    "- Is there a high variance? yes/no\n",
    "\n",
    "Also recall that \"high bias\" is a relative concept. Knowing we have 7 classes and the topics are related, we'll assume that a 90% accuracy is pretty good and the bias on the training set is low. (We've also discussed concepts like precision, recall as well as AUC and ROC curves.)   \n",
    "\n",
    "In this lab, we'll use the notion of training/validation/test set to get better insights of how we can mitigate our variance, and we'll look at a few regularization techniques. You'll start by repeating the process from the last section: importing the data and performing preprocessing including one-hot encoding. Then, just before you go on to train the model, we'll introduce how to include a validation set. You'll then define and compile the model as before. This time, when you are presented with the `history` dictionary of the model, you will have additional data entries for not only the train and test, but the train, test and validation  and then defigning, compiling and training the model. \n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "You will be able to:\n",
    "\n",
    "* Construct and run a basic model in Keras\n",
    "* Construct a validation set and explain potential benefits\n",
    "* Apply L1 and L2 regularization\n",
    "* Aplly dropout regularization\n",
    "* Observe and comment on the effect of using more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the libraries\n",
    "\n",
    "As usual, start by importing some of the packages and modules that you intend to use. The first thing we'll be doing is importing the data and taking a random sample, so that should clue you in to what tools to import. If you need more tools down the line, you can always import additional packages later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#Your code here; import some packages/modules you plan to use\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Data\n",
    "\n",
    "As with the previous lab, the data is stored in a file **Bank_complaints.csv**. Load and preview the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; load and preview the dataset\n",
    "df = pd.read_csv('Bank_complaints.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>In XX/XX/XXXX I filled out the Fedlaon applica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I am being contacted by a debt collector for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>I cosigned XXXX student loans at SallieMae for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Navient has sytematically and illegally failed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My wife became eligible for XXXX Loan Forgiven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>Hello, I am a XXXX resident who has multiple X...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My account was sold to them and expect me to p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>On XX/XX/XXXX, I was sued in XXXX County, XXXX...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>To Whom It May Concern, I applied for a privat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Student loan</td>\n",
       "      <td>My loans through XXXX were deferred as I am a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Product                       Consumer complaint narrative\n",
       "0  Student loan  In XX/XX/XXXX I filled out the Fedlaon applica...\n",
       "1  Student loan  I am being contacted by a debt collector for p...\n",
       "2  Student loan  I cosigned XXXX student loans at SallieMae for...\n",
       "3  Student loan  Navient has sytematically and illegally failed...\n",
       "4  Student loan  My wife became eligible for XXXX Loan Forgiven...\n",
       "5  Student loan  Hello, I am a XXXX resident who has multiple X...\n",
       "6  Student loan  My account was sold to them and expect me to p...\n",
       "7  Student loan  On XX/XX/XXXX, I was sued in XXXX County, XXXX...\n",
       "8  Student loan  To Whom It May Concern, I applied for a privat...\n",
       "9  Student loan  My loans through XXXX were deferred as I am a ..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Overview\n",
    "\n",
    "Before we begin to practice some of our new tools regarding regularization and optimization, let's practice munging some data as we did in the previous section with bank complaints. Recall some techniques:\n",
    "\n",
    "* Sampling in order to reduce training time (investigate model accuracy vs data size later on)\n",
    "* One-hot encoding our complaint text\n",
    "* Transforming our category labels\n",
    "* Train - test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Generate a Random Sample\n",
    "\n",
    "Since we have quite a bit of data and training networks takes a substantial amount of time and resources, we will downsample in order to test our initial pipeline. Going forward, these can be interesting areas of investigation: how does our models performance change as we increase (or decrease) the size of our dataset?  \n",
    "\n",
    "Generate the random sample using seed 123 for consistency of results. Make your new sample have 10,000 observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "df = df.sample(10000)\n",
    "df.index = range(10000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: One-hot Encoding of the Complaints\n",
    "\n",
    "As before, we need to do some preprocessing and data manipulationg before building the neural network. Last time, we guided you through the process, and now its time for you to practice that pipeline independently.  \n",
    "\n",
    "Only keep 2,000 most common words and use one-hot encoding to reformat the complaints into a matrix of vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here; use one-hot encoding to reformat the complaints into a matrix of vectors.\n",
    "#Only keep the 2000 most common words.\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_results[10].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'xxxx': 2,\n",
       " 'i': 3,\n",
       " 'to': 4,\n",
       " 'and': 5,\n",
       " 'my': 6,\n",
       " 'a': 7,\n",
       " 'that': 8,\n",
       " 'of': 9,\n",
       " 'was': 10,\n",
       " 'in': 11,\n",
       " 'they': 12,\n",
       " 'on': 13,\n",
       " 'for': 14,\n",
       " 'have': 15,\n",
       " 'not': 16,\n",
       " 'me': 17,\n",
       " 'this': 18,\n",
       " 'is': 19,\n",
       " 'xx': 20,\n",
       " 'with': 21,\n",
       " 'it': 22,\n",
       " 'account': 23,\n",
       " '00': 24,\n",
       " 'credit': 25,\n",
       " 'from': 26,\n",
       " 'had': 27,\n",
       " 'as': 28,\n",
       " 'be': 29,\n",
       " 'loan': 30,\n",
       " 'bank': 31,\n",
       " 'payment': 32,\n",
       " 'would': 33,\n",
       " 'at': 34,\n",
       " 'them': 35,\n",
       " 'an': 36,\n",
       " 'been': 37,\n",
       " 'by': 38,\n",
       " 'but': 39,\n",
       " 'no': 40,\n",
       " 'we': 41,\n",
       " 'told': 42,\n",
       " 'are': 43,\n",
       " 'or': 44,\n",
       " 'did': 45,\n",
       " 'do': 46,\n",
       " 'has': 47,\n",
       " 'when': 48,\n",
       " 'were': 49,\n",
       " 'payments': 50,\n",
       " \"n't\": 51,\n",
       " 'all': 52,\n",
       " 'which': 53,\n",
       " 'their': 54,\n",
       " 'am': 55,\n",
       " 'so': 56,\n",
       " 'time': 57,\n",
       " 'card': 58,\n",
       " 'after': 59,\n",
       " 'received': 60,\n",
       " 'because': 61,\n",
       " 'called': 62,\n",
       " 'pay': 63,\n",
       " 'you': 64,\n",
       " 'if': 65,\n",
       " 'out': 66,\n",
       " 'any': 67,\n",
       " 'information': 68,\n",
       " 'can': 69,\n",
       " \"''\": 70,\n",
       " \"'s\": 71,\n",
       " 'back': 72,\n",
       " 'there': 73,\n",
       " 'will': 74,\n",
       " 'up': 75,\n",
       " 'get': 76,\n",
       " 'she': 77,\n",
       " 'could': 78,\n",
       " 'never': 79,\n",
       " 'money': 80,\n",
       " 'then': 81,\n",
       " 'made': 82,\n",
       " 'due': 83,\n",
       " 'about': 84,\n",
       " 'said': 85,\n",
       " 'over': 86,\n",
       " 'interest': 87,\n",
       " 'now': 88,\n",
       " 'our': 89,\n",
       " 'report': 90,\n",
       " 'paid': 91,\n",
       " 'amount': 92,\n",
       " 'what': 93,\n",
       " 'call': 94,\n",
       " 'loans': 95,\n",
       " 'he': 96,\n",
       " 'only': 97,\n",
       " 'check': 98,\n",
       " 'also': 99,\n",
       " 'company': 100,\n",
       " 'one': 101,\n",
       " 'balance': 102,\n",
       " 'sent': 103,\n",
       " 'since': 104,\n",
       " 'mortgage': 105,\n",
       " 'late': 106,\n",
       " 'being': 107,\n",
       " 'days': 108,\n",
       " 'month': 109,\n",
       " 'make': 110,\n",
       " 'asked': 111,\n",
       " 'letter': 112,\n",
       " 'years': 113,\n",
       " 'phone': 114,\n",
       " 'still': 115,\n",
       " 'should': 116,\n",
       " 'again': 117,\n",
       " 'even': 118,\n",
       " 'months': 119,\n",
       " 'off': 120,\n",
       " 'fees': 121,\n",
       " 'number': 122,\n",
       " 'into': 123,\n",
       " 'more': 124,\n",
       " 'who': 125,\n",
       " 'us': 126,\n",
       " 'help': 127,\n",
       " 'these': 128,\n",
       " 'service': 129,\n",
       " 'through': 130,\n",
       " 'customer': 131,\n",
       " 'just': 132,\n",
       " 'her': 133,\n",
       " 'other': 134,\n",
       " 'day': 135,\n",
       " 'new': 136,\n",
       " 'your': 137,\n",
       " 'chase': 138,\n",
       " 'how': 139,\n",
       " 'home': 140,\n",
       " 'why': 141,\n",
       " 'fee': 142,\n",
       " 'navient': 143,\n",
       " 'date': 144,\n",
       " 'wells': 145,\n",
       " 'fargo': 146,\n",
       " 'times': 147,\n",
       " 'accounts': 148,\n",
       " 'another': 149,\n",
       " 'than': 150,\n",
       " 'car': 151,\n",
       " 'first': 152,\n",
       " 'like': 153,\n",
       " 'student': 154,\n",
       " 'until': 155,\n",
       " 'contacted': 156,\n",
       " 'went': 157,\n",
       " 'know': 158,\n",
       " 'same': 159,\n",
       " 'funds': 160,\n",
       " 'debt': 161,\n",
       " 'complaint': 162,\n",
       " 'checking': 163,\n",
       " 'monthly': 164,\n",
       " 'stated': 165,\n",
       " 'issue': 166,\n",
       " 'year': 167,\n",
       " 'charge': 168,\n",
       " 'closed': 169,\n",
       " 'financial': 170,\n",
       " 'several': 171,\n",
       " 'without': 172,\n",
       " 'need': 173,\n",
       " 'want': 174,\n",
       " 'charged': 175,\n",
       " 'please': 176,\n",
       " 'take': 177,\n",
       " 'name': 178,\n",
       " 'able': 179,\n",
       " 'going': 180,\n",
       " 'however': 181,\n",
       " '2': 182,\n",
       " 'representative': 183,\n",
       " 'see': 184,\n",
       " 'before': 185,\n",
       " 'statement': 186,\n",
       " 'reporting': 187,\n",
       " 'request': 188,\n",
       " 'charges': 189,\n",
       " 'online': 190,\n",
       " 'paying': 191,\n",
       " 'business': 192,\n",
       " 'very': 193,\n",
       " 'work': 194,\n",
       " '2016': 195,\n",
       " 'fraud': 196,\n",
       " 'last': 197,\n",
       " 'receive': 198,\n",
       " 'email': 199,\n",
       " 'spoke': 200,\n",
       " 'america': 201,\n",
       " '3': 202,\n",
       " 'got': 203,\n",
       " 'go': 204,\n",
       " 'requested': 205,\n",
       " 'every': 206,\n",
       " 'well': 207,\n",
       " 'process': 208,\n",
       " 'full': 209,\n",
       " 'contact': 210,\n",
       " 'calls': 211,\n",
       " 'deposit': 212,\n",
       " 'some': 213,\n",
       " 'later': 214,\n",
       " 'send': 215,\n",
       " 'took': 216,\n",
       " 'put': 217,\n",
       " 'informed': 218,\n",
       " 'department': 219,\n",
       " 'way': 220,\n",
       " 'past': 221,\n",
       " 'applied': 222,\n",
       " 'address': 223,\n",
       " 'mail': 224,\n",
       " 'dispute': 225,\n",
       " 'rate': 226,\n",
       " 'each': 227,\n",
       " 'where': 228,\n",
       " 'income': 229,\n",
       " \"'m\": 230,\n",
       " 'nothing': 231,\n",
       " 'file': 232,\n",
       " 'tried': 233,\n",
       " 'branch': 234,\n",
       " 'trying': 235,\n",
       " 'case': 236,\n",
       " 'making': 237,\n",
       " 'does': 238,\n",
       " 'two': 239,\n",
       " 'claim': 240,\n",
       " 'provide': 241,\n",
       " 'under': 242,\n",
       " 'bill': 243,\n",
       " 'documents': 244,\n",
       " 'stating': 245,\n",
       " 'today': 246,\n",
       " 'done': 247,\n",
       " 'provided': 248,\n",
       " '2015': 249,\n",
       " 'give': 250,\n",
       " 'reported': 251,\n",
       " 'opened': 252,\n",
       " 'both': 253,\n",
       " 'used': 254,\n",
       " 'vehicle': 255,\n",
       " 'while': 256,\n",
       " 'insurance': 257,\n",
       " 'manager': 258,\n",
       " 'person': 259,\n",
       " \"'ve\": 260,\n",
       " 'filed': 261,\n",
       " 'find': 262,\n",
       " 'given': 263,\n",
       " 'anything': 264,\n",
       " 'his': 265,\n",
       " 'consumer': 266,\n",
       " 'within': 267,\n",
       " 'needed': 268,\n",
       " 'refused': 269,\n",
       " 'removed': 270,\n",
       " 'response': 271,\n",
       " 'someone': 272,\n",
       " 'yet': 273,\n",
       " 'federal': 274,\n",
       " 'different': 275,\n",
       " 'many': 276,\n",
       " 'current': 277,\n",
       " 'hold': 278,\n",
       " 'believe': 279,\n",
       " 'plan': 280,\n",
       " 'next': 281,\n",
       " 'during': 282,\n",
       " 'modification': 283,\n",
       " 'problem': 284,\n",
       " 'once': 285,\n",
       " '5': 286,\n",
       " 'already': 287,\n",
       " 'offer': 288,\n",
       " 'against': 289,\n",
       " 'feel': 290,\n",
       " 'matter': 291,\n",
       " 'him': 292,\n",
       " 'situation': 293,\n",
       " 'immediately': 294,\n",
       " 'advised': 295,\n",
       " 'notice': 296,\n",
       " 'close': 297,\n",
       " '30': 298,\n",
       " 'signed': 299,\n",
       " '1': 300,\n",
       " 'fraudulent': 301,\n",
       " 'keep': 302,\n",
       " 'application': 303,\n",
       " 'though': 304,\n",
       " 'order': 305,\n",
       " 'reason': 306,\n",
       " 'regarding': 307,\n",
       " 'found': 308,\n",
       " 'denied': 309,\n",
       " 'score': 310,\n",
       " 'property': 311,\n",
       " 'people': 312,\n",
       " 'open': 313,\n",
       " 'down': 314,\n",
       " 'use': 315,\n",
       " 'explained': 316,\n",
       " 'right': 317,\n",
       " 'ago': 318,\n",
       " 'original': 319,\n",
       " 'submitted': 320,\n",
       " 'copy': 321,\n",
       " 'transaction': 322,\n",
       " 'point': 323,\n",
       " 'fact': 324,\n",
       " 'much': 325,\n",
       " 'debit': 326,\n",
       " 'having': 327,\n",
       " 'transfer': 328,\n",
       " 'end': 329,\n",
       " 'overdraft': 330,\n",
       " 'correct': 331,\n",
       " 'collection': 332,\n",
       " 'per': 333,\n",
       " 'supervisor': 334,\n",
       " 'its': 335,\n",
       " 'such': 336,\n",
       " 'total': 337,\n",
       " 'taken': 338,\n",
       " 'repayment': 339,\n",
       " 'via': 340,\n",
       " '10': 341,\n",
       " 'statements': 342,\n",
       " 'program': 343,\n",
       " 'additional': 344,\n",
       " 'tell': 345,\n",
       " 'school': 346,\n",
       " 'speak': 347,\n",
       " 'checks': 348,\n",
       " 'services': 349,\n",
       " 'getting': 350,\n",
       " 'capital': 351,\n",
       " 'ca': 352,\n",
       " 'based': 353,\n",
       " 'law': 354,\n",
       " 'personal': 355,\n",
       " 'house': 356,\n",
       " 'state': 357,\n",
       " 'purchase': 358,\n",
       " 'attached': 359,\n",
       " 'remove': 360,\n",
       " 'line': 361,\n",
       " 'returned': 362,\n",
       " 'multiple': 363,\n",
       " 'set': 364,\n",
       " 'system': 365,\n",
       " 'job': 366,\n",
       " 'weeks': 367,\n",
       " 'started': 368,\n",
       " 'saying': 369,\n",
       " 'gave': 370,\n",
       " 'proof': 371,\n",
       " 'access': 372,\n",
       " 'office': 373,\n",
       " 'citibank': 374,\n",
       " 'error': 375,\n",
       " 'available': 376,\n",
       " 'few': 377,\n",
       " 'around': 378,\n",
       " 'week': 379,\n",
       " 'good': 380,\n",
       " 'ask': 381,\n",
       " 'owe': 382,\n",
       " 'unable': 383,\n",
       " 'say': 384,\n",
       " 'refund': 385,\n",
       " 'negative': 386,\n",
       " 'change': 387,\n",
       " 'continue': 388,\n",
       " 'may': 389,\n",
       " 'terms': 390,\n",
       " 'agreement': 391,\n",
       " 'finally': 392,\n",
       " 'status': 393,\n",
       " 'cash': 394,\n",
       " 'bankruptcy': 395,\n",
       " 'documentation': 396,\n",
       " 'security': 397,\n",
       " 'principal': 398,\n",
       " 'instead': 399,\n",
       " '4': 400,\n",
       " '15': 401,\n",
       " 'calling': 402,\n",
       " 'transactions': 403,\n",
       " 'citi': 404,\n",
       " 'return': 405,\n",
       " 'required': 406,\n",
       " 'forbearance': 407,\n",
       " 'website': 408,\n",
       " 'wanted': 409,\n",
       " 'escrow': 410,\n",
       " 'left': 411,\n",
       " 'transferred': 412,\n",
       " 'agreed': 413,\n",
       " 'rep': 414,\n",
       " 'using': 415,\n",
       " 'auto': 416,\n",
       " '2017': 417,\n",
       " 'history': 418,\n",
       " 'something': 419,\n",
       " 'understand': 420,\n",
       " 'thank': 421,\n",
       " 'happened': 422,\n",
       " 'contract': 423,\n",
       " 'stop': 424,\n",
       " 'legal': 425,\n",
       " 'showing': 426,\n",
       " 'wrong': 427,\n",
       " 'agency': 428,\n",
       " 'those': 429,\n",
       " 'owed': 430,\n",
       " 'recently': 431,\n",
       " 'lender': 432,\n",
       " 'dollars': 433,\n",
       " 'disputed': 434,\n",
       " 'asking': 435,\n",
       " 'equifax': 436,\n",
       " 'sold': 437,\n",
       " 'closing': 438,\n",
       " 'further': 439,\n",
       " 'investigation': 440,\n",
       " 'attorney': 441,\n",
       " 'sure': 442,\n",
       " 'ever': 443,\n",
       " 'period': 444,\n",
       " 'own': 445,\n",
       " 'form': 446,\n",
       " 'resolve': 447,\n",
       " 'came': 448,\n",
       " 'direct': 449,\n",
       " 'prior': 450,\n",
       " 'wife': 451,\n",
       " 'longer': 452,\n",
       " 'deposited': 453,\n",
       " 'following': 454,\n",
       " 'approved': 455,\n",
       " 'issues': 456,\n",
       " 'cards': 457,\n",
       " 'cfpb': 458,\n",
       " 'part': 459,\n",
       " 'sale': 460,\n",
       " 'servicing': 461,\n",
       " 'customers': 462,\n",
       " 'long': 463,\n",
       " 'hours': 464,\n",
       " 'foreclosure': 465,\n",
       " 'care': 466,\n",
       " 'reports': 467,\n",
       " 'three': 468,\n",
       " 'private': 469,\n",
       " 'nor': 470,\n",
       " 'upon': 471,\n",
       " 'tax': 472,\n",
       " 'must': 473,\n",
       " '6': 474,\n",
       " 'bureaus': 475,\n",
       " 'placed': 476,\n",
       " 'place': 477,\n",
       " 'working': 478,\n",
       " 'finance': 479,\n",
       " 'note': 480,\n",
       " 'paperwork': 481,\n",
       " 'doing': 482,\n",
       " 'court': 483,\n",
       " 'everything': 484,\n",
       " 'offered': 485,\n",
       " 'agent': 486,\n",
       " 'husband': 487,\n",
       " 'let': 488,\n",
       " 'old': 489,\n",
       " 'letters': 490,\n",
       " 're': 491,\n",
       " 'atm': 492,\n",
       " 'second': 493,\n",
       " 'apply': 494,\n",
       " 'changed': 495,\n",
       " 'review': 496,\n",
       " 'almost': 497,\n",
       " 'hard': 498,\n",
       " 'family': 499,\n",
       " 'come': 500,\n",
       " 'show': 501,\n",
       " 'purchased': 502,\n",
       " 'listed': 503,\n",
       " 'bonus': 504,\n",
       " 'always': 505,\n",
       " 'answer': 506,\n",
       " 'either': 507,\n",
       " 'bureau': 508,\n",
       " 'lower': 509,\n",
       " 'bills': 510,\n",
       " 'previous': 511,\n",
       " 'receiving': 512,\n",
       " 'complete': 513,\n",
       " 'try': 514,\n",
       " 'american': 515,\n",
       " 'savings': 516,\n",
       " 'states': 517,\n",
       " 'title': 518,\n",
       " 'option': 519,\n",
       " 'points': 520,\n",
       " 'banking': 521,\n",
       " 'afford': 522,\n",
       " 'record': 523,\n",
       " 'social': 524,\n",
       " 'dated': 525,\n",
       " 'writing': 526,\n",
       " 'lost': 527,\n",
       " 'identity': 528,\n",
       " 'clear': 529,\n",
       " 'practices': 530,\n",
       " 'telling': 531,\n",
       " 'myself': 532,\n",
       " 'numerous': 533,\n",
       " 'notified': 534,\n",
       " \"'\": 535,\n",
       " 'servicer': 536,\n",
       " 'too': 537,\n",
       " 'failed': 538,\n",
       " 'currently': 539,\n",
       " 'resolved': 540,\n",
       " 'continued': 541,\n",
       " 'taking': 542,\n",
       " 'policy': 543,\n",
       " 'verify': 544,\n",
       " 'actually': 545,\n",
       " 's': 546,\n",
       " 'high': 547,\n",
       " '7': 548,\n",
       " 'thought': 549,\n",
       " 'least': 550,\n",
       " 'assistance': 551,\n",
       " 'promotion': 552,\n",
       " 'most': 553,\n",
       " 'experian': 554,\n",
       " 'here': 555,\n",
       " 'including': 556,\n",
       " 'think': 557,\n",
       " 'claims': 558,\n",
       " 'written': 559,\n",
       " 'options': 560,\n",
       " 'mailed': 561,\n",
       " 'look': 562,\n",
       " 'sign': 563,\n",
       " 'action': 564,\n",
       " 'thing': 565,\n",
       " 'according': 566,\n",
       " 'ocwen': 567,\n",
       " 'protection': 568,\n",
       " 'allow': 569,\n",
       " 'result': 570,\n",
       " 'enough': 571,\n",
       " 'completed': 572,\n",
       " 'police': 573,\n",
       " 'message': 574,\n",
       " 'incorrect': 575,\n",
       " '100': 576,\n",
       " 'companies': 577,\n",
       " 'explain': 578,\n",
       " '0': 579,\n",
       " 'shows': 580,\n",
       " 'e': 581,\n",
       " 'banks': 582,\n",
       " 'agencies': 583,\n",
       " 'less': 584,\n",
       " 'possible': 585,\n",
       " 'clearly': 586,\n",
       " 'act': 587,\n",
       " 'mistake': 588,\n",
       " 'attempted': 589,\n",
       " 'co': 590,\n",
       " 'allowed': 591,\n",
       " 'life': 592,\n",
       " 'wait': 593,\n",
       " 'away': 594,\n",
       " 't': 595,\n",
       " 'minimum': 596,\n",
       " 'seems': 597,\n",
       " 'caused': 598,\n",
       " 'document': 599,\n",
       " 'pnc': 600,\n",
       " 'p': 601,\n",
       " 'taxes': 602,\n",
       " 'noticed': 603,\n",
       " 'fair': 604,\n",
       " 'says': 605,\n",
       " 'confirmed': 606,\n",
       " 'really': 607,\n",
       " 'aware': 608,\n",
       " 'requesting': 609,\n",
       " 'despite': 610,\n",
       " 'college': 611,\n",
       " 'records': 612,\n",
       " 'inquiry': 613,\n",
       " 'issued': 614,\n",
       " 'deferment': 615,\n",
       " 'refuse': 616,\n",
       " 'decided': 617,\n",
       " 'id': 618,\n",
       " 'theft': 619,\n",
       " 'added': 620,\n",
       " 'collections': 621,\n",
       " '16': 622,\n",
       " 'express': 623,\n",
       " 'payoff': 624,\n",
       " 'kept': 625,\n",
       " 'extra': 626,\n",
       " 'behind': 627,\n",
       " 'default': 628,\n",
       " 'processed': 629,\n",
       " 'release': 630,\n",
       " 'questions': 631,\n",
       " 'sending': 632,\n",
       " 'began': 633,\n",
       " 'posted': 634,\n",
       " 'ally': 635,\n",
       " 'boa': 636,\n",
       " 'items': 637,\n",
       " 'checked': 638,\n",
       " 'billing': 639,\n",
       " 'best': 640,\n",
       " 'numbers': 641,\n",
       " 'cover': 642,\n",
       " 'supposed': 643,\n",
       " 'explanation': 644,\n",
       " 'short': 645,\n",
       " 'requirements': 646,\n",
       " 'education': 647,\n",
       " 'authorized': 648,\n",
       " 'start': 649,\n",
       " 'else': 650,\n",
       " 'between': 651,\n",
       " 'included': 652,\n",
       " 'mother': 653,\n",
       " 'etc': 654,\n",
       " 'representatives': 655,\n",
       " 'therefore': 656,\n",
       " 'promised': 657,\n",
       " 'third': 658,\n",
       " 'false': 659,\n",
       " '12': 660,\n",
       " 'minutes': 661,\n",
       " 'delinquent': 662,\n",
       " 'missed': 663,\n",
       " 'filing': 664,\n",
       " 'anyone': 665,\n",
       " 'illegal': 666,\n",
       " 'nationstar': 667,\n",
       " 'free': 668,\n",
       " 'needs': 669,\n",
       " 'responsible': 670,\n",
       " '20': 671,\n",
       " 'charging': 672,\n",
       " 'cost': 673,\n",
       " 'verified': 674,\n",
       " 'item': 675,\n",
       " 'refinance': 676,\n",
       " 'activity': 677,\n",
       " 'local': 678,\n",
       " 'directly': 679,\n",
       " 'above': 680,\n",
       " 'plus': 681,\n",
       " '1000': 682,\n",
       " 'talk': 683,\n",
       " 'info': 684,\n",
       " 'towards': 685,\n",
       " 'settlement': 686,\n",
       " 'accept': 687,\n",
       " '60': 688,\n",
       " 'increase': 689,\n",
       " 'miles': 690,\n",
       " 'twice': 691,\n",
       " 'lease': 692,\n",
       " 'fax': 693,\n",
       " 'signature': 694,\n",
       " 'simply': 695,\n",
       " 'violation': 696,\n",
       " 'completely': 697,\n",
       " 'usaa': 698,\n",
       " 'party': 699,\n",
       " 'synchrony': 700,\n",
       " 'early': 701,\n",
       " 'showed': 702,\n",
       " 'entire': 703,\n",
       " 'unauthorized': 704,\n",
       " '25': 705,\n",
       " 'higher': 706,\n",
       " 'merchant': 707,\n",
       " '90': 708,\n",
       " 'addition': 709,\n",
       " 'qualify': 710,\n",
       " 'credited': 711,\n",
       " 'mine': 712,\n",
       " 'repeatedly': 713,\n",
       " 'met': 714,\n",
       " 'along': 715,\n",
       " 'knew': 716,\n",
       " 'discover': 717,\n",
       " 'looking': 718,\n",
       " 'resolution': 719,\n",
       " 'hardship': 720,\n",
       " '8': 721,\n",
       " 'problems': 722,\n",
       " 'bad': 723,\n",
       " 'fixed': 724,\n",
       " 'td': 725,\n",
       " 'submit': 726,\n",
       " 'although': 727,\n",
       " 'non': 728,\n",
       " 'son': 729,\n",
       " 'live': 730,\n",
       " 'inaccurate': 731,\n",
       " 'receipt': 732,\n",
       " 'buy': 733,\n",
       " 'heard': 734,\n",
       " 'attempt': 735,\n",
       " 'unfair': 736,\n",
       " 'fix': 737,\n",
       " 'follow': 738,\n",
       " 'pending': 739,\n",
       " 'soon': 740,\n",
       " 'inquiries': 741,\n",
       " 'consumers': 742,\n",
       " 'decision': 743,\n",
       " 'held': 744,\n",
       " 'things': 745,\n",
       " 'rates': 746,\n",
       " 'loss': 747,\n",
       " 'gone': 748,\n",
       " 'trust': 749,\n",
       " 'purchases': 750,\n",
       " 'initial': 751,\n",
       " 'opening': 752,\n",
       " 'complaints': 753,\n",
       " 'question': 754,\n",
       " 'creditor': 755,\n",
       " 'waiting': 756,\n",
       " 'amounts': 757,\n",
       " 'deal': 758,\n",
       " 'talked': 759,\n",
       " 'union': 760,\n",
       " 'practice': 761,\n",
       " 'paypal': 762,\n",
       " 'type': 763,\n",
       " 'emails': 764,\n",
       " 'approximately': 765,\n",
       " 'limit': 766,\n",
       " 'saw': 767,\n",
       " '500': 768,\n",
       " 'cleared': 769,\n",
       " 'update': 770,\n",
       " 'deposits': 771,\n",
       " 'investigate': 772,\n",
       " '200': 773,\n",
       " 'knowledge': 774,\n",
       " 'automatic': 775,\n",
       " 'conversation': 776,\n",
       " 'cancel': 777,\n",
       " 'happen': 778,\n",
       " 'morning': 779,\n",
       " 'worked': 780,\n",
       " 'paper': 781,\n",
       " 'responded': 782,\n",
       " 'acct': 783,\n",
       " 'requests': 784,\n",
       " 'notification': 785,\n",
       " 'dept': 786,\n",
       " 'public': 787,\n",
       " 'fedloan': 788,\n",
       " 'remaining': 789,\n",
       " 'collect': 790,\n",
       " 'lien': 791,\n",
       " 'missing': 792,\n",
       " 'support': 793,\n",
       " 'employee': 794,\n",
       " 'dates': 795,\n",
       " 'value': 796,\n",
       " 'previously': 797,\n",
       " 'reached': 798,\n",
       " 'final': 799,\n",
       " 'principle': 800,\n",
       " 'communication': 801,\n",
       " 'updated': 802,\n",
       " 'far': 803,\n",
       " 'below': 804,\n",
       " 'stolen': 805,\n",
       " 'forward': 806,\n",
       " 'correspondence': 807,\n",
       " 'cancelled': 808,\n",
       " 'suntrust': 809,\n",
       " 'forgiveness': 810,\n",
       " 'dealership': 811,\n",
       " 'government': 812,\n",
       " 'giving': 813,\n",
       " 'post': 814,\n",
       " 'read': 815,\n",
       " 'single': 816,\n",
       " 'transunion': 817,\n",
       " 'couple': 818,\n",
       " 'forced': 819,\n",
       " 'obtain': 820,\n",
       " 'course': 821,\n",
       " 'corrected': 822,\n",
       " 'copies': 823,\n",
       " 'accepted': 824,\n",
       " 'assured': 825,\n",
       " 'eligible': 826,\n",
       " 'occasions': 827,\n",
       " 'run': 828,\n",
       " 'respond': 829,\n",
       " 'wrote': 830,\n",
       " 'rights': 831,\n",
       " 'santander': 832,\n",
       " 'c': 833,\n",
       " 'claimed': 834,\n",
       " 'research': 835,\n",
       " 'manner': 836,\n",
       " 'better': 837,\n",
       " 'processing': 838,\n",
       " 'little': 839,\n",
       " 'daughter': 840,\n",
       " 'certified': 841,\n",
       " 'reviewed': 842,\n",
       " 'real': 843,\n",
       " 'continues': 844,\n",
       " 'dealer': 845,\n",
       " 'lending': 846,\n",
       " 'annual': 847,\n",
       " 'fcra': 848,\n",
       " 'standing': 849,\n",
       " 'refunded': 850,\n",
       " 'became': 851,\n",
       " 'none': 852,\n",
       " 'spent': 853,\n",
       " 'cashed': 854,\n",
       " 'small': 855,\n",
       " 'confirmation': 856,\n",
       " 'u': 857,\n",
       " 'great': 858,\n",
       " 'causing': 859,\n",
       " 'looked': 860,\n",
       " 'whom': 861,\n",
       " 'recorded': 862,\n",
       " 'fault': 863,\n",
       " 'scheduled': 864,\n",
       " 'section': 865,\n",
       " 'confirm': 866,\n",
       " 'b': 867,\n",
       " 'code': 868,\n",
       " '9': 869,\n",
       " 'monday': 870,\n",
       " 'concern': 871,\n",
       " 'willing': 872,\n",
       " 'lack': 873,\n",
       " 'attempts': 874,\n",
       " 'recent': 875,\n",
       " 'appraisal': 876,\n",
       " 'father': 877,\n",
       " 'low': 878,\n",
       " 'increased': 879,\n",
       " 'bought': 880,\n",
       " 'ended': 881,\n",
       " 'timely': 882,\n",
       " 'verification': 883,\n",
       " \"'d\": 884,\n",
       " 'declined': 885,\n",
       " 'contacting': 886,\n",
       " 'lot': 887,\n",
       " 'indicated': 888,\n",
       " 'correctly': 889,\n",
       " 'approval': 890,\n",
       " 'rude': 891,\n",
       " 'borrower': 892,\n",
       " '2014': 893,\n",
       " 'deleted': 894,\n",
       " 'properly': 895,\n",
       " 'victim': 896,\n",
       " 'hour': 897,\n",
       " 'stopped': 898,\n",
       " 'ditech': 899,\n",
       " 'future': 900,\n",
       " 'advantage': 901,\n",
       " '300': 902,\n",
       " 'details': 903,\n",
       " 'passed': 904,\n",
       " 'move': 905,\n",
       " 'evidence': 906,\n",
       " 'beginning': 907,\n",
       " 'outstanding': 908,\n",
       " 'advance': 909,\n",
       " 'faxed': 910,\n",
       " 'speaking': 911,\n",
       " 'true': 912,\n",
       " 'means': 913,\n",
       " 'telephone': 914,\n",
       " 'refuses': 915,\n",
       " 'wo': 916,\n",
       " 'avoid': 917,\n",
       " 'actions': 918,\n",
       " '35': 919,\n",
       " 'prove': 920,\n",
       " 'reversed': 921,\n",
       " 'nelnet': 922,\n",
       " 'institution': 923,\n",
       " 'difficult': 924,\n",
       " 'separate': 925,\n",
       " 'reach': 926,\n",
       " 'makes': 927,\n",
       " 'unfortunately': 928,\n",
       " 'turned': 929,\n",
       " 'promotional': 930,\n",
       " 'unless': 931,\n",
       " 'cause': 932,\n",
       " 'lied': 933,\n",
       " 'idea': 934,\n",
       " 'extremely': 935,\n",
       " 'half': 936,\n",
       " 'kind': 937,\n",
       " 'conditions': 938,\n",
       " '120': 939,\n",
       " 'applying': 940,\n",
       " 'inform': 941,\n",
       " 'apparently': 942,\n",
       " 'moved': 943,\n",
       " 'whole': 944,\n",
       " 'aes': 945,\n",
       " 'occurred': 946,\n",
       " 'emailed': 947,\n",
       " 'discovered': 948,\n",
       " '24': 949,\n",
       " 'deed': 950,\n",
       " 'accurate': 951,\n",
       " 'regular': 952,\n",
       " 'responsibility': 953,\n",
       " 'reverse': 954,\n",
       " 'estate': 955,\n",
       " 'providing': 956,\n",
       " 'add': 957,\n",
       " 'appears': 958,\n",
       " 'named': 959,\n",
       " 'hit': 960,\n",
       " 'rather': 961,\n",
       " '17': 962,\n",
       " 'owner': 963,\n",
       " 'thus': 964,\n",
       " 'involved': 965,\n",
       " 'behalf': 966,\n",
       " 'friday': 967,\n",
       " 'seem': 968,\n",
       " 'living': 969,\n",
       " 'large': 970,\n",
       " 'originally': 971,\n",
       " '50': 972,\n",
       " 'attempting': 973,\n",
       " 'store': 974,\n",
       " 'teller': 975,\n",
       " 'denial': 976,\n",
       " 'daily': 977,\n",
       " 'worth': 978,\n",
       " 'assist': 979,\n",
       " 'data': 980,\n",
       " 'balances': 981,\n",
       " 'hung': 982,\n",
       " '400': 983,\n",
       " 'regards': 984,\n",
       " 'x': 985,\n",
       " 'banker': 986,\n",
       " 'coming': 987,\n",
       " 'chapter': 988,\n",
       " 'consolidated': 989,\n",
       " 'honor': 990,\n",
       " 'university': 991,\n",
       " 'valid': 992,\n",
       " 'yes': 993,\n",
       " 'changes': 994,\n",
       " 'pulled': 995,\n",
       " 'bring': 996,\n",
       " 'dealing': 997,\n",
       " 'leave': 998,\n",
       " 'employees': 999,\n",
       " 'necessary': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing: Encoding the Products\n",
    "\n",
    "Similarly, now transform the descriptive product labels to integers labels. After transforming them to integer labels, retransform them into a matrix of binary flags, one for each of the various product labels.  \n",
    "  \n",
    "  (Note: this is similar to our previous work with dummy variables: each of the various product categories will be its own column, and each observation will be a row. Each of these observation rows will have a 1 in the column associated with it's label, and all other entries for the row will be zero.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; transform the product labels to numerical values\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "\n",
    "#Your code here; transform the product labels to numerical values\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "product_cat = le.transform(product) \n",
    "\n",
    "#Then transform these integer values into a matrix of binary flags\n",
    "product_onehot = to_categorical(product_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train-test Split\n",
    "\n",
    "Now onto the ever familiar train-test split! Be sure to split both the complaint data (now transformed into word vectors) as well as their associated labels. Perform an appropriate train test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(one_hot_results, product_onehot, test_size=1500, random_state=42)\n",
    "\n",
    "\n",
    "#Your code here\n",
    "#X_train = \n",
    "##X_test = \n",
    "#y_train = \n",
    "#y_test = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the model using a validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Validation Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture we mentioned that in deep learning, we generally keep aside a validation set, which is used during hyperparameter tuning. Then when we have made the final model decision, the test set is used to define the final model perforance. \n",
    "\n",
    "In this example, let's take the first 1000 cases out of the training set to become the validation set. You should do this for both `train` and `label_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Just run this block of code \n",
    "random.seed(123)\n",
    "val = X_train[:1000]\n",
    "train_final = X_train[1000:]\n",
    "label_val = y_train[:1000]\n",
    "label_train_final = y_train[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a fully connected (Dense) layer network with relu activations in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that we used 2 hidden with 50 units in the first layer and 25 in the second, both with a `relu` activation function. Because we are dealing with a multiclass problem (classifying the complaints into 7 classes), we use a use a softmax classifyer in order to output 7 class probabilities per case.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here; build a neural network using Keras as described above.\n",
    "random.seed(123)\n",
    "from keras import models\n",
    "from keras import layers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling the Model\n",
    "In the compiler, you'll be passing the optimizer, loss function, and metrics. Train the model for 120 epochs in mini-batches of 256 samples. This time, let's include the argument `validation_data` and assign it `(val, label_val)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Code Along\n",
    "\n",
    "The remaining portion of this lab will introduce you to code snippets for a myriad of different methods discussed in the lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "Ok, now for the resource intensive part: time to train our model! Note that this is where we also introduce the validation data to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.9402 - acc: 0.1825 - val_loss: 1.9386 - val_acc: 0.1910\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.9150 - acc: 0.2163 - val_loss: 1.9204 - val_acc: 0.2130\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8984 - acc: 0.2345 - val_loss: 1.9045 - val_acc: 0.2270\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.8811 - acc: 0.2491 - val_loss: 1.8874 - val_acc: 0.2370\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8610 - acc: 0.2691 - val_loss: 1.8680 - val_acc: 0.2510\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.8363 - acc: 0.2872 - val_loss: 1.8437 - val_acc: 0.2750\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.8055 - acc: 0.3132 - val_loss: 1.8120 - val_acc: 0.3030\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.7679 - acc: 0.3395 - val_loss: 1.7732 - val_acc: 0.3360\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.7245 - acc: 0.3647 - val_loss: 1.7305 - val_acc: 0.3570\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.6764 - acc: 0.3908 - val_loss: 1.6840 - val_acc: 0.3790\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.6249 - acc: 0.4187 - val_loss: 1.6337 - val_acc: 0.4060\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5711 - acc: 0.4465 - val_loss: 1.5818 - val_acc: 0.4350\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.5166 - acc: 0.4776 - val_loss: 1.5287 - val_acc: 0.4690\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4624 - acc: 0.5091 - val_loss: 1.4780 - val_acc: 0.4980\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.4089 - acc: 0.5423 - val_loss: 1.4265 - val_acc: 0.5270\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3573 - acc: 0.5677 - val_loss: 1.3780 - val_acc: 0.5650\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3076 - acc: 0.5925 - val_loss: 1.3305 - val_acc: 0.5930\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.2603 - acc: 0.6101 - val_loss: 1.2852 - val_acc: 0.6310\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2148 - acc: 0.6333 - val_loss: 1.2421 - val_acc: 0.6400\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1722 - acc: 0.6491 - val_loss: 1.2026 - val_acc: 0.6500\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1319 - acc: 0.6572 - val_loss: 1.1629 - val_acc: 0.6630\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0938 - acc: 0.6747 - val_loss: 1.1305 - val_acc: 0.6690\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0581 - acc: 0.6820 - val_loss: 1.0941 - val_acc: 0.6800\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0242 - acc: 0.6899 - val_loss: 1.0661 - val_acc: 0.6850\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9931 - acc: 0.7023 - val_loss: 1.0374 - val_acc: 0.6880\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9639 - acc: 0.7056 - val_loss: 1.0093 - val_acc: 0.6960\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9364 - acc: 0.7133 - val_loss: 0.9856 - val_acc: 0.7010\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9108 - acc: 0.7185 - val_loss: 0.9571 - val_acc: 0.7060\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8871 - acc: 0.7251 - val_loss: 0.9367 - val_acc: 0.7140\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8651 - acc: 0.7303 - val_loss: 0.9184 - val_acc: 0.7140\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8439 - acc: 0.7361 - val_loss: 0.9004 - val_acc: 0.7150\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8254 - acc: 0.7380 - val_loss: 0.8826 - val_acc: 0.7240\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8067 - acc: 0.7455 - val_loss: 0.8660 - val_acc: 0.7280\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7898 - acc: 0.7473 - val_loss: 0.8538 - val_acc: 0.7280\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7738 - acc: 0.7501 - val_loss: 0.8389 - val_acc: 0.7300\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7590 - acc: 0.7567 - val_loss: 0.8269 - val_acc: 0.7330\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7451 - acc: 0.7592 - val_loss: 0.8155 - val_acc: 0.7320\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.7316 - acc: 0.7612 - val_loss: 0.8044 - val_acc: 0.7360\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7192 - acc: 0.7647 - val_loss: 0.7907 - val_acc: 0.7380\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.7076 - acc: 0.7681 - val_loss: 0.7821 - val_acc: 0.7420\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.6964 - acc: 0.7689 - val_loss: 0.7711 - val_acc: 0.7450\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.6853 - acc: 0.7732 - val_loss: 0.7658 - val_acc: 0.7460\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.6754 - acc: 0.7765 - val_loss: 0.7574 - val_acc: 0.7400\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.6659 - acc: 0.7771 - val_loss: 0.7471 - val_acc: 0.7490\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.6568 - acc: 0.7789 - val_loss: 0.7423 - val_acc: 0.7410\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.6479 - acc: 0.7823 - val_loss: 0.7362 - val_acc: 0.7500\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.6392 - acc: 0.7855 - val_loss: 0.7286 - val_acc: 0.7450\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.6314 - acc: 0.7853 - val_loss: 0.7301 - val_acc: 0.7440\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.6237 - acc: 0.7896 - val_loss: 0.7178 - val_acc: 0.7530\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.6164 - acc: 0.7917 - val_loss: 0.7137 - val_acc: 0.7530\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.6094 - acc: 0.7936 - val_loss: 0.7086 - val_acc: 0.7580\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.6022 - acc: 0.7947 - val_loss: 0.7033 - val_acc: 0.7520\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.5960 - acc: 0.7979 - val_loss: 0.7014 - val_acc: 0.7600\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.5898 - acc: 0.7980 - val_loss: 0.6918 - val_acc: 0.7590\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.5833 - acc: 0.7981 - val_loss: 0.6895 - val_acc: 0.7570\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.5774 - acc: 0.7997 - val_loss: 0.6865 - val_acc: 0.7630\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.5724 - acc: 0.8043 - val_loss: 0.6867 - val_acc: 0.7570\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.5659 - acc: 0.8061 - val_loss: 0.6806 - val_acc: 0.7610\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.5605 - acc: 0.8045 - val_loss: 0.6782 - val_acc: 0.7640\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.5553 - acc: 0.8083 - val_loss: 0.6737 - val_acc: 0.7630\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5500 - acc: 0.8108 - val_loss: 0.6712 - val_acc: 0.7610\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.5452 - acc: 0.8125 - val_loss: 0.6724 - val_acc: 0.7590\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.5401 - acc: 0.8121 - val_loss: 0.6723 - val_acc: 0.7590\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.5358 - acc: 0.8152 - val_loss: 0.6663 - val_acc: 0.7610\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.5302 - acc: 0.8159 - val_loss: 0.6602 - val_acc: 0.7640\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.5258 - acc: 0.8200 - val_loss: 0.6609 - val_acc: 0.7670\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.5214 - acc: 0.8187 - val_loss: 0.6557 - val_acc: 0.7680\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.5166 - acc: 0.8217 - val_loss: 0.6558 - val_acc: 0.7620\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.5124 - acc: 0.8208 - val_loss: 0.6526 - val_acc: 0.7670\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.5079 - acc: 0.8225 - val_loss: 0.6535 - val_acc: 0.7680\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.5040 - acc: 0.8253 - val_loss: 0.6565 - val_acc: 0.7620\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.5001 - acc: 0.8263 - val_loss: 0.6568 - val_acc: 0.7610\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.4959 - acc: 0.8293 - val_loss: 0.6481 - val_acc: 0.7680\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.4919 - acc: 0.8279 - val_loss: 0.6463 - val_acc: 0.7710\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4881 - acc: 0.8295 - val_loss: 0.6436 - val_acc: 0.7710\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.4840 - acc: 0.8311 - val_loss: 0.6408 - val_acc: 0.7670\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4805 - acc: 0.8325 - val_loss: 0.6408 - val_acc: 0.7660\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.4766 - acc: 0.8348 - val_loss: 0.6400 - val_acc: 0.7720\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.4724 - acc: 0.8359 - val_loss: 0.6381 - val_acc: 0.7680\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4690 - acc: 0.8376 - val_loss: 0.6428 - val_acc: 0.7680\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.4656 - acc: 0.8380 - val_loss: 0.6353 - val_acc: 0.7760\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4620 - acc: 0.8396 - val_loss: 0.6374 - val_acc: 0.7640\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.4585 - acc: 0.8411 - val_loss: 0.6387 - val_acc: 0.7740\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.4552 - acc: 0.8431 - val_loss: 0.6332 - val_acc: 0.7780\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.4514 - acc: 0.8444 - val_loss: 0.6350 - val_acc: 0.7710\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.4483 - acc: 0.8467 - val_loss: 0.6299 - val_acc: 0.7760\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.4449 - acc: 0.8472 - val_loss: 0.6318 - val_acc: 0.7770\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4418 - acc: 0.8479 - val_loss: 0.6325 - val_acc: 0.7780\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.4383 - acc: 0.8495 - val_loss: 0.6291 - val_acc: 0.7670\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.4351 - acc: 0.8516 - val_loss: 0.6287 - val_acc: 0.7800\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.4323 - acc: 0.8519 - val_loss: 0.6282 - val_acc: 0.7820\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.4286 - acc: 0.8521 - val_loss: 0.6249 - val_acc: 0.7740\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4257 - acc: 0.8547 - val_loss: 0.6328 - val_acc: 0.7810\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.4225 - acc: 0.8564 - val_loss: 0.6289 - val_acc: 0.7820\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.4192 - acc: 0.8567 - val_loss: 0.6258 - val_acc: 0.7780\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.4162 - acc: 0.8583 - val_loss: 0.6262 - val_acc: 0.7780\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.4136 - acc: 0.8604 - val_loss: 0.6232 - val_acc: 0.7860\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.4106 - acc: 0.8611 - val_loss: 0.6243 - val_acc: 0.7790\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4076 - acc: 0.8632 - val_loss: 0.6293 - val_acc: 0.7780\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.4047 - acc: 0.8612 - val_loss: 0.6245 - val_acc: 0.7850\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.4017 - acc: 0.8639 - val_loss: 0.6233 - val_acc: 0.7890\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.3989 - acc: 0.8652 - val_loss: 0.6258 - val_acc: 0.7830\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.3957 - acc: 0.8665 - val_loss: 0.6269 - val_acc: 0.7780\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.3930 - acc: 0.8684 - val_loss: 0.6230 - val_acc: 0.7860\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.3903 - acc: 0.8691 - val_loss: 0.6264 - val_acc: 0.7850\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.3874 - acc: 0.8693 - val_loss: 0.6225 - val_acc: 0.7820\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.3847 - acc: 0.8703 - val_loss: 0.6248 - val_acc: 0.7810\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.3818 - acc: 0.8712 - val_loss: 0.6242 - val_acc: 0.7840\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.3791 - acc: 0.8744 - val_loss: 0.6252 - val_acc: 0.7860\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.3766 - acc: 0.8741 - val_loss: 0.6241 - val_acc: 0.7810\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.3733 - acc: 0.8755 - val_loss: 0.6274 - val_acc: 0.7800\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.3709 - acc: 0.8768 - val_loss: 0.6251 - val_acc: 0.7820\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.3682 - acc: 0.8784 - val_loss: 0.6299 - val_acc: 0.7860\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.3654 - acc: 0.8799 - val_loss: 0.6254 - val_acc: 0.7850\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.3630 - acc: 0.8815 - val_loss: 0.6248 - val_acc: 0.7830\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.3606 - acc: 0.8820 - val_loss: 0.6271 - val_acc: 0.7820\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.3577 - acc: 0.8825 - val_loss: 0.6268 - val_acc: 0.7830\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.3558 - acc: 0.8821 - val_loss: 0.6230 - val_acc: 0.7840\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.3525 - acc: 0.8839 - val_loss: 0.6265 - val_acc: 0.7820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.3499 - acc: 0.8859 - val_loss: 0.6247 - val_acc: 0.7810\n"
     ]
    }
   ],
   "source": [
    "#Code provided; note the extra validation parameter passed.\n",
    "model_val = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Performance Results: the `history` dictionary\n",
    "\n",
    "The dictionary `history` contains four entries this time: one per metric that was being monitored during training and during validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_val_dict = model_val.history\n",
    "model_val_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 36us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 37us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3461201845010122, 0.8878666666984558]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7091857327620188, 0.7486666671435038]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the result isn't exactly the same as before. Note that this because the training set is slightly different! We remove 1000 instances for validation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the result similarly to what we have done in the previous lab. This time though, let's include the training and the validation loss in the same plot. We'll do the same thing for the training and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "loss_values = model_val_dict['loss']\n",
    "val_loss_values = model_val_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss_values) + 1)\n",
    "fig = plt.figure(figsize = (12,8))\n",
    "plt.plot(epochs, loss_values, 'g', label='Training loss')\n",
    "plt.plot(epochs, val_loss_values, 'blue', label='Validation loss')\n",
    "\n",
    "plt.title('Training & validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = model_val_dict['acc'] \n",
    "val_acc_values = model_val_dict['val_acc']\n",
    "\n",
    "plt.plot(epochs, acc_values, 'r', label='Training acc')\n",
    "plt.plot(epochs, val_acc_values, 'blue', label='Validation acc')\n",
    "plt.title('Training & validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe an interesting pattern here: although the training accuracy keeps increasing when going through more epochs, and the training loss keeps decreasing, the validation accuracy and loss seem to be reaching a status quo around the 60th epoch. This means that we're actually **overfitting** to the train data when we do as many epochs as we were doing. Luckily, you learned how to tackle overfitting in the previous lecture! For starters, it does seem clear that we are training too long. So let's stop training at the 60th epoch first (so-called \"early stopping\") before we move to more advanced regularization techniques!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/60\n",
      "7500/7500 [==============================] - 0s 36us/step - loss: 1.9318 - acc: 0.1668 - val_loss: 1.9224 - val_acc: 0.1740\n",
      "Epoch 2/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.9064 - acc: 0.2023 - val_loss: 1.8989 - val_acc: 0.2130\n",
      "Epoch 3/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.8805 - acc: 0.2289 - val_loss: 1.8734 - val_acc: 0.2400\n",
      "Epoch 4/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.8516 - acc: 0.2523 - val_loss: 1.8442 - val_acc: 0.2660\n",
      "Epoch 5/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8174 - acc: 0.2780 - val_loss: 1.8087 - val_acc: 0.2760\n",
      "Epoch 6/60\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.7782 - acc: 0.3024 - val_loss: 1.7712 - val_acc: 0.3030\n",
      "Epoch 7/60\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.7356 - acc: 0.3372 - val_loss: 1.7302 - val_acc: 0.3280\n",
      "Epoch 8/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.6909 - acc: 0.3656 - val_loss: 1.6870 - val_acc: 0.3340\n",
      "Epoch 9/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.6446 - acc: 0.3961 - val_loss: 1.6432 - val_acc: 0.3500\n",
      "Epoch 10/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.5966 - acc: 0.4247 - val_loss: 1.5976 - val_acc: 0.3740\n",
      "Epoch 11/60\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.5483 - acc: 0.4543 - val_loss: 1.5541 - val_acc: 0.4220\n",
      "Epoch 12/60\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4998 - acc: 0.4873 - val_loss: 1.5075 - val_acc: 0.4530\n",
      "Epoch 13/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.4509 - acc: 0.5121 - val_loss: 1.4608 - val_acc: 0.4910\n",
      "Epoch 14/60\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4018 - acc: 0.5420 - val_loss: 1.4153 - val_acc: 0.5240\n",
      "Epoch 15/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.3533 - acc: 0.5725 - val_loss: 1.3692 - val_acc: 0.5380\n",
      "Epoch 16/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3053 - acc: 0.5953 - val_loss: 1.3245 - val_acc: 0.5760\n",
      "Epoch 17/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.2589 - acc: 0.6160 - val_loss: 1.2850 - val_acc: 0.5760\n",
      "Epoch 18/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.2140 - acc: 0.6356 - val_loss: 1.2399 - val_acc: 0.5970\n",
      "Epoch 19/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1708 - acc: 0.6480 - val_loss: 1.1992 - val_acc: 0.6260\n",
      "Epoch 20/60\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1295 - acc: 0.6632 - val_loss: 1.1639 - val_acc: 0.6240\n",
      "Epoch 21/60\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0907 - acc: 0.6773 - val_loss: 1.1256 - val_acc: 0.6540\n",
      "Epoch 22/60\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.0543 - acc: 0.6859 - val_loss: 1.0925 - val_acc: 0.6640\n",
      "Epoch 23/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0195 - acc: 0.6959 - val_loss: 1.0602 - val_acc: 0.6760\n",
      "Epoch 24/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9872 - acc: 0.7016 - val_loss: 1.0324 - val_acc: 0.6780\n",
      "Epoch 25/60\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.9569 - acc: 0.7137 - val_loss: 1.0053 - val_acc: 0.6820\n",
      "Epoch 26/60\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9291 - acc: 0.7200 - val_loss: 0.9785 - val_acc: 0.6980\n",
      "Epoch 27/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.9027 - acc: 0.7281 - val_loss: 0.9599 - val_acc: 0.7010\n",
      "Epoch 28/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8788 - acc: 0.7331 - val_loss: 0.9318 - val_acc: 0.7170\n",
      "Epoch 29/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8561 - acc: 0.7389 - val_loss: 0.9113 - val_acc: 0.7210\n",
      "Epoch 30/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8346 - acc: 0.7415 - val_loss: 0.8926 - val_acc: 0.7240\n",
      "Epoch 31/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.8149 - acc: 0.7473 - val_loss: 0.8758 - val_acc: 0.7240\n",
      "Epoch 32/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.7964 - acc: 0.7533 - val_loss: 0.8606 - val_acc: 0.7270\n",
      "Epoch 33/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.7795 - acc: 0.7556 - val_loss: 0.8474 - val_acc: 0.7210\n",
      "Epoch 34/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.7634 - acc: 0.7576 - val_loss: 0.8295 - val_acc: 0.7320\n",
      "Epoch 35/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.7479 - acc: 0.7620 - val_loss: 0.8165 - val_acc: 0.7350\n",
      "Epoch 36/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.7337 - acc: 0.7645 - val_loss: 0.8052 - val_acc: 0.7340\n",
      "Epoch 37/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7210 - acc: 0.7676 - val_loss: 0.7957 - val_acc: 0.7330\n",
      "Epoch 38/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.7086 - acc: 0.7728 - val_loss: 0.7840 - val_acc: 0.7380\n",
      "Epoch 39/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.6976 - acc: 0.7731 - val_loss: 0.7729 - val_acc: 0.7430\n",
      "Epoch 40/60\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.6861 - acc: 0.7769 - val_loss: 0.7620 - val_acc: 0.7440\n",
      "Epoch 41/60\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.6755 - acc: 0.7803 - val_loss: 0.7565 - val_acc: 0.7440\n",
      "Epoch 42/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.6653 - acc: 0.7812 - val_loss: 0.7519 - val_acc: 0.7460\n",
      "Epoch 43/60\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.6563 - acc: 0.7861 - val_loss: 0.7396 - val_acc: 0.7510\n",
      "Epoch 44/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.6471 - acc: 0.7883 - val_loss: 0.7321 - val_acc: 0.7540\n",
      "Epoch 45/60\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.6393 - acc: 0.7892 - val_loss: 0.7278 - val_acc: 0.7540\n",
      "Epoch 46/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.6303 - acc: 0.7919 - val_loss: 0.7227 - val_acc: 0.7540\n",
      "Epoch 47/60\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.6223 - acc: 0.7947 - val_loss: 0.7156 - val_acc: 0.7570\n",
      "Epoch 48/60\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.6147 - acc: 0.7963 - val_loss: 0.7099 - val_acc: 0.7560\n",
      "Epoch 49/60\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.6077 - acc: 0.7983 - val_loss: 0.7038 - val_acc: 0.7600\n",
      "Epoch 50/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.6002 - acc: 0.8028 - val_loss: 0.6969 - val_acc: 0.7710\n",
      "Epoch 51/60\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.5935 - acc: 0.8053 - val_loss: 0.6937 - val_acc: 0.7610\n",
      "Epoch 52/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5868 - acc: 0.8032 - val_loss: 0.6924 - val_acc: 0.7640\n",
      "Epoch 53/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.5806 - acc: 0.8061 - val_loss: 0.6863 - val_acc: 0.7670\n",
      "Epoch 54/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5744 - acc: 0.8081 - val_loss: 0.6803 - val_acc: 0.7700\n",
      "Epoch 55/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.5686 - acc: 0.8108 - val_loss: 0.6791 - val_acc: 0.7690\n",
      "Epoch 56/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5622 - acc: 0.8112 - val_loss: 0.6720 - val_acc: 0.7730\n",
      "Epoch 57/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5567 - acc: 0.8167 - val_loss: 0.6774 - val_acc: 0.7720\n",
      "Epoch 58/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5513 - acc: 0.8161 - val_loss: 0.6663 - val_acc: 0.7790\n",
      "Epoch 59/60\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.5460 - acc: 0.8195 - val_loss: 0.6610 - val_acc: 0.7730\n",
      "Epoch 60/60\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 0.5403 - acc: 0.8196 - val_loss: 0.6619 - val_acc: 0.7760\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "final_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=60,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can use the test set to make label predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 34us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1500/1500 [==============================] - 0s 32us/step\n"
     ]
    }
   ],
   "source": [
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.5352478552182516, 0.8228]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.7396075248718261, 0.7326666669845581]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've significantly reduced the variance, so this is already pretty good! Our test set accuracy is slightly worse, but this model will definitely be more robust than the 120 epochs one we fitted before.\n",
    "\n",
    "Now, let's see what else we can do to improve the result!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's include L2 regularization. You can easily do this in keras adding the argument kernel_regulizers.l2 and adding a value for the regularization parameter lambda between parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 2.5996 - acc: 0.1564 - val_loss: 2.5771 - val_acc: 0.1640\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 2.5655 - acc: 0.1752 - val_loss: 2.5514 - val_acc: 0.1980\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 2.5395 - acc: 0.2028 - val_loss: 2.5273 - val_acc: 0.2130\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.5139 - acc: 0.2173 - val_loss: 2.5026 - val_acc: 0.2300\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 2.4876 - acc: 0.2340 - val_loss: 2.4764 - val_acc: 0.2510\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 2.4595 - acc: 0.2487 - val_loss: 2.4477 - val_acc: 0.2610\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 2.4287 - acc: 0.2705 - val_loss: 2.4167 - val_acc: 0.2710\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 2.3956 - acc: 0.2939 - val_loss: 2.3833 - val_acc: 0.2980\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 2.3604 - acc: 0.3219 - val_loss: 2.3475 - val_acc: 0.3130\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 2.3228 - acc: 0.3477 - val_loss: 2.3110 - val_acc: 0.3400\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 2.2829 - acc: 0.3799 - val_loss: 2.2717 - val_acc: 0.3560\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 2.2411 - acc: 0.4044 - val_loss: 2.2311 - val_acc: 0.3860\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.1975 - acc: 0.4311 - val_loss: 2.1887 - val_acc: 0.3980\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 2.1521 - acc: 0.4541 - val_loss: 2.1452 - val_acc: 0.4240\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 2.1060 - acc: 0.4799 - val_loss: 2.1010 - val_acc: 0.4510\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 2.0588 - acc: 0.5013 - val_loss: 2.0562 - val_acc: 0.4770\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 2.0117 - acc: 0.5261 - val_loss: 2.0127 - val_acc: 0.4990\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.9653 - acc: 0.5463 - val_loss: 1.9682 - val_acc: 0.5200\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.9196 - acc: 0.5664 - val_loss: 1.9267 - val_acc: 0.5420\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.8756 - acc: 0.5855 - val_loss: 1.8860 - val_acc: 0.5570\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.8334 - acc: 0.6031 - val_loss: 1.8472 - val_acc: 0.5750\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.7933 - acc: 0.6196 - val_loss: 1.8092 - val_acc: 0.5950\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.7551 - acc: 0.6343 - val_loss: 1.7748 - val_acc: 0.5990\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.7193 - acc: 0.6456 - val_loss: 1.7430 - val_acc: 0.6090\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.6852 - acc: 0.6557 - val_loss: 1.7106 - val_acc: 0.6230\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 1.6529 - acc: 0.6660 - val_loss: 1.6825 - val_acc: 0.6330\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.6224 - acc: 0.6776 - val_loss: 1.6558 - val_acc: 0.6330\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5940 - acc: 0.6833 - val_loss: 1.6282 - val_acc: 0.6450\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5671 - acc: 0.6928 - val_loss: 1.6037 - val_acc: 0.6570\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5417 - acc: 0.6979 - val_loss: 1.5807 - val_acc: 0.6660\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5175 - acc: 0.7063 - val_loss: 1.5598 - val_acc: 0.6730\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4949 - acc: 0.7131 - val_loss: 1.5392 - val_acc: 0.6770\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4732 - acc: 0.7187 - val_loss: 1.5215 - val_acc: 0.6870\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4531 - acc: 0.7231 - val_loss: 1.5015 - val_acc: 0.6850\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4339 - acc: 0.7293 - val_loss: 1.4859 - val_acc: 0.6930\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.4156 - acc: 0.7328 - val_loss: 1.4675 - val_acc: 0.6960\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3981 - acc: 0.7384 - val_loss: 1.4533 - val_acc: 0.7040\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.3816 - acc: 0.7405 - val_loss: 1.4386 - val_acc: 0.7120\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 1.3659 - acc: 0.7448 - val_loss: 1.4238 - val_acc: 0.7100\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.3508 - acc: 0.7481 - val_loss: 1.4117 - val_acc: 0.7090\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3365 - acc: 0.7517 - val_loss: 1.4011 - val_acc: 0.7140\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3228 - acc: 0.7540 - val_loss: 1.3888 - val_acc: 0.7180\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3100 - acc: 0.7589 - val_loss: 1.3769 - val_acc: 0.7170\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.2972 - acc: 0.7615 - val_loss: 1.3647 - val_acc: 0.7250\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.2852 - acc: 0.7633 - val_loss: 1.3538 - val_acc: 0.7300\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.2732 - acc: 0.7628 - val_loss: 1.3440 - val_acc: 0.7290\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.2625 - acc: 0.7672 - val_loss: 1.3333 - val_acc: 0.7330\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.2517 - acc: 0.7732 - val_loss: 1.3255 - val_acc: 0.7310\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.2410 - acc: 0.7748 - val_loss: 1.3174 - val_acc: 0.7340\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.2311 - acc: 0.7787 - val_loss: 1.3069 - val_acc: 0.7380\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.2215 - acc: 0.7752 - val_loss: 1.3023 - val_acc: 0.7360\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.2121 - acc: 0.7843 - val_loss: 1.2965 - val_acc: 0.7410\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.2032 - acc: 0.7801 - val_loss: 1.2832 - val_acc: 0.7430\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1945 - acc: 0.7861 - val_loss: 1.2782 - val_acc: 0.7390\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1858 - acc: 0.7887 - val_loss: 1.2715 - val_acc: 0.7500\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1777 - acc: 0.7888 - val_loss: 1.2609 - val_acc: 0.7480\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1696 - acc: 0.7912 - val_loss: 1.2549 - val_acc: 0.7470\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1619 - acc: 0.7940 - val_loss: 1.2493 - val_acc: 0.7510\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1539 - acc: 0.7977 - val_loss: 1.2438 - val_acc: 0.7500\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1470 - acc: 0.7964 - val_loss: 1.2399 - val_acc: 0.7580\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1393 - acc: 0.8012 - val_loss: 1.2363 - val_acc: 0.7550\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1324 - acc: 0.8007 - val_loss: 1.2241 - val_acc: 0.7600\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1253 - acc: 0.8012 - val_loss: 1.2206 - val_acc: 0.7580\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1185 - acc: 0.8051 - val_loss: 1.2162 - val_acc: 0.7620\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1115 - acc: 0.8055 - val_loss: 1.2102 - val_acc: 0.7650\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1055 - acc: 0.8072 - val_loss: 1.2060 - val_acc: 0.7680\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0988 - acc: 0.8083 - val_loss: 1.2012 - val_acc: 0.7660\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0926 - acc: 0.8107 - val_loss: 1.1955 - val_acc: 0.7670\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0867 - acc: 0.8115 - val_loss: 1.1908 - val_acc: 0.7650\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0808 - acc: 0.8123 - val_loss: 1.1867 - val_acc: 0.7660\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0747 - acc: 0.8133 - val_loss: 1.1817 - val_acc: 0.7730\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0690 - acc: 0.8164 - val_loss: 1.1790 - val_acc: 0.7670\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0629 - acc: 0.8179 - val_loss: 1.1714 - val_acc: 0.7740\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0573 - acc: 0.8188 - val_loss: 1.1705 - val_acc: 0.7770\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0518 - acc: 0.8196 - val_loss: 1.1656 - val_acc: 0.7720\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0462 - acc: 0.8211 - val_loss: 1.1605 - val_acc: 0.7770\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0406 - acc: 0.8217 - val_loss: 1.1580 - val_acc: 0.7730\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0357 - acc: 0.8243 - val_loss: 1.1540 - val_acc: 0.7750\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0304 - acc: 0.8256 - val_loss: 1.1517 - val_acc: 0.7770\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0251 - acc: 0.8268 - val_loss: 1.1457 - val_acc: 0.7750\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0200 - acc: 0.8291 - val_loss: 1.1442 - val_acc: 0.7750\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0149 - acc: 0.8291 - val_loss: 1.1396 - val_acc: 0.7790\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0101 - acc: 0.8304 - val_loss: 1.1335 - val_acc: 0.7760\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0050 - acc: 0.8317 - val_loss: 1.1356 - val_acc: 0.7760\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0000 - acc: 0.8323 - val_loss: 1.1294 - val_acc: 0.7780\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9955 - acc: 0.8353 - val_loss: 1.1256 - val_acc: 0.7790\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9906 - acc: 0.8384 - val_loss: 1.1211 - val_acc: 0.7780\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9857 - acc: 0.8379 - val_loss: 1.1204 - val_acc: 0.7790\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9812 - acc: 0.8395 - val_loss: 1.1169 - val_acc: 0.7800\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9766 - acc: 0.8416 - val_loss: 1.1131 - val_acc: 0.7800\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9718 - acc: 0.8393 - val_loss: 1.1116 - val_acc: 0.7850\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9676 - acc: 0.8436 - val_loss: 1.1100 - val_acc: 0.7740\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9627 - acc: 0.8441 - val_loss: 1.1069 - val_acc: 0.7770\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9590 - acc: 0.8445 - val_loss: 1.1022 - val_acc: 0.7830\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9541 - acc: 0.8452 - val_loss: 1.1038 - val_acc: 0.7770\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9505 - acc: 0.8444 - val_loss: 1.0975 - val_acc: 0.7820\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9455 - acc: 0.8475 - val_loss: 1.0927 - val_acc: 0.7840\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9413 - acc: 0.8477 - val_loss: 1.0903 - val_acc: 0.7860\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9375 - acc: 0.8505 - val_loss: 1.0887 - val_acc: 0.7830\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9331 - acc: 0.8492 - val_loss: 1.0857 - val_acc: 0.7820\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9291 - acc: 0.8515 - val_loss: 1.0848 - val_acc: 0.7730\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9251 - acc: 0.8505 - val_loss: 1.0816 - val_acc: 0.7800\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9210 - acc: 0.8528 - val_loss: 1.0797 - val_acc: 0.7720\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9171 - acc: 0.8543 - val_loss: 1.0756 - val_acc: 0.7820\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9129 - acc: 0.8569 - val_loss: 1.0769 - val_acc: 0.7760\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9091 - acc: 0.8572 - val_loss: 1.0745 - val_acc: 0.7780\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9051 - acc: 0.8581 - val_loss: 1.0713 - val_acc: 0.7820\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9011 - acc: 0.8575 - val_loss: 1.0696 - val_acc: 0.7810\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8972 - acc: 0.8567 - val_loss: 1.0681 - val_acc: 0.7810\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8937 - acc: 0.8577 - val_loss: 1.0635 - val_acc: 0.7830\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8897 - acc: 0.8609 - val_loss: 1.0618 - val_acc: 0.7820\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8858 - acc: 0.8608 - val_loss: 1.0647 - val_acc: 0.7780\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8823 - acc: 0.8609 - val_loss: 1.0572 - val_acc: 0.7800\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8788 - acc: 0.8635 - val_loss: 1.0535 - val_acc: 0.7860\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8752 - acc: 0.8636 - val_loss: 1.0521 - val_acc: 0.7810\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8713 - acc: 0.8653 - val_loss: 1.0562 - val_acc: 0.7860\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8682 - acc: 0.8648 - val_loss: 1.0497 - val_acc: 0.7870\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8643 - acc: 0.8661 - val_loss: 1.0479 - val_acc: 0.7850\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8605 - acc: 0.8671 - val_loss: 1.0465 - val_acc: 0.7840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8571 - acc: 0.8679 - val_loss: 1.0451 - val_acc: 0.7830\n"
     ]
    }
   ],
   "source": [
    "from keras import regularizers\n",
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l2(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l2(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L2_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L2_model_dict = L2_model.history\n",
    "L2_model_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the training accuracy as well as the validation accuracy for both the L2 and the model without regularization (for 120 epochs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()\n",
    "\n",
    "acc_values = L2_model_dict['acc'] \n",
    "val_acc_values = L2_model_dict['val_acc']\n",
    "model_acc = model_val_dict['acc']\n",
    "model_val_acc = model_val_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L2')\n",
    "plt.plot(epochs, val_acc_values, 'r', label='Validation acc L2')\n",
    "plt.plot(epochs, model_acc, 'b', label='Training acc')\n",
    "plt.plot(epochs, model_val_acc, 'orange', label='Validation acc')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results of L2 regularization are quite disappointing here. We notice the discrepancy between validation and training accuracy seems to have decreased slightly, but the end result is definitely not getting better. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at L1 regularization. Will this work better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/120\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 16.0520 - acc: 0.1469 - val_loss: 15.6466 - val_acc: 0.1760\n",
      "Epoch 2/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 15.2916 - acc: 0.1668 - val_loss: 14.9051 - val_acc: 0.1810\n",
      "Epoch 3/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 14.5588 - acc: 0.1841 - val_loss: 14.1855 - val_acc: 0.1910\n",
      "Epoch 4/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 13.8482 - acc: 0.2111 - val_loss: 13.4872 - val_acc: 0.2150\n",
      "Epoch 5/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 13.1580 - acc: 0.2376 - val_loss: 12.8083 - val_acc: 0.2520\n",
      "Epoch 6/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 12.4870 - acc: 0.2632 - val_loss: 12.1475 - val_acc: 0.2720\n",
      "Epoch 7/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 11.8341 - acc: 0.2909 - val_loss: 11.5048 - val_acc: 0.3010\n",
      "Epoch 8/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 11.1996 - acc: 0.3212 - val_loss: 10.8820 - val_acc: 0.3150\n",
      "Epoch 9/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 10.5847 - acc: 0.3425 - val_loss: 10.2798 - val_acc: 0.3320\n",
      "Epoch 10/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 9.9904 - acc: 0.3785 - val_loss: 9.6957 - val_acc: 0.3590\n",
      "Epoch 11/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 9.4166 - acc: 0.4020 - val_loss: 9.1334 - val_acc: 0.3850\n",
      "Epoch 12/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 8.8631 - acc: 0.4371 - val_loss: 8.5913 - val_acc: 0.4150\n",
      "Epoch 13/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 8.3300 - acc: 0.4716 - val_loss: 8.0732 - val_acc: 0.4340\n",
      "Epoch 14/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 7.8184 - acc: 0.4905 - val_loss: 7.5715 - val_acc: 0.4630\n",
      "Epoch 15/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 7.3282 - acc: 0.5143 - val_loss: 7.0941 - val_acc: 0.4840\n",
      "Epoch 16/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 6.8603 - acc: 0.5317 - val_loss: 6.6386 - val_acc: 0.5120\n",
      "Epoch 17/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 6.4150 - acc: 0.5581 - val_loss: 6.2079 - val_acc: 0.5260\n",
      "Epoch 18/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 5.9920 - acc: 0.5673 - val_loss: 5.7968 - val_acc: 0.5540\n",
      "Epoch 19/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 5.5909 - acc: 0.5908 - val_loss: 5.4089 - val_acc: 0.5550\n",
      "Epoch 20/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 5.2121 - acc: 0.6000 - val_loss: 5.0419 - val_acc: 0.5830\n",
      "Epoch 21/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 4.8567 - acc: 0.6116 - val_loss: 4.6990 - val_acc: 0.5800\n",
      "Epoch 22/120\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 4.5243 - acc: 0.6156 - val_loss: 4.3797 - val_acc: 0.5950\n",
      "Epoch 23/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 4.2141 - acc: 0.6236 - val_loss: 4.0801 - val_acc: 0.6060\n",
      "Epoch 24/120\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 3.9264 - acc: 0.6308 - val_loss: 3.8045 - val_acc: 0.6190\n",
      "Epoch 25/120\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 3.6615 - acc: 0.6365 - val_loss: 3.5523 - val_acc: 0.6340\n",
      "Epoch 26/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 3.4192 - acc: 0.6423 - val_loss: 3.3213 - val_acc: 0.6380\n",
      "Epoch 27/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 3.1992 - acc: 0.6519 - val_loss: 3.1123 - val_acc: 0.6200\n",
      "Epoch 28/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 3.0003 - acc: 0.6507 - val_loss: 2.9239 - val_acc: 0.6350\n",
      "Epoch 29/120\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 2.8222 - acc: 0.6533 - val_loss: 2.7576 - val_acc: 0.6500\n",
      "Epoch 30/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.6652 - acc: 0.6583 - val_loss: 2.6102 - val_acc: 0.6550\n",
      "Epoch 31/120\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 2.5288 - acc: 0.6589 - val_loss: 2.4822 - val_acc: 0.6570\n",
      "Epoch 32/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 2.4129 - acc: 0.6577 - val_loss: 2.3760 - val_acc: 0.6720\n",
      "Epoch 33/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 2.3158 - acc: 0.6643 - val_loss: 2.2900 - val_acc: 0.6470\n",
      "Epoch 34/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.2376 - acc: 0.6605 - val_loss: 2.2187 - val_acc: 0.6720\n",
      "Epoch 35/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.1762 - acc: 0.6655 - val_loss: 2.1634 - val_acc: 0.6790\n",
      "Epoch 36/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.1298 - acc: 0.6665 - val_loss: 2.1226 - val_acc: 0.6810\n",
      "Epoch 37/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.0952 - acc: 0.6713 - val_loss: 2.0907 - val_acc: 0.6810\n",
      "Epoch 38/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 2.0673 - acc: 0.6691 - val_loss: 2.0646 - val_acc: 0.6740\n",
      "Epoch 39/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.0434 - acc: 0.6699 - val_loss: 2.0422 - val_acc: 0.6830\n",
      "Epoch 40/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.0215 - acc: 0.6695 - val_loss: 2.0198 - val_acc: 0.6880\n",
      "Epoch 41/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 2.0016 - acc: 0.6715 - val_loss: 2.0017 - val_acc: 0.6750\n",
      "Epoch 42/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.9828 - acc: 0.6719 - val_loss: 1.9800 - val_acc: 0.6840\n",
      "Epoch 43/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.9647 - acc: 0.6728 - val_loss: 1.9656 - val_acc: 0.6860\n",
      "Epoch 44/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.9481 - acc: 0.6745 - val_loss: 1.9491 - val_acc: 0.6770\n",
      "Epoch 45/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.9319 - acc: 0.6744 - val_loss: 1.9289 - val_acc: 0.6920\n",
      "Epoch 46/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.9161 - acc: 0.6737 - val_loss: 1.9123 - val_acc: 0.6950\n",
      "Epoch 47/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.9008 - acc: 0.6768 - val_loss: 1.8975 - val_acc: 0.6950\n",
      "Epoch 48/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.8865 - acc: 0.6757 - val_loss: 1.8828 - val_acc: 0.6920\n",
      "Epoch 49/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.8726 - acc: 0.6783 - val_loss: 1.8688 - val_acc: 0.6970\n",
      "Epoch 50/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.8588 - acc: 0.6800 - val_loss: 1.8550 - val_acc: 0.6980\n",
      "Epoch 51/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.8457 - acc: 0.6819 - val_loss: 1.8405 - val_acc: 0.6940\n",
      "Epoch 52/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8328 - acc: 0.6832 - val_loss: 1.8312 - val_acc: 0.6990\n",
      "Epoch 53/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8204 - acc: 0.6851 - val_loss: 1.8183 - val_acc: 0.6980\n",
      "Epoch 54/120\n",
      "7500/7500 [==============================] - 0s 16us/step - loss: 1.8088 - acc: 0.6841 - val_loss: 1.8036 - val_acc: 0.6920\n",
      "Epoch 55/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.7970 - acc: 0.6835 - val_loss: 1.7928 - val_acc: 0.6990\n",
      "Epoch 56/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.7858 - acc: 0.6856 - val_loss: 1.7804 - val_acc: 0.6990\n",
      "Epoch 57/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.7745 - acc: 0.6844 - val_loss: 1.7692 - val_acc: 0.6860\n",
      "Epoch 58/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.7637 - acc: 0.6851 - val_loss: 1.7586 - val_acc: 0.6980\n",
      "Epoch 59/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.7526 - acc: 0.6859 - val_loss: 1.7504 - val_acc: 0.6950\n",
      "Epoch 60/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.7425 - acc: 0.6868 - val_loss: 1.7393 - val_acc: 0.7020\n",
      "Epoch 61/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.7327 - acc: 0.6861 - val_loss: 1.7272 - val_acc: 0.6970\n",
      "Epoch 62/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.7220 - acc: 0.6881 - val_loss: 1.7178 - val_acc: 0.7010\n",
      "Epoch 63/120\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.7131 - acc: 0.6888 - val_loss: 1.7077 - val_acc: 0.7000\n",
      "Epoch 64/120\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.7027 - acc: 0.6895 - val_loss: 1.7029 - val_acc: 0.6980\n",
      "Epoch 65/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.6935 - acc: 0.6901 - val_loss: 1.6874 - val_acc: 0.6990\n",
      "Epoch 66/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.6841 - acc: 0.6911 - val_loss: 1.6783 - val_acc: 0.7000\n",
      "Epoch 67/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.6753 - acc: 0.6920 - val_loss: 1.6700 - val_acc: 0.6980\n",
      "Epoch 68/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.6665 - acc: 0.6928 - val_loss: 1.6620 - val_acc: 0.6980\n",
      "Epoch 69/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6576 - acc: 0.6921 - val_loss: 1.6549 - val_acc: 0.6980\n",
      "Epoch 70/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6492 - acc: 0.6935 - val_loss: 1.6441 - val_acc: 0.7070\n",
      "Epoch 71/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6409 - acc: 0.6932 - val_loss: 1.6352 - val_acc: 0.7010\n",
      "Epoch 72/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6325 - acc: 0.6945 - val_loss: 1.6284 - val_acc: 0.7030\n",
      "Epoch 73/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6246 - acc: 0.6935 - val_loss: 1.6231 - val_acc: 0.7040\n",
      "Epoch 74/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6164 - acc: 0.6949 - val_loss: 1.6102 - val_acc: 0.7050\n",
      "Epoch 75/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6088 - acc: 0.6945 - val_loss: 1.6056 - val_acc: 0.7100\n",
      "Epoch 76/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6010 - acc: 0.6967 - val_loss: 1.5989 - val_acc: 0.7000\n",
      "Epoch 77/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5941 - acc: 0.6945 - val_loss: 1.5879 - val_acc: 0.7030\n",
      "Epoch 78/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5864 - acc: 0.6964 - val_loss: 1.5786 - val_acc: 0.7070\n",
      "Epoch 79/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5785 - acc: 0.6977 - val_loss: 1.5750 - val_acc: 0.7120\n",
      "Epoch 80/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5713 - acc: 0.6960 - val_loss: 1.5648 - val_acc: 0.7070\n",
      "Epoch 81/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5640 - acc: 0.6977 - val_loss: 1.5592 - val_acc: 0.7110\n",
      "Epoch 82/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5574 - acc: 0.6965 - val_loss: 1.5507 - val_acc: 0.7090\n",
      "Epoch 83/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5502 - acc: 0.6977 - val_loss: 1.5466 - val_acc: 0.7120\n",
      "Epoch 84/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5436 - acc: 0.6971 - val_loss: 1.5384 - val_acc: 0.7110\n",
      "Epoch 85/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.5366 - acc: 0.6995 - val_loss: 1.5391 - val_acc: 0.7070\n",
      "Epoch 86/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5303 - acc: 0.6979 - val_loss: 1.5237 - val_acc: 0.7130\n",
      "Epoch 87/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5240 - acc: 0.6977 - val_loss: 1.5180 - val_acc: 0.7130\n",
      "Epoch 88/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5168 - acc: 0.6997 - val_loss: 1.5097 - val_acc: 0.7120\n",
      "Epoch 89/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5102 - acc: 0.6984 - val_loss: 1.5057 - val_acc: 0.7090\n",
      "Epoch 90/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.5037 - acc: 0.6980 - val_loss: 1.4991 - val_acc: 0.7130\n",
      "Epoch 91/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.4977 - acc: 0.6989 - val_loss: 1.4956 - val_acc: 0.7130\n",
      "Epoch 92/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.4920 - acc: 0.6999 - val_loss: 1.4880 - val_acc: 0.7160\n",
      "Epoch 93/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4852 - acc: 0.6989 - val_loss: 1.4825 - val_acc: 0.7130\n",
      "Epoch 94/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4796 - acc: 0.6976 - val_loss: 1.4764 - val_acc: 0.7130\n",
      "Epoch 95/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4731 - acc: 0.6996 - val_loss: 1.4718 - val_acc: 0.7170\n",
      "Epoch 96/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4669 - acc: 0.6988 - val_loss: 1.4618 - val_acc: 0.7160\n",
      "Epoch 97/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4607 - acc: 0.7005 - val_loss: 1.4571 - val_acc: 0.7160\n",
      "Epoch 98/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4555 - acc: 0.7008 - val_loss: 1.4512 - val_acc: 0.7140\n",
      "Epoch 99/120\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4492 - acc: 0.6997 - val_loss: 1.4441 - val_acc: 0.7200\n",
      "Epoch 100/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4435 - acc: 0.6997 - val_loss: 1.4414 - val_acc: 0.7210\n",
      "Epoch 101/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4382 - acc: 0.7035 - val_loss: 1.4353 - val_acc: 0.7180\n",
      "Epoch 102/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4324 - acc: 0.7025 - val_loss: 1.4350 - val_acc: 0.7180\n",
      "Epoch 103/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4273 - acc: 0.7023 - val_loss: 1.4271 - val_acc: 0.7210\n",
      "Epoch 104/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4218 - acc: 0.7021 - val_loss: 1.4200 - val_acc: 0.7180\n",
      "Epoch 105/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4165 - acc: 0.7031 - val_loss: 1.4187 - val_acc: 0.7210\n",
      "Epoch 106/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4109 - acc: 0.7017 - val_loss: 1.4084 - val_acc: 0.7210\n",
      "Epoch 107/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4057 - acc: 0.7032 - val_loss: 1.4013 - val_acc: 0.7170\n",
      "Epoch 108/120\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4004 - acc: 0.7053 - val_loss: 1.3956 - val_acc: 0.7200\n",
      "Epoch 109/120\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3951 - acc: 0.7036 - val_loss: 1.3929 - val_acc: 0.7190\n",
      "Epoch 110/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.3900 - acc: 0.7040 - val_loss: 1.3874 - val_acc: 0.7230\n",
      "Epoch 111/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3854 - acc: 0.7049 - val_loss: 1.3834 - val_acc: 0.7270\n",
      "Epoch 112/120\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.3800 - acc: 0.7031 - val_loss: 1.3787 - val_acc: 0.7210\n",
      "Epoch 113/120\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.3754 - acc: 0.7059 - val_loss: 1.3716 - val_acc: 0.7220\n",
      "Epoch 114/120\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.3703 - acc: 0.7057 - val_loss: 1.3715 - val_acc: 0.7210\n",
      "Epoch 115/120\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.3656 - acc: 0.7065 - val_loss: 1.3623 - val_acc: 0.7240\n",
      "Epoch 116/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3604 - acc: 0.7063 - val_loss: 1.3564 - val_acc: 0.7220\n",
      "Epoch 117/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3557 - acc: 0.7064 - val_loss: 1.3592 - val_acc: 0.7200\n",
      "Epoch 118/120\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.3510 - acc: 0.7069 - val_loss: 1.3488 - val_acc: 0.7220\n",
      "Epoch 119/120\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.3466 - acc: 0.7055 - val_loss: 1.3434 - val_acc: 0.7210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/120\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.3418 - acc: 0.7063 - val_loss: 1.3394 - val_acc: 0.7280\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',\n",
    "                       kernel_regularizer=regularizers.l1(0.005), \n",
    "                       input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), \n",
    "                       activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'r.', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy with L1 regularization')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how The training and validation accuracy don't diverge as much as before! Unfortunately, the validation accuracy doesn't reach rates much higher than 70%. It does seem like we can still improve the model by training much longer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 16.0109 - acc: 0.1737 - val_loss: 15.5959 - val_acc: 0.1950\n",
      "Epoch 2/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 15.2428 - acc: 0.2103 - val_loss: 14.8495 - val_acc: 0.2120\n",
      "Epoch 3/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 14.5025 - acc: 0.2245 - val_loss: 14.1244 - val_acc: 0.2320\n",
      "Epoch 4/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 13.7850 - acc: 0.2369 - val_loss: 13.4199 - val_acc: 0.2440\n",
      "Epoch 5/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 13.0882 - acc: 0.2528 - val_loss: 12.7336 - val_acc: 0.2560\n",
      "Epoch 6/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 12.4107 - acc: 0.2669 - val_loss: 12.0657 - val_acc: 0.2890\n",
      "Epoch 7/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 11.7519 - acc: 0.2925 - val_loss: 11.4175 - val_acc: 0.3160\n",
      "Epoch 8/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 11.1127 - acc: 0.3168 - val_loss: 10.7892 - val_acc: 0.3470\n",
      "Epoch 9/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 10.4932 - acc: 0.3496 - val_loss: 10.1803 - val_acc: 0.3650\n",
      "Epoch 10/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 9.8943 - acc: 0.3856 - val_loss: 9.5931 - val_acc: 0.3870\n",
      "Epoch 11/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 9.3171 - acc: 0.4201 - val_loss: 9.0284 - val_acc: 0.4210\n",
      "Epoch 12/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 8.7618 - acc: 0.4516 - val_loss: 8.4854 - val_acc: 0.4690\n",
      "Epoch 13/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 8.2289 - acc: 0.4843 - val_loss: 7.9648 - val_acc: 0.4790\n",
      "Epoch 14/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 7.7183 - acc: 0.5116 - val_loss: 7.4648 - val_acc: 0.5120\n",
      "Epoch 15/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 7.2301 - acc: 0.5371 - val_loss: 6.9886 - val_acc: 0.5420\n",
      "Epoch 16/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 6.7643 - acc: 0.5623 - val_loss: 6.5353 - val_acc: 0.5710\n",
      "Epoch 17/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 6.3203 - acc: 0.5831 - val_loss: 6.1033 - val_acc: 0.5830\n",
      "Epoch 18/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 5.8992 - acc: 0.6012 - val_loss: 5.6925 - val_acc: 0.5940\n",
      "Epoch 19/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 5.5011 - acc: 0.6171 - val_loss: 5.3070 - val_acc: 0.5940\n",
      "Epoch 20/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 5.1255 - acc: 0.6241 - val_loss: 4.9447 - val_acc: 0.6440\n",
      "Epoch 21/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 4.7718 - acc: 0.6368 - val_loss: 4.6033 - val_acc: 0.6270\n",
      "Epoch 22/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 4.4407 - acc: 0.6457 - val_loss: 4.2840 - val_acc: 0.6590\n",
      "Epoch 23/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 4.1325 - acc: 0.6513 - val_loss: 3.9875 - val_acc: 0.6750\n",
      "Epoch 24/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 3.8475 - acc: 0.6613 - val_loss: 3.7140 - val_acc: 0.6710\n",
      "Epoch 25/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 3.5842 - acc: 0.6636 - val_loss: 3.4645 - val_acc: 0.6820\n",
      "Epoch 26/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 3.3434 - acc: 0.6689 - val_loss: 3.2342 - val_acc: 0.6720\n",
      "Epoch 27/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 3.1244 - acc: 0.6697 - val_loss: 3.0263 - val_acc: 0.6790\n",
      "Epoch 28/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 2.9272 - acc: 0.6731 - val_loss: 2.8396 - val_acc: 0.6790\n",
      "Epoch 29/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 2.7514 - acc: 0.6744 - val_loss: 2.6757 - val_acc: 0.6820\n",
      "Epoch 30/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 2.5964 - acc: 0.6743 - val_loss: 2.5303 - val_acc: 0.6860\n",
      "Epoch 31/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 2.4622 - acc: 0.6768 - val_loss: 2.4101 - val_acc: 0.6840\n",
      "Epoch 32/1000\n",
      "7500/7500 [==============================] - 0s 44us/step - loss: 2.3483 - acc: 0.6781 - val_loss: 2.3011 - val_acc: 0.6860\n",
      "Epoch 33/1000\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 2.2533 - acc: 0.6789 - val_loss: 2.2145 - val_acc: 0.6840\n",
      "Epoch 34/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 2.1767 - acc: 0.6801 - val_loss: 2.1502 - val_acc: 0.6940\n",
      "Epoch 35/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.1180 - acc: 0.6805 - val_loss: 2.0953 - val_acc: 0.6960\n",
      "Epoch 36/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 2.0744 - acc: 0.6796 - val_loss: 2.0583 - val_acc: 0.6920\n",
      "Epoch 37/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.0421 - acc: 0.6816 - val_loss: 2.0284 - val_acc: 0.6940\n",
      "Epoch 38/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 2.0157 - acc: 0.6813 - val_loss: 2.0038 - val_acc: 0.6980\n",
      "Epoch 39/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.9927 - acc: 0.6812 - val_loss: 1.9806 - val_acc: 0.7000\n",
      "Epoch 40/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.9722 - acc: 0.6832 - val_loss: 1.9604 - val_acc: 0.6940\n",
      "Epoch 41/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.9525 - acc: 0.6844 - val_loss: 1.9391 - val_acc: 0.7010\n",
      "Epoch 42/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.9340 - acc: 0.6853 - val_loss: 1.9242 - val_acc: 0.7000\n",
      "Epoch 43/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.9171 - acc: 0.6860 - val_loss: 1.9040 - val_acc: 0.7060\n",
      "Epoch 44/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.9005 - acc: 0.6857 - val_loss: 1.8869 - val_acc: 0.7030\n",
      "Epoch 45/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8844 - acc: 0.6871 - val_loss: 1.8711 - val_acc: 0.7030\n",
      "Epoch 46/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8696 - acc: 0.6883 - val_loss: 1.8577 - val_acc: 0.7030\n",
      "Epoch 47/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.8549 - acc: 0.6857 - val_loss: 1.8417 - val_acc: 0.7050\n",
      "Epoch 48/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8412 - acc: 0.6895 - val_loss: 1.8273 - val_acc: 0.7050\n",
      "Epoch 49/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.8278 - acc: 0.6877 - val_loss: 1.8176 - val_acc: 0.7030\n",
      "Epoch 50/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8144 - acc: 0.6891 - val_loss: 1.8024 - val_acc: 0.7060\n",
      "Epoch 51/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.8023 - acc: 0.6899 - val_loss: 1.7884 - val_acc: 0.7110\n",
      "Epoch 52/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.7899 - acc: 0.6896 - val_loss: 1.7766 - val_acc: 0.7030\n",
      "Epoch 53/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.7781 - acc: 0.6901 - val_loss: 1.7629 - val_acc: 0.7030\n",
      "Epoch 54/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.7665 - acc: 0.6904 - val_loss: 1.7514 - val_acc: 0.7050\n",
      "Epoch 55/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.7552 - acc: 0.6901 - val_loss: 1.7389 - val_acc: 0.7060\n",
      "Epoch 56/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.7453 - acc: 0.6912 - val_loss: 1.7285 - val_acc: 0.7080\n",
      "Epoch 57/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.7340 - acc: 0.6921 - val_loss: 1.7195 - val_acc: 0.7080\n",
      "Epoch 58/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.7240 - acc: 0.6936 - val_loss: 1.7081 - val_acc: 0.7090\n",
      "Epoch 59/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.7141 - acc: 0.6951 - val_loss: 1.7004 - val_acc: 0.7020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.7043 - acc: 0.6944 - val_loss: 1.6869 - val_acc: 0.7100\n",
      "Epoch 61/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6944 - acc: 0.6964 - val_loss: 1.6769 - val_acc: 0.7090\n",
      "Epoch 62/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6851 - acc: 0.6976 - val_loss: 1.6720 - val_acc: 0.7030\n",
      "Epoch 63/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6764 - acc: 0.6977 - val_loss: 1.6570 - val_acc: 0.7150\n",
      "Epoch 64/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6670 - acc: 0.6991 - val_loss: 1.6487 - val_acc: 0.7100\n",
      "Epoch 65/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6584 - acc: 0.6967 - val_loss: 1.6391 - val_acc: 0.7160\n",
      "Epoch 66/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6496 - acc: 0.6999 - val_loss: 1.6329 - val_acc: 0.7100\n",
      "Epoch 67/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6416 - acc: 0.6984 - val_loss: 1.6249 - val_acc: 0.7200\n",
      "Epoch 68/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6335 - acc: 0.6991 - val_loss: 1.6170 - val_acc: 0.7180\n",
      "Epoch 69/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6254 - acc: 0.7005 - val_loss: 1.6065 - val_acc: 0.7070\n",
      "Epoch 70/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.6170 - acc: 0.7012 - val_loss: 1.6020 - val_acc: 0.7180\n",
      "Epoch 71/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.6090 - acc: 0.7011 - val_loss: 1.5940 - val_acc: 0.7250\n",
      "Epoch 72/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.6023 - acc: 0.6993 - val_loss: 1.5849 - val_acc: 0.7180\n",
      "Epoch 73/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.5946 - acc: 0.7016 - val_loss: 1.5764 - val_acc: 0.7280\n",
      "Epoch 74/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5869 - acc: 0.7019 - val_loss: 1.5704 - val_acc: 0.7190\n",
      "Epoch 75/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5795 - acc: 0.7015 - val_loss: 1.5635 - val_acc: 0.7180\n",
      "Epoch 76/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5727 - acc: 0.7037 - val_loss: 1.5587 - val_acc: 0.7180\n",
      "Epoch 77/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5659 - acc: 0.7033 - val_loss: 1.5482 - val_acc: 0.7150\n",
      "Epoch 78/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5589 - acc: 0.7040 - val_loss: 1.5400 - val_acc: 0.7210\n",
      "Epoch 79/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5518 - acc: 0.7032 - val_loss: 1.5324 - val_acc: 0.7220\n",
      "Epoch 80/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5451 - acc: 0.7053 - val_loss: 1.5259 - val_acc: 0.7270\n",
      "Epoch 81/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5385 - acc: 0.7065 - val_loss: 1.5194 - val_acc: 0.7240\n",
      "Epoch 82/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5320 - acc: 0.7047 - val_loss: 1.5150 - val_acc: 0.7280\n",
      "Epoch 83/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5251 - acc: 0.7060 - val_loss: 1.5070 - val_acc: 0.7210\n",
      "Epoch 84/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5187 - acc: 0.7061 - val_loss: 1.5067 - val_acc: 0.7230\n",
      "Epoch 85/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5130 - acc: 0.7057 - val_loss: 1.4965 - val_acc: 0.7220\n",
      "Epoch 86/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.5066 - acc: 0.7068 - val_loss: 1.4990 - val_acc: 0.7190\n",
      "Epoch 87/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.5010 - acc: 0.7069 - val_loss: 1.4851 - val_acc: 0.7290\n",
      "Epoch 88/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4944 - acc: 0.7076 - val_loss: 1.4810 - val_acc: 0.7180\n",
      "Epoch 89/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.4882 - acc: 0.7081 - val_loss: 1.4717 - val_acc: 0.7270\n",
      "Epoch 90/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4824 - acc: 0.7072 - val_loss: 1.4663 - val_acc: 0.7310\n",
      "Epoch 91/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4765 - acc: 0.7104 - val_loss: 1.4594 - val_acc: 0.7270\n",
      "Epoch 92/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4706 - acc: 0.7096 - val_loss: 1.4553 - val_acc: 0.7280\n",
      "Epoch 93/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4649 - acc: 0.7083 - val_loss: 1.4484 - val_acc: 0.7240\n",
      "Epoch 94/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4591 - acc: 0.7092 - val_loss: 1.4457 - val_acc: 0.7310\n",
      "Epoch 95/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4535 - acc: 0.7115 - val_loss: 1.4408 - val_acc: 0.7260\n",
      "Epoch 96/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4483 - acc: 0.7085 - val_loss: 1.4316 - val_acc: 0.7270\n",
      "Epoch 97/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4434 - acc: 0.7107 - val_loss: 1.4261 - val_acc: 0.7290\n",
      "Epoch 98/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.4376 - acc: 0.7099 - val_loss: 1.4215 - val_acc: 0.7310\n",
      "Epoch 99/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.4322 - acc: 0.7119 - val_loss: 1.4149 - val_acc: 0.7310\n",
      "Epoch 100/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4270 - acc: 0.7108 - val_loss: 1.4123 - val_acc: 0.7300\n",
      "Epoch 101/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4218 - acc: 0.7112 - val_loss: 1.4063 - val_acc: 0.7240\n",
      "Epoch 102/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4168 - acc: 0.7104 - val_loss: 1.3991 - val_acc: 0.7290\n",
      "Epoch 103/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4115 - acc: 0.7127 - val_loss: 1.4017 - val_acc: 0.7330\n",
      "Epoch 104/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4069 - acc: 0.7129 - val_loss: 1.3942 - val_acc: 0.7280\n",
      "Epoch 105/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.4020 - acc: 0.7135 - val_loss: 1.3853 - val_acc: 0.7300\n",
      "Epoch 106/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.3976 - acc: 0.7127 - val_loss: 1.3808 - val_acc: 0.7320\n",
      "Epoch 107/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3918 - acc: 0.7129 - val_loss: 1.3783 - val_acc: 0.7290\n",
      "Epoch 108/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3874 - acc: 0.7132 - val_loss: 1.3705 - val_acc: 0.7310\n",
      "Epoch 109/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3826 - acc: 0.7128 - val_loss: 1.3754 - val_acc: 0.7310\n",
      "Epoch 110/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3784 - acc: 0.7136 - val_loss: 1.3643 - val_acc: 0.7310\n",
      "Epoch 111/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3734 - acc: 0.7152 - val_loss: 1.3598 - val_acc: 0.7300\n",
      "Epoch 112/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3692 - acc: 0.7135 - val_loss: 1.3541 - val_acc: 0.7340\n",
      "Epoch 113/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3643 - acc: 0.7145 - val_loss: 1.3492 - val_acc: 0.7320\n",
      "Epoch 114/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3596 - acc: 0.7160 - val_loss: 1.3491 - val_acc: 0.7300\n",
      "Epoch 115/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3555 - acc: 0.7167 - val_loss: 1.3407 - val_acc: 0.7340\n",
      "Epoch 116/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3507 - acc: 0.7179 - val_loss: 1.3418 - val_acc: 0.7290\n",
      "Epoch 117/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3469 - acc: 0.7160 - val_loss: 1.3334 - val_acc: 0.7330\n",
      "Epoch 118/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3422 - acc: 0.7165 - val_loss: 1.3328 - val_acc: 0.7370\n",
      "Epoch 119/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3383 - acc: 0.7183 - val_loss: 1.3272 - val_acc: 0.7330\n",
      "Epoch 120/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3333 - acc: 0.7175 - val_loss: 1.3204 - val_acc: 0.7350\n",
      "Epoch 121/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3295 - acc: 0.7172 - val_loss: 1.3139 - val_acc: 0.7370\n",
      "Epoch 122/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3249 - acc: 0.7191 - val_loss: 1.3147 - val_acc: 0.7300\n",
      "Epoch 123/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3211 - acc: 0.7192 - val_loss: 1.3100 - val_acc: 0.7390\n",
      "Epoch 124/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3169 - acc: 0.7183 - val_loss: 1.3094 - val_acc: 0.7330\n",
      "Epoch 125/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3135 - acc: 0.7180 - val_loss: 1.3033 - val_acc: 0.7370\n",
      "Epoch 126/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.3095 - acc: 0.7212 - val_loss: 1.2999 - val_acc: 0.7350\n",
      "Epoch 127/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3057 - acc: 0.7187 - val_loss: 1.2918 - val_acc: 0.7360\n",
      "Epoch 128/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.3015 - acc: 0.7204 - val_loss: 1.2891 - val_acc: 0.7330\n",
      "Epoch 129/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2977 - acc: 0.7235 - val_loss: 1.2836 - val_acc: 0.7350\n",
      "Epoch 130/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2940 - acc: 0.7225 - val_loss: 1.2838 - val_acc: 0.7400\n",
      "Epoch 131/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2903 - acc: 0.7199 - val_loss: 1.2800 - val_acc: 0.7340\n",
      "Epoch 132/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2866 - acc: 0.7220 - val_loss: 1.2761 - val_acc: 0.7360\n",
      "Epoch 133/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2831 - acc: 0.7225 - val_loss: 1.2774 - val_acc: 0.7360\n",
      "Epoch 134/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2794 - acc: 0.7219 - val_loss: 1.2674 - val_acc: 0.7400\n",
      "Epoch 135/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2756 - acc: 0.7219 - val_loss: 1.2634 - val_acc: 0.7400\n",
      "Epoch 136/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2724 - acc: 0.7212 - val_loss: 1.2633 - val_acc: 0.7370\n",
      "Epoch 137/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2687 - acc: 0.7231 - val_loss: 1.2574 - val_acc: 0.7410\n",
      "Epoch 138/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2656 - acc: 0.7247 - val_loss: 1.2556 - val_acc: 0.7330\n",
      "Epoch 139/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2622 - acc: 0.7241 - val_loss: 1.2521 - val_acc: 0.7340\n",
      "Epoch 140/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2583 - acc: 0.7224 - val_loss: 1.2460 - val_acc: 0.7360\n",
      "Epoch 141/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2552 - acc: 0.7260 - val_loss: 1.2511 - val_acc: 0.7330\n",
      "Epoch 142/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2516 - acc: 0.7252 - val_loss: 1.2422 - val_acc: 0.7380\n",
      "Epoch 143/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2486 - acc: 0.7239 - val_loss: 1.2384 - val_acc: 0.7380\n",
      "Epoch 144/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2454 - acc: 0.7252 - val_loss: 1.2359 - val_acc: 0.7340\n",
      "Epoch 145/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2418 - acc: 0.7256 - val_loss: 1.2334 - val_acc: 0.7350\n",
      "Epoch 146/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2395 - acc: 0.7261 - val_loss: 1.2298 - val_acc: 0.7300\n",
      "Epoch 147/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2354 - acc: 0.7271 - val_loss: 1.2267 - val_acc: 0.7350\n",
      "Epoch 148/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2327 - acc: 0.7272 - val_loss: 1.2288 - val_acc: 0.7390\n",
      "Epoch 149/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2301 - acc: 0.7263 - val_loss: 1.2222 - val_acc: 0.7450\n",
      "Epoch 150/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2265 - acc: 0.7279 - val_loss: 1.2170 - val_acc: 0.7390\n",
      "Epoch 151/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2231 - acc: 0.7261 - val_loss: 1.2136 - val_acc: 0.7390\n",
      "Epoch 152/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2206 - acc: 0.7268 - val_loss: 1.2164 - val_acc: 0.7380\n",
      "Epoch 153/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.2176 - acc: 0.7261 - val_loss: 1.2119 - val_acc: 0.7370\n",
      "Epoch 154/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2146 - acc: 0.7276 - val_loss: 1.2076 - val_acc: 0.7420\n",
      "Epoch 155/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2125 - acc: 0.7273 - val_loss: 1.2023 - val_acc: 0.7390\n",
      "Epoch 156/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2091 - acc: 0.7283 - val_loss: 1.2066 - val_acc: 0.7390\n",
      "Epoch 157/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2060 - acc: 0.7295 - val_loss: 1.2059 - val_acc: 0.7360\n",
      "Epoch 158/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.2035 - acc: 0.7280 - val_loss: 1.1947 - val_acc: 0.7480\n",
      "Epoch 159/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.2007 - acc: 0.7295 - val_loss: 1.1932 - val_acc: 0.7370\n",
      "Epoch 160/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1976 - acc: 0.7300 - val_loss: 1.1892 - val_acc: 0.7380\n",
      "Epoch 161/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1952 - acc: 0.7308 - val_loss: 1.1878 - val_acc: 0.7370\n",
      "Epoch 162/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1924 - acc: 0.7299 - val_loss: 1.1857 - val_acc: 0.7380\n",
      "Epoch 163/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.1904 - acc: 0.7325 - val_loss: 1.1843 - val_acc: 0.7420\n",
      "Epoch 164/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.1873 - acc: 0.7311 - val_loss: 1.1821 - val_acc: 0.7350\n",
      "Epoch 165/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1847 - acc: 0.7303 - val_loss: 1.1810 - val_acc: 0.7460\n",
      "Epoch 166/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1824 - acc: 0.7327 - val_loss: 1.1774 - val_acc: 0.7420\n",
      "Epoch 167/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.1806 - acc: 0.7303 - val_loss: 1.1789 - val_acc: 0.7460\n",
      "Epoch 168/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.1780 - acc: 0.7327 - val_loss: 1.1708 - val_acc: 0.7400\n",
      "Epoch 169/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1750 - acc: 0.7332 - val_loss: 1.1728 - val_acc: 0.7450\n",
      "Epoch 170/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.1729 - acc: 0.7313 - val_loss: 1.1699 - val_acc: 0.7380\n",
      "Epoch 171/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.1713 - acc: 0.7329 - val_loss: 1.1695 - val_acc: 0.7410\n",
      "Epoch 172/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.1688 - acc: 0.7339 - val_loss: 1.1640 - val_acc: 0.7370\n",
      "Epoch 173/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1668 - acc: 0.7329 - val_loss: 1.1616 - val_acc: 0.7440\n",
      "Epoch 174/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.1643 - acc: 0.7315 - val_loss: 1.1623 - val_acc: 0.7490\n",
      "Epoch 175/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1625 - acc: 0.7352 - val_loss: 1.1598 - val_acc: 0.7390\n",
      "Epoch 176/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.1599 - acc: 0.7328 - val_loss: 1.1578 - val_acc: 0.7390\n",
      "Epoch 177/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1585 - acc: 0.7313 - val_loss: 1.1581 - val_acc: 0.7460\n",
      "Epoch 178/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1565 - acc: 0.7349 - val_loss: 1.1528 - val_acc: 0.7470\n",
      "Epoch 179/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1539 - acc: 0.7351 - val_loss: 1.1529 - val_acc: 0.7420\n",
      "Epoch 180/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1519 - acc: 0.7363 - val_loss: 1.1502 - val_acc: 0.7410\n",
      "Epoch 181/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1507 - acc: 0.7344 - val_loss: 1.1491 - val_acc: 0.7480\n",
      "Epoch 182/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1487 - acc: 0.7367 - val_loss: 1.1444 - val_acc: 0.7460\n",
      "Epoch 183/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1463 - acc: 0.7349 - val_loss: 1.1498 - val_acc: 0.7380\n",
      "Epoch 184/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1449 - acc: 0.7347 - val_loss: 1.1568 - val_acc: 0.7460\n",
      "Epoch 185/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1440 - acc: 0.7364 - val_loss: 1.1415 - val_acc: 0.7420\n",
      "Epoch 186/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1415 - acc: 0.7361 - val_loss: 1.1449 - val_acc: 0.7460\n",
      "Epoch 187/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1397 - acc: 0.7352 - val_loss: 1.1377 - val_acc: 0.7460\n",
      "Epoch 188/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1381 - acc: 0.7356 - val_loss: 1.1374 - val_acc: 0.7450\n",
      "Epoch 189/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1365 - acc: 0.7369 - val_loss: 1.1382 - val_acc: 0.7400\n",
      "Epoch 190/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1350 - acc: 0.7355 - val_loss: 1.1337 - val_acc: 0.7400\n",
      "Epoch 191/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1330 - acc: 0.7357 - val_loss: 1.1318 - val_acc: 0.7410\n",
      "Epoch 192/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1311 - acc: 0.7389 - val_loss: 1.1286 - val_acc: 0.7480\n",
      "Epoch 193/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1300 - acc: 0.7383 - val_loss: 1.1316 - val_acc: 0.7440\n",
      "Epoch 194/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.1284 - acc: 0.7363 - val_loss: 1.1267 - val_acc: 0.7490\n",
      "Epoch 195/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.1268 - acc: 0.7379 - val_loss: 1.1269 - val_acc: 0.7410\n",
      "Epoch 196/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.1253 - acc: 0.7379 - val_loss: 1.1241 - val_acc: 0.7430\n",
      "Epoch 197/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.1240 - acc: 0.7392 - val_loss: 1.1219 - val_acc: 0.7440\n",
      "Epoch 198/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.1225 - acc: 0.7392 - val_loss: 1.1363 - val_acc: 0.7460\n",
      "Epoch 199/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.1225 - acc: 0.7385 - val_loss: 1.1280 - val_acc: 0.7480\n",
      "Epoch 200/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1203 - acc: 0.7379 - val_loss: 1.1203 - val_acc: 0.7460\n",
      "Epoch 201/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1177 - acc: 0.7387 - val_loss: 1.1164 - val_acc: 0.7480\n",
      "Epoch 202/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1165 - acc: 0.7419 - val_loss: 1.1167 - val_acc: 0.7500\n",
      "Epoch 203/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.1153 - acc: 0.7405 - val_loss: 1.1183 - val_acc: 0.7430\n",
      "Epoch 204/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1139 - acc: 0.7396 - val_loss: 1.1145 - val_acc: 0.7460\n",
      "Epoch 205/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.1121 - acc: 0.7383 - val_loss: 1.1179 - val_acc: 0.7440\n",
      "Epoch 206/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1114 - acc: 0.7412 - val_loss: 1.1120 - val_acc: 0.7510\n",
      "Epoch 207/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1102 - acc: 0.7400 - val_loss: 1.1104 - val_acc: 0.7490\n",
      "Epoch 208/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1081 - acc: 0.7393 - val_loss: 1.1119 - val_acc: 0.7420\n",
      "Epoch 209/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1081 - acc: 0.7397 - val_loss: 1.1107 - val_acc: 0.7440\n",
      "Epoch 210/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1056 - acc: 0.7376 - val_loss: 1.1093 - val_acc: 0.7470\n",
      "Epoch 211/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.1043 - acc: 0.7403 - val_loss: 1.1070 - val_acc: 0.7460\n",
      "Epoch 212/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.1028 - acc: 0.7409 - val_loss: 1.1053 - val_acc: 0.7490\n",
      "Epoch 213/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.1021 - acc: 0.7416 - val_loss: 1.1034 - val_acc: 0.7490\n",
      "Epoch 214/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.1002 - acc: 0.7411 - val_loss: 1.1018 - val_acc: 0.7520\n",
      "Epoch 215/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0993 - acc: 0.7413 - val_loss: 1.1009 - val_acc: 0.7460\n",
      "Epoch 216/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0979 - acc: 0.7407 - val_loss: 1.1009 - val_acc: 0.7480\n",
      "Epoch 217/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0971 - acc: 0.7404 - val_loss: 1.1004 - val_acc: 0.7450\n",
      "Epoch 218/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0955 - acc: 0.7408 - val_loss: 1.0990 - val_acc: 0.7550\n",
      "Epoch 219/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0942 - acc: 0.7416 - val_loss: 1.0968 - val_acc: 0.7460\n",
      "Epoch 220/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0937 - acc: 0.7400 - val_loss: 1.1026 - val_acc: 0.7430\n",
      "Epoch 221/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0923 - acc: 0.7433 - val_loss: 1.0940 - val_acc: 0.7470\n",
      "Epoch 222/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0908 - acc: 0.7432 - val_loss: 1.1020 - val_acc: 0.7500\n",
      "Epoch 223/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0899 - acc: 0.7424 - val_loss: 1.0956 - val_acc: 0.7490\n",
      "Epoch 224/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0885 - acc: 0.7421 - val_loss: 1.0935 - val_acc: 0.7450\n",
      "Epoch 225/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0878 - acc: 0.7420 - val_loss: 1.0939 - val_acc: 0.7430\n",
      "Epoch 226/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0863 - acc: 0.7412 - val_loss: 1.0960 - val_acc: 0.7450\n",
      "Epoch 227/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0856 - acc: 0.7397 - val_loss: 1.0933 - val_acc: 0.7470\n",
      "Epoch 228/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0847 - acc: 0.7425 - val_loss: 1.0896 - val_acc: 0.7490\n",
      "Epoch 229/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0826 - acc: 0.7408 - val_loss: 1.0878 - val_acc: 0.7460\n",
      "Epoch 230/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0830 - acc: 0.7412 - val_loss: 1.0898 - val_acc: 0.7490\n",
      "Epoch 231/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0813 - acc: 0.7437 - val_loss: 1.0855 - val_acc: 0.7430\n",
      "Epoch 232/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0793 - acc: 0.7428 - val_loss: 1.0885 - val_acc: 0.7490\n",
      "Epoch 233/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0789 - acc: 0.7451 - val_loss: 1.0838 - val_acc: 0.7490\n",
      "Epoch 234/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0775 - acc: 0.7459 - val_loss: 1.0810 - val_acc: 0.7470\n",
      "Epoch 235/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0761 - acc: 0.7436 - val_loss: 1.0822 - val_acc: 0.7480\n",
      "Epoch 236/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0754 - acc: 0.7451 - val_loss: 1.0806 - val_acc: 0.7520\n",
      "Epoch 237/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0744 - acc: 0.7452 - val_loss: 1.0798 - val_acc: 0.7530\n",
      "Epoch 238/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0728 - acc: 0.7445 - val_loss: 1.0771 - val_acc: 0.7550\n",
      "Epoch 239/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0720 - acc: 0.7447 - val_loss: 1.0786 - val_acc: 0.7520\n",
      "Epoch 240/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0716 - acc: 0.7445 - val_loss: 1.0789 - val_acc: 0.7500\n",
      "Epoch 241/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0699 - acc: 0.7435 - val_loss: 1.0765 - val_acc: 0.7500\n",
      "Epoch 242/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0691 - acc: 0.7448 - val_loss: 1.0765 - val_acc: 0.7530\n",
      "Epoch 243/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0678 - acc: 0.7456 - val_loss: 1.0778 - val_acc: 0.7520\n",
      "Epoch 244/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0672 - acc: 0.7447 - val_loss: 1.0721 - val_acc: 0.7540\n",
      "Epoch 245/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0660 - acc: 0.7453 - val_loss: 1.0878 - val_acc: 0.7430\n",
      "Epoch 246/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0663 - acc: 0.7456 - val_loss: 1.0720 - val_acc: 0.7550\n",
      "Epoch 247/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0645 - acc: 0.7469 - val_loss: 1.0742 - val_acc: 0.7520\n",
      "Epoch 248/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0641 - acc: 0.7461 - val_loss: 1.0753 - val_acc: 0.7510\n",
      "Epoch 249/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0625 - acc: 0.7468 - val_loss: 1.0752 - val_acc: 0.7550\n",
      "Epoch 250/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0616 - acc: 0.7443 - val_loss: 1.0673 - val_acc: 0.7540\n",
      "Epoch 251/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0602 - acc: 0.7469 - val_loss: 1.0691 - val_acc: 0.7550\n",
      "Epoch 252/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0597 - acc: 0.7456 - val_loss: 1.0658 - val_acc: 0.7590\n",
      "Epoch 253/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0579 - acc: 0.7473 - val_loss: 1.0661 - val_acc: 0.7550\n",
      "Epoch 254/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0575 - acc: 0.7467 - val_loss: 1.0706 - val_acc: 0.7580\n",
      "Epoch 255/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0573 - acc: 0.7489 - val_loss: 1.0662 - val_acc: 0.7570\n",
      "Epoch 256/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0566 - acc: 0.7485 - val_loss: 1.0663 - val_acc: 0.7530\n",
      "Epoch 257/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0547 - acc: 0.7484 - val_loss: 1.0663 - val_acc: 0.7510\n",
      "Epoch 258/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.0541 - acc: 0.7463 - val_loss: 1.0628 - val_acc: 0.7490\n",
      "Epoch 259/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0532 - acc: 0.7477 - val_loss: 1.0666 - val_acc: 0.7530\n",
      "Epoch 260/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.0526 - acc: 0.7481 - val_loss: 1.0598 - val_acc: 0.7530\n",
      "Epoch 261/1000\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 1.0512 - acc: 0.7495 - val_loss: 1.0654 - val_acc: 0.7540\n",
      "Epoch 262/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0504 - acc: 0.7484 - val_loss: 1.0631 - val_acc: 0.7520\n",
      "Epoch 263/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0495 - acc: 0.7477 - val_loss: 1.0690 - val_acc: 0.7500\n",
      "Epoch 264/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0500 - acc: 0.7479 - val_loss: 1.0604 - val_acc: 0.7540\n",
      "Epoch 265/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 1.0479 - acc: 0.7473 - val_loss: 1.0594 - val_acc: 0.7570\n",
      "Epoch 266/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0470 - acc: 0.7495 - val_loss: 1.0555 - val_acc: 0.7560\n",
      "Epoch 267/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0455 - acc: 0.7489 - val_loss: 1.0552 - val_acc: 0.7640\n",
      "Epoch 268/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0454 - acc: 0.7503 - val_loss: 1.0582 - val_acc: 0.7610\n",
      "Epoch 269/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0445 - acc: 0.7492 - val_loss: 1.0546 - val_acc: 0.7570\n",
      "Epoch 270/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0438 - acc: 0.7499 - val_loss: 1.0537 - val_acc: 0.7600\n",
      "Epoch 271/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0423 - acc: 0.7492 - val_loss: 1.0545 - val_acc: 0.7550\n",
      "Epoch 272/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0422 - acc: 0.7501 - val_loss: 1.0606 - val_acc: 0.7560\n",
      "Epoch 273/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0416 - acc: 0.7508 - val_loss: 1.0544 - val_acc: 0.7510\n",
      "Epoch 274/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0399 - acc: 0.7495 - val_loss: 1.0518 - val_acc: 0.7560\n",
      "Epoch 275/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0398 - acc: 0.7485 - val_loss: 1.0555 - val_acc: 0.7580\n",
      "Epoch 276/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0394 - acc: 0.7525 - val_loss: 1.0514 - val_acc: 0.7590\n",
      "Epoch 277/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0379 - acc: 0.7488 - val_loss: 1.0506 - val_acc: 0.7580\n",
      "Epoch 278/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0379 - acc: 0.7513 - val_loss: 1.0486 - val_acc: 0.7600\n",
      "Epoch 279/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0365 - acc: 0.7500 - val_loss: 1.0504 - val_acc: 0.7590\n",
      "Epoch 280/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0359 - acc: 0.7509 - val_loss: 1.0452 - val_acc: 0.7550\n",
      "Epoch 281/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0351 - acc: 0.7517 - val_loss: 1.0452 - val_acc: 0.7530\n",
      "Epoch 282/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0346 - acc: 0.7536 - val_loss: 1.0668 - val_acc: 0.7580\n",
      "Epoch 283/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0339 - acc: 0.7520 - val_loss: 1.0462 - val_acc: 0.7520\n",
      "Epoch 284/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0320 - acc: 0.7535 - val_loss: 1.0422 - val_acc: 0.7520\n",
      "Epoch 285/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0319 - acc: 0.7523 - val_loss: 1.0436 - val_acc: 0.7600\n",
      "Epoch 286/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0309 - acc: 0.7500 - val_loss: 1.0417 - val_acc: 0.7590\n",
      "Epoch 287/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0303 - acc: 0.7524 - val_loss: 1.0415 - val_acc: 0.7580\n",
      "Epoch 288/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0298 - acc: 0.7536 - val_loss: 1.0433 - val_acc: 0.7560\n",
      "Epoch 289/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0284 - acc: 0.7517 - val_loss: 1.0432 - val_acc: 0.7500\n",
      "Epoch 290/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.0277 - acc: 0.7528 - val_loss: 1.0432 - val_acc: 0.7640\n",
      "Epoch 291/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0271 - acc: 0.7523 - val_loss: 1.0397 - val_acc: 0.7610\n",
      "Epoch 292/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.0262 - acc: 0.7548 - val_loss: 1.0396 - val_acc: 0.7590\n",
      "Epoch 293/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0253 - acc: 0.7541 - val_loss: 1.0398 - val_acc: 0.7560\n",
      "Epoch 294/1000\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.0250 - acc: 0.7529 - val_loss: 1.0395 - val_acc: 0.7600\n",
      "Epoch 295/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.0252 - acc: 0.7544 - val_loss: 1.0363 - val_acc: 0.7630\n",
      "Epoch 296/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0229 - acc: 0.7535 - val_loss: 1.0382 - val_acc: 0.7620\n",
      "Epoch 297/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.0228 - acc: 0.7537 - val_loss: 1.0396 - val_acc: 0.7650\n",
      "Epoch 298/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.0228 - acc: 0.7529 - val_loss: 1.0331 - val_acc: 0.7590\n",
      "Epoch 299/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0218 - acc: 0.7561 - val_loss: 1.0346 - val_acc: 0.7580\n",
      "Epoch 300/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0211 - acc: 0.7544 - val_loss: 1.0390 - val_acc: 0.7580\n",
      "Epoch 301/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0204 - acc: 0.7564 - val_loss: 1.0316 - val_acc: 0.7610\n",
      "Epoch 302/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 1.0193 - acc: 0.7572 - val_loss: 1.0320 - val_acc: 0.7590\n",
      "Epoch 303/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 1.0194 - acc: 0.7543 - val_loss: 1.0346 - val_acc: 0.7620\n",
      "Epoch 304/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0182 - acc: 0.7537 - val_loss: 1.0352 - val_acc: 0.7540\n",
      "Epoch 305/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0181 - acc: 0.7548 - val_loss: 1.0361 - val_acc: 0.7530\n",
      "Epoch 306/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0172 - acc: 0.7563 - val_loss: 1.0299 - val_acc: 0.7650\n",
      "Epoch 307/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0158 - acc: 0.7560 - val_loss: 1.0362 - val_acc: 0.7630\n",
      "Epoch 308/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0155 - acc: 0.7549 - val_loss: 1.0296 - val_acc: 0.7600\n",
      "Epoch 309/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 1.0145 - acc: 0.7547 - val_loss: 1.0327 - val_acc: 0.7620\n",
      "Epoch 310/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 1.0143 - acc: 0.7544 - val_loss: 1.0266 - val_acc: 0.7610\n",
      "Epoch 311/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0131 - acc: 0.7564 - val_loss: 1.0268 - val_acc: 0.7640\n",
      "Epoch 312/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0132 - acc: 0.7559 - val_loss: 1.0288 - val_acc: 0.7600\n",
      "Epoch 313/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0125 - acc: 0.7579 - val_loss: 1.0262 - val_acc: 0.7650\n",
      "Epoch 314/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0120 - acc: 0.7560 - val_loss: 1.0345 - val_acc: 0.7540\n",
      "Epoch 315/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0112 - acc: 0.7580 - val_loss: 1.0269 - val_acc: 0.7590\n",
      "Epoch 316/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0101 - acc: 0.7568 - val_loss: 1.0238 - val_acc: 0.7620\n",
      "Epoch 317/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0098 - acc: 0.7556 - val_loss: 1.0276 - val_acc: 0.7600\n",
      "Epoch 318/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0092 - acc: 0.7559 - val_loss: 1.0335 - val_acc: 0.7620\n",
      "Epoch 319/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0087 - acc: 0.7564 - val_loss: 1.0232 - val_acc: 0.7570\n",
      "Epoch 320/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0082 - acc: 0.7556 - val_loss: 1.0263 - val_acc: 0.7570\n",
      "Epoch 321/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0075 - acc: 0.7565 - val_loss: 1.0239 - val_acc: 0.7610\n",
      "Epoch 322/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0073 - acc: 0.7564 - val_loss: 1.0224 - val_acc: 0.7610\n",
      "Epoch 323/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0068 - acc: 0.7553 - val_loss: 1.0214 - val_acc: 0.7580\n",
      "Epoch 324/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0049 - acc: 0.7596 - val_loss: 1.0218 - val_acc: 0.7570\n",
      "Epoch 325/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0048 - acc: 0.7567 - val_loss: 1.0208 - val_acc: 0.7620\n",
      "Epoch 326/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0042 - acc: 0.7551 - val_loss: 1.0245 - val_acc: 0.7620\n",
      "Epoch 327/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 1.0044 - acc: 0.7568 - val_loss: 1.0168 - val_acc: 0.7610\n",
      "Epoch 328/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 1.0027 - acc: 0.7572 - val_loss: 1.0209 - val_acc: 0.7600\n",
      "Epoch 329/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0027 - acc: 0.7591 - val_loss: 1.0161 - val_acc: 0.7630\n",
      "Epoch 330/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0022 - acc: 0.7555 - val_loss: 1.0164 - val_acc: 0.7620\n",
      "Epoch 331/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0008 - acc: 0.7559 - val_loss: 1.0195 - val_acc: 0.7630\n",
      "Epoch 332/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 1.0001 - acc: 0.7571 - val_loss: 1.0194 - val_acc: 0.7600\n",
      "Epoch 333/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9996 - acc: 0.7563 - val_loss: 1.0262 - val_acc: 0.7530\n",
      "Epoch 334/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9996 - acc: 0.7591 - val_loss: 1.0293 - val_acc: 0.7620\n",
      "Epoch 335/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9992 - acc: 0.7572 - val_loss: 1.0223 - val_acc: 0.7580\n",
      "Epoch 336/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9981 - acc: 0.7572 - val_loss: 1.0242 - val_acc: 0.7620\n",
      "Epoch 337/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9992 - acc: 0.7593 - val_loss: 1.0175 - val_acc: 0.7650\n",
      "Epoch 338/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9971 - acc: 0.7585 - val_loss: 1.0156 - val_acc: 0.7570\n",
      "Epoch 339/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9971 - acc: 0.7573 - val_loss: 1.0206 - val_acc: 0.7570\n",
      "Epoch 340/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9967 - acc: 0.7604 - val_loss: 1.0137 - val_acc: 0.7590\n",
      "Epoch 341/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9950 - acc: 0.7592 - val_loss: 1.0141 - val_acc: 0.7570\n",
      "Epoch 342/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9956 - acc: 0.7585 - val_loss: 1.0103 - val_acc: 0.7600\n",
      "Epoch 343/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9944 - acc: 0.7600 - val_loss: 1.0202 - val_acc: 0.7670\n",
      "Epoch 344/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9946 - acc: 0.7593 - val_loss: 1.0237 - val_acc: 0.7530\n",
      "Epoch 345/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9938 - acc: 0.7580 - val_loss: 1.0147 - val_acc: 0.7550\n",
      "Epoch 346/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9928 - acc: 0.7603 - val_loss: 1.0110 - val_acc: 0.7670\n",
      "Epoch 347/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9925 - acc: 0.7595 - val_loss: 1.0112 - val_acc: 0.7600\n",
      "Epoch 348/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9910 - acc: 0.7592 - val_loss: 1.0077 - val_acc: 0.7670\n",
      "Epoch 349/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9911 - acc: 0.7600 - val_loss: 1.0189 - val_acc: 0.7500\n",
      "Epoch 350/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9904 - acc: 0.7600 - val_loss: 1.0112 - val_acc: 0.7570\n",
      "Epoch 351/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9898 - acc: 0.7588 - val_loss: 1.0092 - val_acc: 0.7600\n",
      "Epoch 352/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9885 - acc: 0.7580 - val_loss: 1.0081 - val_acc: 0.7550\n",
      "Epoch 353/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9884 - acc: 0.7596 - val_loss: 1.0066 - val_acc: 0.7680\n",
      "Epoch 354/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9878 - acc: 0.7601 - val_loss: 1.0073 - val_acc: 0.7570\n",
      "Epoch 355/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9875 - acc: 0.7593 - val_loss: 1.0041 - val_acc: 0.7610\n",
      "Epoch 356/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9877 - acc: 0.7585 - val_loss: 1.0122 - val_acc: 0.7620\n",
      "Epoch 357/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9865 - acc: 0.7612 - val_loss: 1.0099 - val_acc: 0.7500\n",
      "Epoch 358/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9865 - acc: 0.7623 - val_loss: 1.0075 - val_acc: 0.7620\n",
      "Epoch 359/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9858 - acc: 0.7608 - val_loss: 1.0100 - val_acc: 0.7600\n",
      "Epoch 360/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9852 - acc: 0.7609 - val_loss: 1.0010 - val_acc: 0.7640\n",
      "Epoch 361/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9841 - acc: 0.7600 - val_loss: 1.0032 - val_acc: 0.7700\n",
      "Epoch 362/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9842 - acc: 0.7577 - val_loss: 1.0032 - val_acc: 0.7720\n",
      "Epoch 363/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9832 - acc: 0.7593 - val_loss: 1.0030 - val_acc: 0.7580\n",
      "Epoch 364/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9830 - acc: 0.7593 - val_loss: 0.9987 - val_acc: 0.7620\n",
      "Epoch 365/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9819 - acc: 0.7603 - val_loss: 1.0031 - val_acc: 0.7610\n",
      "Epoch 366/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9815 - acc: 0.7600 - val_loss: 1.0065 - val_acc: 0.7560\n",
      "Epoch 367/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9815 - acc: 0.7620 - val_loss: 1.0075 - val_acc: 0.7590\n",
      "Epoch 368/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9807 - acc: 0.7613 - val_loss: 1.0006 - val_acc: 0.7580\n",
      "Epoch 369/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9800 - acc: 0.7627 - val_loss: 1.0044 - val_acc: 0.7610\n",
      "Epoch 370/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9793 - acc: 0.7631 - val_loss: 1.0036 - val_acc: 0.7540\n",
      "Epoch 371/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9790 - acc: 0.7612 - val_loss: 1.0007 - val_acc: 0.7700\n",
      "Epoch 372/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9790 - acc: 0.7621 - val_loss: 1.0032 - val_acc: 0.7590\n",
      "Epoch 373/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9786 - acc: 0.7617 - val_loss: 0.9984 - val_acc: 0.7630\n",
      "Epoch 374/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9769 - acc: 0.7631 - val_loss: 0.9998 - val_acc: 0.7690\n",
      "Epoch 375/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9771 - acc: 0.7640 - val_loss: 0.9967 - val_acc: 0.7630\n",
      "Epoch 376/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9764 - acc: 0.7615 - val_loss: 1.0013 - val_acc: 0.7630\n",
      "Epoch 377/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9764 - acc: 0.7623 - val_loss: 1.0020 - val_acc: 0.7550\n",
      "Epoch 378/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9757 - acc: 0.7597 - val_loss: 0.9957 - val_acc: 0.7650\n",
      "Epoch 379/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9755 - acc: 0.7591 - val_loss: 1.0000 - val_acc: 0.7650\n",
      "Epoch 380/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9755 - acc: 0.7613 - val_loss: 0.9962 - val_acc: 0.7630\n",
      "Epoch 381/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9743 - acc: 0.7629 - val_loss: 0.9943 - val_acc: 0.7610\n",
      "Epoch 382/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.9731 - acc: 0.7657 - val_loss: 0.9952 - val_acc: 0.7660\n",
      "Epoch 383/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9730 - acc: 0.7637 - val_loss: 0.9920 - val_acc: 0.7640\n",
      "Epoch 384/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9732 - acc: 0.7603 - val_loss: 0.9994 - val_acc: 0.7660\n",
      "Epoch 385/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9729 - acc: 0.7609 - val_loss: 1.0079 - val_acc: 0.7550\n",
      "Epoch 386/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9729 - acc: 0.7585 - val_loss: 0.9907 - val_acc: 0.7640\n",
      "Epoch 387/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9705 - acc: 0.7613 - val_loss: 0.9939 - val_acc: 0.7610\n",
      "Epoch 388/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9706 - acc: 0.7621 - val_loss: 0.9946 - val_acc: 0.7590\n",
      "Epoch 389/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9702 - acc: 0.7615 - val_loss: 0.9965 - val_acc: 0.7570\n",
      "Epoch 390/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9699 - acc: 0.7599 - val_loss: 0.9922 - val_acc: 0.7620\n",
      "Epoch 391/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9688 - acc: 0.7632 - val_loss: 0.9904 - val_acc: 0.7610\n",
      "Epoch 392/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9683 - acc: 0.7624 - val_loss: 0.9993 - val_acc: 0.7610\n",
      "Epoch 393/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9699 - acc: 0.7636 - val_loss: 0.9898 - val_acc: 0.7670\n",
      "Epoch 394/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9676 - acc: 0.7636 - val_loss: 0.9944 - val_acc: 0.7610\n",
      "Epoch 395/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9673 - acc: 0.7631 - val_loss: 0.9915 - val_acc: 0.7600\n",
      "Epoch 396/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9668 - acc: 0.7637 - val_loss: 0.9884 - val_acc: 0.7660\n",
      "Epoch 397/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9666 - acc: 0.7636 - val_loss: 0.9870 - val_acc: 0.7660\n",
      "Epoch 398/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9651 - acc: 0.7629 - val_loss: 0.9888 - val_acc: 0.7650\n",
      "Epoch 399/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9650 - acc: 0.7629 - val_loss: 0.9922 - val_acc: 0.7640\n",
      "Epoch 400/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9647 - acc: 0.7621 - val_loss: 0.9913 - val_acc: 0.7610\n",
      "Epoch 401/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9648 - acc: 0.7635 - val_loss: 0.9884 - val_acc: 0.7590\n",
      "Epoch 402/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9642 - acc: 0.7640 - val_loss: 0.9876 - val_acc: 0.7590\n",
      "Epoch 403/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9638 - acc: 0.7644 - val_loss: 0.9856 - val_acc: 0.7650\n",
      "Epoch 404/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9643 - acc: 0.7625 - val_loss: 0.9837 - val_acc: 0.7650\n",
      "Epoch 405/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9623 - acc: 0.7628 - val_loss: 0.9886 - val_acc: 0.7560\n",
      "Epoch 406/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9621 - acc: 0.7639 - val_loss: 0.9869 - val_acc: 0.7550\n",
      "Epoch 407/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9618 - acc: 0.7643 - val_loss: 0.9957 - val_acc: 0.7590\n",
      "Epoch 408/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9608 - acc: 0.7640 - val_loss: 0.9906 - val_acc: 0.7620\n",
      "Epoch 409/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9607 - acc: 0.7645 - val_loss: 0.9819 - val_acc: 0.7640\n",
      "Epoch 410/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9598 - acc: 0.7641 - val_loss: 0.9877 - val_acc: 0.7600\n",
      "Epoch 411/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9595 - acc: 0.7659 - val_loss: 0.9812 - val_acc: 0.7670\n",
      "Epoch 412/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9595 - acc: 0.7617 - val_loss: 0.9820 - val_acc: 0.7630\n",
      "Epoch 413/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9585 - acc: 0.7663 - val_loss: 0.9831 - val_acc: 0.7640\n",
      "Epoch 414/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9592 - acc: 0.7655 - val_loss: 0.9834 - val_acc: 0.7600\n",
      "Epoch 415/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9577 - acc: 0.7667 - val_loss: 0.9826 - val_acc: 0.7630\n",
      "Epoch 416/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9572 - acc: 0.7668 - val_loss: 0.9811 - val_acc: 0.7640\n",
      "Epoch 417/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9574 - acc: 0.7627 - val_loss: 0.9820 - val_acc: 0.7610\n",
      "Epoch 418/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9566 - acc: 0.7643 - val_loss: 0.9837 - val_acc: 0.7620\n",
      "Epoch 419/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9559 - acc: 0.7637 - val_loss: 0.9800 - val_acc: 0.7620\n",
      "Epoch 420/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9558 - acc: 0.7651 - val_loss: 0.9799 - val_acc: 0.7550\n",
      "Epoch 421/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9555 - acc: 0.7631 - val_loss: 0.9789 - val_acc: 0.7650\n",
      "Epoch 422/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9553 - acc: 0.7648 - val_loss: 0.9801 - val_acc: 0.7660\n",
      "Epoch 423/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9548 - acc: 0.7665 - val_loss: 0.9800 - val_acc: 0.7640\n",
      "Epoch 424/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9530 - acc: 0.7675 - val_loss: 0.9908 - val_acc: 0.7490\n",
      "Epoch 425/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9535 - acc: 0.7663 - val_loss: 0.9754 - val_acc: 0.7640\n",
      "Epoch 426/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9532 - acc: 0.7663 - val_loss: 0.9949 - val_acc: 0.7620\n",
      "Epoch 427/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9529 - acc: 0.7649 - val_loss: 0.9800 - val_acc: 0.7620\n",
      "Epoch 428/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9522 - acc: 0.7657 - val_loss: 0.9797 - val_acc: 0.7680\n",
      "Epoch 429/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9518 - acc: 0.7647 - val_loss: 0.9879 - val_acc: 0.7550\n",
      "Epoch 430/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9521 - acc: 0.7645 - val_loss: 0.9796 - val_acc: 0.7630\n",
      "Epoch 431/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9515 - acc: 0.7653 - val_loss: 0.9756 - val_acc: 0.7620\n",
      "Epoch 432/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9499 - acc: 0.7655 - val_loss: 0.9763 - val_acc: 0.7570\n",
      "Epoch 433/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9501 - acc: 0.7672 - val_loss: 0.9766 - val_acc: 0.7630\n",
      "Epoch 434/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9495 - acc: 0.7648 - val_loss: 0.9754 - val_acc: 0.7660\n",
      "Epoch 435/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9484 - acc: 0.7668 - val_loss: 0.9751 - val_acc: 0.7650\n",
      "Epoch 436/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9488 - acc: 0.7653 - val_loss: 0.9779 - val_acc: 0.7650\n",
      "Epoch 437/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9490 - acc: 0.7663 - val_loss: 0.9804 - val_acc: 0.7550\n",
      "Epoch 438/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9478 - acc: 0.7648 - val_loss: 0.9944 - val_acc: 0.7560\n",
      "Epoch 439/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9482 - acc: 0.7657 - val_loss: 0.9732 - val_acc: 0.7630\n",
      "Epoch 440/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9469 - acc: 0.7648 - val_loss: 0.9788 - val_acc: 0.7590\n",
      "Epoch 441/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9463 - acc: 0.7671 - val_loss: 0.9714 - val_acc: 0.7610\n",
      "Epoch 442/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9458 - acc: 0.7655 - val_loss: 0.9779 - val_acc: 0.7650\n",
      "Epoch 443/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9458 - acc: 0.7672 - val_loss: 0.9789 - val_acc: 0.7570\n",
      "Epoch 444/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9447 - acc: 0.7676 - val_loss: 0.9711 - val_acc: 0.7670\n",
      "Epoch 445/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9451 - acc: 0.7675 - val_loss: 0.9691 - val_acc: 0.7680\n",
      "Epoch 446/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9443 - acc: 0.7673 - val_loss: 0.9709 - val_acc: 0.7620\n",
      "Epoch 447/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9444 - acc: 0.7657 - val_loss: 0.9706 - val_acc: 0.7590\n",
      "Epoch 448/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9431 - acc: 0.7687 - val_loss: 0.9732 - val_acc: 0.7610\n",
      "Epoch 449/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9433 - acc: 0.7660 - val_loss: 0.9683 - val_acc: 0.7670\n",
      "Epoch 450/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9425 - acc: 0.7672 - val_loss: 0.9728 - val_acc: 0.7640\n",
      "Epoch 451/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.9434 - acc: 0.7655 - val_loss: 0.9666 - val_acc: 0.7630\n",
      "Epoch 452/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9411 - acc: 0.7672 - val_loss: 0.9694 - val_acc: 0.7620\n",
      "Epoch 453/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9407 - acc: 0.7651 - val_loss: 0.9712 - val_acc: 0.7660\n",
      "Epoch 454/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9404 - acc: 0.7673 - val_loss: 0.9734 - val_acc: 0.7570\n",
      "Epoch 455/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9400 - acc: 0.7671 - val_loss: 0.9704 - val_acc: 0.7630\n",
      "Epoch 456/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9402 - acc: 0.7671 - val_loss: 0.9710 - val_acc: 0.7570\n",
      "Epoch 457/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9389 - acc: 0.7667 - val_loss: 0.9674 - val_acc: 0.7630\n",
      "Epoch 458/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9387 - acc: 0.7685 - val_loss: 0.9814 - val_acc: 0.7650\n",
      "Epoch 459/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9384 - acc: 0.7661 - val_loss: 0.9687 - val_acc: 0.7610\n",
      "Epoch 460/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9383 - acc: 0.7661 - val_loss: 0.9658 - val_acc: 0.7610\n",
      "Epoch 461/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9382 - acc: 0.7660 - val_loss: 0.9701 - val_acc: 0.7620\n",
      "Epoch 462/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9376 - acc: 0.7691 - val_loss: 0.9644 - val_acc: 0.7680\n",
      "Epoch 463/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9374 - acc: 0.7689 - val_loss: 0.9716 - val_acc: 0.7620\n",
      "Epoch 464/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9362 - acc: 0.7664 - val_loss: 0.9651 - val_acc: 0.7610\n",
      "Epoch 465/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9362 - acc: 0.7707 - val_loss: 0.9662 - val_acc: 0.7630\n",
      "Epoch 466/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9351 - acc: 0.7683 - val_loss: 0.9623 - val_acc: 0.7670\n",
      "Epoch 467/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9356 - acc: 0.7668 - val_loss: 0.9614 - val_acc: 0.7660\n",
      "Epoch 468/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9348 - acc: 0.7696 - val_loss: 0.9722 - val_acc: 0.7680\n",
      "Epoch 469/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9354 - acc: 0.7677 - val_loss: 0.9617 - val_acc: 0.7640\n",
      "Epoch 470/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9344 - acc: 0.7684 - val_loss: 0.9700 - val_acc: 0.7550\n",
      "Epoch 471/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9349 - acc: 0.7671 - val_loss: 0.9762 - val_acc: 0.7650\n",
      "Epoch 472/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9344 - acc: 0.7711 - val_loss: 0.9695 - val_acc: 0.7530\n",
      "Epoch 473/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9337 - acc: 0.7675 - val_loss: 0.9674 - val_acc: 0.7750\n",
      "Epoch 474/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9332 - acc: 0.7673 - val_loss: 0.9618 - val_acc: 0.7680\n",
      "Epoch 475/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9316 - acc: 0.7692 - val_loss: 0.9592 - val_acc: 0.7660\n",
      "Epoch 476/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9321 - acc: 0.7692 - val_loss: 0.9579 - val_acc: 0.7660\n",
      "Epoch 477/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9308 - acc: 0.7707 - val_loss: 0.9676 - val_acc: 0.7590\n",
      "Epoch 478/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9312 - acc: 0.7680 - val_loss: 0.9577 - val_acc: 0.7640\n",
      "Epoch 479/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9315 - acc: 0.7676 - val_loss: 0.9568 - val_acc: 0.7640\n",
      "Epoch 480/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9301 - acc: 0.7673 - val_loss: 0.9598 - val_acc: 0.7650\n",
      "Epoch 481/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9290 - acc: 0.7697 - val_loss: 0.9710 - val_acc: 0.7590\n",
      "Epoch 482/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9293 - acc: 0.7673 - val_loss: 0.9617 - val_acc: 0.7640\n",
      "Epoch 483/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9283 - acc: 0.7689 - val_loss: 0.9587 - val_acc: 0.7680\n",
      "Epoch 484/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9290 - acc: 0.7677 - val_loss: 0.9606 - val_acc: 0.7640\n",
      "Epoch 485/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9287 - acc: 0.7709 - val_loss: 0.9601 - val_acc: 0.7700\n",
      "Epoch 486/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9278 - acc: 0.7685 - val_loss: 0.9609 - val_acc: 0.7680\n",
      "Epoch 487/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.9281 - acc: 0.7676 - val_loss: 0.9711 - val_acc: 0.7590\n",
      "Epoch 488/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9270 - acc: 0.7695 - val_loss: 0.9560 - val_acc: 0.7670\n",
      "Epoch 489/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9279 - acc: 0.7689 - val_loss: 0.9610 - val_acc: 0.7650\n",
      "Epoch 490/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9268 - acc: 0.7705 - val_loss: 0.9546 - val_acc: 0.7650\n",
      "Epoch 491/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9266 - acc: 0.7709 - val_loss: 0.9571 - val_acc: 0.7640\n",
      "Epoch 492/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9258 - acc: 0.7699 - val_loss: 0.9527 - val_acc: 0.7680\n",
      "Epoch 493/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9239 - acc: 0.7712 - val_loss: 0.9558 - val_acc: 0.7690\n",
      "Epoch 494/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9239 - acc: 0.7687 - val_loss: 0.9532 - val_acc: 0.7680\n",
      "Epoch 495/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9244 - acc: 0.7695 - val_loss: 0.9586 - val_acc: 0.7660\n",
      "Epoch 496/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9246 - acc: 0.7705 - val_loss: 0.9515 - val_acc: 0.7670\n",
      "Epoch 497/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9236 - acc: 0.7696 - val_loss: 0.9692 - val_acc: 0.7670\n",
      "Epoch 498/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9235 - acc: 0.7685 - val_loss: 0.9679 - val_acc: 0.7560\n",
      "Epoch 499/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9230 - acc: 0.7743 - val_loss: 0.9552 - val_acc: 0.7640\n",
      "Epoch 500/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9220 - acc: 0.7713 - val_loss: 0.9586 - val_acc: 0.7680\n",
      "Epoch 501/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9226 - acc: 0.7700 - val_loss: 0.9500 - val_acc: 0.7670\n",
      "Epoch 502/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9222 - acc: 0.7689 - val_loss: 0.9593 - val_acc: 0.7640\n",
      "Epoch 503/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9208 - acc: 0.7716 - val_loss: 0.9555 - val_acc: 0.7630\n",
      "Epoch 504/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9204 - acc: 0.7708 - val_loss: 0.9610 - val_acc: 0.7670\n",
      "Epoch 505/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9216 - acc: 0.7689 - val_loss: 0.9540 - val_acc: 0.7670\n",
      "Epoch 506/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9200 - acc: 0.7701 - val_loss: 0.9507 - val_acc: 0.7630\n",
      "Epoch 507/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9194 - acc: 0.7713 - val_loss: 0.9578 - val_acc: 0.7580\n",
      "Epoch 508/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9196 - acc: 0.7707 - val_loss: 0.9484 - val_acc: 0.7630\n",
      "Epoch 509/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9189 - acc: 0.7700 - val_loss: 0.9540 - val_acc: 0.7630\n",
      "Epoch 510/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9180 - acc: 0.7712 - val_loss: 0.9513 - val_acc: 0.7660\n",
      "Epoch 511/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.9193 - acc: 0.7713 - val_loss: 0.9524 - val_acc: 0.7720\n",
      "Epoch 512/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9180 - acc: 0.7700 - val_loss: 0.9511 - val_acc: 0.7660\n",
      "Epoch 513/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9174 - acc: 0.7719 - val_loss: 0.9506 - val_acc: 0.7650\n",
      "Epoch 514/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9171 - acc: 0.7691 - val_loss: 0.9523 - val_acc: 0.7650\n",
      "Epoch 515/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9167 - acc: 0.7712 - val_loss: 0.9479 - val_acc: 0.7640\n",
      "Epoch 516/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9168 - acc: 0.7737 - val_loss: 0.9489 - val_acc: 0.7630\n",
      "Epoch 517/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9155 - acc: 0.7685 - val_loss: 0.9472 - val_acc: 0.7620\n",
      "Epoch 518/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9158 - acc: 0.7717 - val_loss: 0.9637 - val_acc: 0.7570\n",
      "Epoch 519/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9152 - acc: 0.7729 - val_loss: 0.9624 - val_acc: 0.7620\n",
      "Epoch 520/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9163 - acc: 0.7712 - val_loss: 0.9497 - val_acc: 0.7640\n",
      "Epoch 521/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9153 - acc: 0.7708 - val_loss: 0.9515 - val_acc: 0.7620\n",
      "Epoch 522/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9135 - acc: 0.7719 - val_loss: 0.9488 - val_acc: 0.7650\n",
      "Epoch 523/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9137 - acc: 0.7725 - val_loss: 0.9582 - val_acc: 0.7600\n",
      "Epoch 524/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9141 - acc: 0.7737 - val_loss: 0.9476 - val_acc: 0.7700\n",
      "Epoch 525/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9132 - acc: 0.7712 - val_loss: 0.9522 - val_acc: 0.7700\n",
      "Epoch 526/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9125 - acc: 0.7716 - val_loss: 0.9456 - val_acc: 0.7720\n",
      "Epoch 527/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9120 - acc: 0.7736 - val_loss: 0.9559 - val_acc: 0.7620\n",
      "Epoch 528/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9124 - acc: 0.7736 - val_loss: 0.9505 - val_acc: 0.7680\n",
      "Epoch 529/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9119 - acc: 0.7732 - val_loss: 0.9443 - val_acc: 0.7690\n",
      "Epoch 530/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9106 - acc: 0.7712 - val_loss: 0.9469 - val_acc: 0.7700\n",
      "Epoch 531/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9130 - acc: 0.7701 - val_loss: 0.9477 - val_acc: 0.7630\n",
      "Epoch 532/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9116 - acc: 0.7736 - val_loss: 0.9479 - val_acc: 0.7690\n",
      "Epoch 533/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9112 - acc: 0.7725 - val_loss: 0.9474 - val_acc: 0.7680\n",
      "Epoch 534/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9101 - acc: 0.7743 - val_loss: 0.9450 - val_acc: 0.7670\n",
      "Epoch 535/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9098 - acc: 0.7733 - val_loss: 0.9445 - val_acc: 0.7670\n",
      "Epoch 536/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9102 - acc: 0.7717 - val_loss: 0.9409 - val_acc: 0.7680\n",
      "Epoch 537/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9091 - acc: 0.7733 - val_loss: 0.9468 - val_acc: 0.7630\n",
      "Epoch 538/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9080 - acc: 0.7712 - val_loss: 0.9470 - val_acc: 0.7640\n",
      "Epoch 539/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9091 - acc: 0.7725 - val_loss: 0.9525 - val_acc: 0.7620\n",
      "Epoch 540/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9084 - acc: 0.7747 - val_loss: 0.9403 - val_acc: 0.7720\n",
      "Epoch 541/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9081 - acc: 0.7741 - val_loss: 0.9565 - val_acc: 0.7700\n",
      "Epoch 542/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9087 - acc: 0.7727 - val_loss: 0.9440 - val_acc: 0.7690\n",
      "Epoch 543/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9063 - acc: 0.7728 - val_loss: 0.9544 - val_acc: 0.7640\n",
      "Epoch 544/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9078 - acc: 0.7716 - val_loss: 0.9400 - val_acc: 0.7690\n",
      "Epoch 545/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9055 - acc: 0.7721 - val_loss: 0.9748 - val_acc: 0.7560\n",
      "Epoch 546/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9085 - acc: 0.7727 - val_loss: 0.9567 - val_acc: 0.7630\n",
      "Epoch 547/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9068 - acc: 0.7736 - val_loss: 0.9379 - val_acc: 0.7690\n",
      "Epoch 548/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9054 - acc: 0.7732 - val_loss: 0.9417 - val_acc: 0.7660\n",
      "Epoch 549/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9045 - acc: 0.7736 - val_loss: 0.9367 - val_acc: 0.7700\n",
      "Epoch 550/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9042 - acc: 0.7743 - val_loss: 0.9459 - val_acc: 0.7680\n",
      "Epoch 551/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9043 - acc: 0.7747 - val_loss: 0.9410 - val_acc: 0.7680\n",
      "Epoch 552/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9045 - acc: 0.7735 - val_loss: 0.9375 - val_acc: 0.7660\n",
      "Epoch 553/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9039 - acc: 0.7735 - val_loss: 0.9353 - val_acc: 0.7700\n",
      "Epoch 554/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9038 - acc: 0.7749 - val_loss: 0.9388 - val_acc: 0.7710\n",
      "Epoch 555/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9031 - acc: 0.7749 - val_loss: 0.9403 - val_acc: 0.7670\n",
      "Epoch 556/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9022 - acc: 0.7768 - val_loss: 0.9403 - val_acc: 0.7680\n",
      "Epoch 557/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9028 - acc: 0.7747 - val_loss: 0.9403 - val_acc: 0.7670\n",
      "Epoch 558/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9016 - acc: 0.7752 - val_loss: 0.9392 - val_acc: 0.7670\n",
      "Epoch 559/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9028 - acc: 0.7729 - val_loss: 0.9354 - val_acc: 0.7680\n",
      "Epoch 560/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9023 - acc: 0.7721 - val_loss: 0.9551 - val_acc: 0.7600\n",
      "Epoch 561/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9015 - acc: 0.7733 - val_loss: 0.9423 - val_acc: 0.7690\n",
      "Epoch 562/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9017 - acc: 0.7767 - val_loss: 0.9351 - val_acc: 0.7680\n",
      "Epoch 563/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8997 - acc: 0.7733 - val_loss: 0.9392 - val_acc: 0.7660\n",
      "Epoch 564/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.9014 - acc: 0.7749 - val_loss: 0.9448 - val_acc: 0.7590\n",
      "Epoch 565/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9012 - acc: 0.7732 - val_loss: 0.9383 - val_acc: 0.7720\n",
      "Epoch 566/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9001 - acc: 0.7755 - val_loss: 0.9367 - val_acc: 0.7680\n",
      "Epoch 567/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8999 - acc: 0.7763 - val_loss: 0.9346 - val_acc: 0.7750\n",
      "Epoch 568/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.9001 - acc: 0.7771 - val_loss: 0.9369 - val_acc: 0.7660\n",
      "Epoch 569/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8992 - acc: 0.7751 - val_loss: 0.9416 - val_acc: 0.7650\n",
      "Epoch 570/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8996 - acc: 0.7741 - val_loss: 0.9447 - val_acc: 0.7670\n",
      "Epoch 571/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8987 - acc: 0.7731 - val_loss: 0.9339 - val_acc: 0.7670\n",
      "Epoch 572/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8973 - acc: 0.7736 - val_loss: 0.9433 - val_acc: 0.7650\n",
      "Epoch 573/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8978 - acc: 0.7745 - val_loss: 0.9332 - val_acc: 0.7710\n",
      "Epoch 574/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8982 - acc: 0.7760 - val_loss: 0.9459 - val_acc: 0.7660\n",
      "Epoch 575/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8989 - acc: 0.7748 - val_loss: 0.9323 - val_acc: 0.7710\n",
      "Epoch 576/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8975 - acc: 0.7763 - val_loss: 0.9385 - val_acc: 0.7660\n",
      "Epoch 577/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8964 - acc: 0.7743 - val_loss: 0.9358 - val_acc: 0.7700\n",
      "Epoch 578/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8963 - acc: 0.7751 - val_loss: 0.9477 - val_acc: 0.7620\n",
      "Epoch 579/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8965 - acc: 0.7765 - val_loss: 0.9387 - val_acc: 0.7750\n",
      "Epoch 580/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8971 - acc: 0.7759 - val_loss: 0.9390 - val_acc: 0.7680\n",
      "Epoch 581/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8959 - acc: 0.7769 - val_loss: 0.9360 - val_acc: 0.7720\n",
      "Epoch 582/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8958 - acc: 0.7771 - val_loss: 0.9481 - val_acc: 0.7660\n",
      "Epoch 583/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8949 - acc: 0.7757 - val_loss: 0.9311 - val_acc: 0.7760\n",
      "Epoch 584/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8948 - acc: 0.7815 - val_loss: 0.9291 - val_acc: 0.7680\n",
      "Epoch 585/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8949 - acc: 0.7751 - val_loss: 0.9406 - val_acc: 0.7640\n",
      "Epoch 586/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8944 - acc: 0.7753 - val_loss: 0.9330 - val_acc: 0.7710\n",
      "Epoch 587/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8938 - acc: 0.7787 - val_loss: 0.9379 - val_acc: 0.7670\n",
      "Epoch 588/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8933 - acc: 0.7760 - val_loss: 0.9365 - val_acc: 0.7680\n",
      "Epoch 589/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8925 - acc: 0.7791 - val_loss: 0.9304 - val_acc: 0.7700\n",
      "Epoch 590/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8932 - acc: 0.7759 - val_loss: 0.9267 - val_acc: 0.7750\n",
      "Epoch 591/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8927 - acc: 0.7763 - val_loss: 0.9391 - val_acc: 0.7650\n",
      "Epoch 592/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8927 - acc: 0.7768 - val_loss: 0.9587 - val_acc: 0.7590\n",
      "Epoch 593/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8927 - acc: 0.7768 - val_loss: 0.9264 - val_acc: 0.7670\n",
      "Epoch 594/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8906 - acc: 0.7799 - val_loss: 0.9268 - val_acc: 0.7670\n",
      "Epoch 595/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8917 - acc: 0.7781 - val_loss: 0.9451 - val_acc: 0.7640\n",
      "Epoch 596/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8913 - acc: 0.7765 - val_loss: 0.9284 - val_acc: 0.7690\n",
      "Epoch 597/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8913 - acc: 0.7783 - val_loss: 0.9385 - val_acc: 0.7680\n",
      "Epoch 598/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8914 - acc: 0.7759 - val_loss: 0.9295 - val_acc: 0.7680\n",
      "Epoch 599/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8911 - acc: 0.7757 - val_loss: 0.9260 - val_acc: 0.7710\n",
      "Epoch 600/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8908 - acc: 0.7781 - val_loss: 0.9239 - val_acc: 0.7710\n",
      "Epoch 601/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8899 - acc: 0.7772 - val_loss: 0.9320 - val_acc: 0.7730\n",
      "Epoch 602/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8904 - acc: 0.7764 - val_loss: 0.9234 - val_acc: 0.7730\n",
      "Epoch 603/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8895 - acc: 0.7760 - val_loss: 0.9299 - val_acc: 0.7750\n",
      "Epoch 604/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8907 - acc: 0.7792 - val_loss: 0.9336 - val_acc: 0.7660\n",
      "Epoch 605/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8893 - acc: 0.7783 - val_loss: 0.9266 - val_acc: 0.7730\n",
      "Epoch 606/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8884 - acc: 0.7812 - val_loss: 0.9246 - val_acc: 0.7700\n",
      "Epoch 607/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8890 - acc: 0.7772 - val_loss: 0.9248 - val_acc: 0.7740\n",
      "Epoch 608/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8883 - acc: 0.7789 - val_loss: 0.9288 - val_acc: 0.7720\n",
      "Epoch 609/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8897 - acc: 0.7780 - val_loss: 0.9223 - val_acc: 0.7740\n",
      "Epoch 610/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8877 - acc: 0.7799 - val_loss: 0.9239 - val_acc: 0.7710\n",
      "Epoch 611/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8863 - acc: 0.7797 - val_loss: 0.9341 - val_acc: 0.7710\n",
      "Epoch 612/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8878 - acc: 0.7787 - val_loss: 0.9232 - val_acc: 0.7690\n",
      "Epoch 613/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8866 - acc: 0.7780 - val_loss: 0.9350 - val_acc: 0.7660\n",
      "Epoch 614/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8874 - acc: 0.7780 - val_loss: 0.9322 - val_acc: 0.7680\n",
      "Epoch 615/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8870 - acc: 0.7811 - val_loss: 0.9577 - val_acc: 0.7610\n",
      "Epoch 616/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8881 - acc: 0.7771 - val_loss: 0.9246 - val_acc: 0.7710\n",
      "Epoch 617/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8852 - acc: 0.7767 - val_loss: 0.9225 - val_acc: 0.7750\n",
      "Epoch 618/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8861 - acc: 0.7768 - val_loss: 0.9229 - val_acc: 0.7770\n",
      "Epoch 619/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8856 - acc: 0.7779 - val_loss: 0.9218 - val_acc: 0.7710\n",
      "Epoch 620/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8841 - acc: 0.7800 - val_loss: 0.9243 - val_acc: 0.7760\n",
      "Epoch 621/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8845 - acc: 0.7799 - val_loss: 0.9249 - val_acc: 0.7650\n",
      "Epoch 622/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8852 - acc: 0.7795 - val_loss: 0.9276 - val_acc: 0.7730\n",
      "Epoch 623/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8843 - acc: 0.7800 - val_loss: 0.9197 - val_acc: 0.7710\n",
      "Epoch 624/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8850 - acc: 0.7792 - val_loss: 0.9274 - val_acc: 0.7760\n",
      "Epoch 625/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8842 - acc: 0.7796 - val_loss: 0.9201 - val_acc: 0.7770\n",
      "Epoch 626/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8846 - acc: 0.7799 - val_loss: 0.9298 - val_acc: 0.7670\n",
      "Epoch 627/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8843 - acc: 0.7796 - val_loss: 0.9253 - val_acc: 0.7700\n",
      "Epoch 628/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8827 - acc: 0.7799 - val_loss: 0.9351 - val_acc: 0.7680\n",
      "Epoch 629/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8835 - acc: 0.7801 - val_loss: 0.9369 - val_acc: 0.7730\n",
      "Epoch 630/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8826 - acc: 0.7805 - val_loss: 0.9312 - val_acc: 0.7680\n",
      "Epoch 631/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8822 - acc: 0.7807 - val_loss: 0.9223 - val_acc: 0.7730\n",
      "Epoch 632/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8812 - acc: 0.7793 - val_loss: 0.9412 - val_acc: 0.7650\n",
      "Epoch 633/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8827 - acc: 0.7788 - val_loss: 0.9329 - val_acc: 0.7730\n",
      "Epoch 634/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8822 - acc: 0.7807 - val_loss: 0.9225 - val_acc: 0.7740\n",
      "Epoch 635/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8808 - acc: 0.7788 - val_loss: 0.9221 - val_acc: 0.7720\n",
      "Epoch 636/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8807 - acc: 0.7795 - val_loss: 0.9203 - val_acc: 0.7740\n",
      "Epoch 637/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8809 - acc: 0.7797 - val_loss: 0.9238 - val_acc: 0.7720\n",
      "Epoch 638/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8819 - acc: 0.7799 - val_loss: 0.9220 - val_acc: 0.7680\n",
      "Epoch 639/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8812 - acc: 0.7809 - val_loss: 0.9212 - val_acc: 0.7700\n",
      "Epoch 640/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8797 - acc: 0.7805 - val_loss: 0.9395 - val_acc: 0.7720\n",
      "Epoch 641/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8803 - acc: 0.7828 - val_loss: 0.9195 - val_acc: 0.7740\n",
      "Epoch 642/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8793 - acc: 0.7832 - val_loss: 0.9211 - val_acc: 0.7670\n",
      "Epoch 643/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8798 - acc: 0.7819 - val_loss: 0.9163 - val_acc: 0.7730\n",
      "Epoch 644/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8792 - acc: 0.7840 - val_loss: 0.9204 - val_acc: 0.7730\n",
      "Epoch 645/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8797 - acc: 0.7803 - val_loss: 0.9146 - val_acc: 0.7710\n",
      "Epoch 646/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8792 - acc: 0.7787 - val_loss: 0.9233 - val_acc: 0.7730\n",
      "Epoch 647/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8798 - acc: 0.7800 - val_loss: 0.9185 - val_acc: 0.7750\n",
      "Epoch 648/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8773 - acc: 0.7829 - val_loss: 0.9258 - val_acc: 0.7730\n",
      "Epoch 649/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8779 - acc: 0.7836 - val_loss: 0.9161 - val_acc: 0.7780\n",
      "Epoch 650/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8773 - acc: 0.7812 - val_loss: 0.9200 - val_acc: 0.7730\n",
      "Epoch 651/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8781 - acc: 0.7808 - val_loss: 0.9144 - val_acc: 0.7720\n",
      "Epoch 652/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8771 - acc: 0.7816 - val_loss: 0.9150 - val_acc: 0.7730\n",
      "Epoch 653/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8767 - acc: 0.7817 - val_loss: 0.9234 - val_acc: 0.7700\n",
      "Epoch 654/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8764 - acc: 0.7835 - val_loss: 0.9167 - val_acc: 0.7770\n",
      "Epoch 655/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8765 - acc: 0.7808 - val_loss: 0.9253 - val_acc: 0.7700\n",
      "Epoch 656/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8775 - acc: 0.7852 - val_loss: 0.9324 - val_acc: 0.7650\n",
      "Epoch 657/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8769 - acc: 0.7801 - val_loss: 0.9311 - val_acc: 0.7660\n",
      "Epoch 658/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8753 - acc: 0.7823 - val_loss: 0.9187 - val_acc: 0.7830\n",
      "Epoch 659/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8761 - acc: 0.7820 - val_loss: 0.9158 - val_acc: 0.7810\n",
      "Epoch 660/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8755 - acc: 0.7819 - val_loss: 0.9227 - val_acc: 0.7750\n",
      "Epoch 661/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8760 - acc: 0.7819 - val_loss: 0.9205 - val_acc: 0.7680\n",
      "Epoch 662/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8741 - acc: 0.7836 - val_loss: 0.9177 - val_acc: 0.7740\n",
      "Epoch 663/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8744 - acc: 0.7811 - val_loss: 0.9225 - val_acc: 0.7690\n",
      "Epoch 664/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8744 - acc: 0.7804 - val_loss: 0.9127 - val_acc: 0.7770\n",
      "Epoch 665/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8733 - acc: 0.7840 - val_loss: 0.9224 - val_acc: 0.7720\n",
      "Epoch 666/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8739 - acc: 0.7807 - val_loss: 0.9223 - val_acc: 0.7710\n",
      "Epoch 667/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8731 - acc: 0.7831 - val_loss: 0.9178 - val_acc: 0.7790\n",
      "Epoch 668/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8737 - acc: 0.7815 - val_loss: 0.9259 - val_acc: 0.7680\n",
      "Epoch 669/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8726 - acc: 0.7840 - val_loss: 0.9244 - val_acc: 0.7770\n",
      "Epoch 670/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8734 - acc: 0.7836 - val_loss: 0.9216 - val_acc: 0.7750\n",
      "Epoch 671/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8725 - acc: 0.7849 - val_loss: 0.9126 - val_acc: 0.7730\n",
      "Epoch 672/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8737 - acc: 0.7847 - val_loss: 0.9169 - val_acc: 0.7730\n",
      "Epoch 673/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8713 - acc: 0.7833 - val_loss: 0.9090 - val_acc: 0.7770\n",
      "Epoch 674/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8715 - acc: 0.7824 - val_loss: 0.9325 - val_acc: 0.7660\n",
      "Epoch 675/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8734 - acc: 0.7824 - val_loss: 0.9130 - val_acc: 0.7730\n",
      "Epoch 676/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8715 - acc: 0.7844 - val_loss: 0.9139 - val_acc: 0.7740\n",
      "Epoch 677/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8716 - acc: 0.7823 - val_loss: 0.9324 - val_acc: 0.7660\n",
      "Epoch 678/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8717 - acc: 0.7855 - val_loss: 0.9234 - val_acc: 0.7720\n",
      "Epoch 679/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8725 - acc: 0.7812 - val_loss: 0.9201 - val_acc: 0.7710\n",
      "Epoch 680/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8710 - acc: 0.7816 - val_loss: 0.9117 - val_acc: 0.7780\n",
      "Epoch 681/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8711 - acc: 0.7849 - val_loss: 0.9174 - val_acc: 0.7770\n",
      "Epoch 682/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8711 - acc: 0.7836 - val_loss: 0.9139 - val_acc: 0.7810\n",
      "Epoch 683/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8688 - acc: 0.7839 - val_loss: 0.9201 - val_acc: 0.7770\n",
      "Epoch 684/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8704 - acc: 0.7817 - val_loss: 0.9107 - val_acc: 0.7740\n",
      "Epoch 685/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8692 - acc: 0.7840 - val_loss: 0.9121 - val_acc: 0.7750\n",
      "Epoch 686/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8705 - acc: 0.7841 - val_loss: 0.9106 - val_acc: 0.7760\n",
      "Epoch 687/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8689 - acc: 0.7831 - val_loss: 0.9102 - val_acc: 0.7780\n",
      "Epoch 688/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8703 - acc: 0.7825 - val_loss: 0.9102 - val_acc: 0.7740\n",
      "Epoch 689/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8679 - acc: 0.7855 - val_loss: 0.9242 - val_acc: 0.7700\n",
      "Epoch 690/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8688 - acc: 0.7839 - val_loss: 0.9128 - val_acc: 0.7740\n",
      "Epoch 691/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8674 - acc: 0.7841 - val_loss: 0.9187 - val_acc: 0.7620\n",
      "Epoch 692/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8690 - acc: 0.7833 - val_loss: 0.9130 - val_acc: 0.7700\n",
      "Epoch 693/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8688 - acc: 0.7837 - val_loss: 0.9178 - val_acc: 0.7730\n",
      "Epoch 694/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8663 - acc: 0.7855 - val_loss: 0.9068 - val_acc: 0.7830\n",
      "Epoch 695/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8668 - acc: 0.7869 - val_loss: 0.9214 - val_acc: 0.7800\n",
      "Epoch 696/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8702 - acc: 0.7832 - val_loss: 0.9118 - val_acc: 0.7660\n",
      "Epoch 697/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8668 - acc: 0.7852 - val_loss: 0.9103 - val_acc: 0.7690\n",
      "Epoch 698/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8657 - acc: 0.7860 - val_loss: 0.9092 - val_acc: 0.7800\n",
      "Epoch 699/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8666 - acc: 0.7853 - val_loss: 0.9120 - val_acc: 0.7670\n",
      "Epoch 700/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8662 - acc: 0.7853 - val_loss: 0.9093 - val_acc: 0.7770\n",
      "Epoch 701/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8668 - acc: 0.7877 - val_loss: 0.9129 - val_acc: 0.7780\n",
      "Epoch 702/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8665 - acc: 0.7825 - val_loss: 0.9043 - val_acc: 0.7770\n",
      "Epoch 703/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8655 - acc: 0.7848 - val_loss: 0.9163 - val_acc: 0.7710\n",
      "Epoch 704/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8636 - acc: 0.7883 - val_loss: 0.9212 - val_acc: 0.7680\n",
      "Epoch 705/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8659 - acc: 0.7840 - val_loss: 0.9200 - val_acc: 0.7700\n",
      "Epoch 706/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8648 - acc: 0.7836 - val_loss: 0.9273 - val_acc: 0.7730\n",
      "Epoch 707/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8654 - acc: 0.7847 - val_loss: 0.9202 - val_acc: 0.7780\n",
      "Epoch 708/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8645 - acc: 0.7851 - val_loss: 0.9171 - val_acc: 0.7660\n",
      "Epoch 709/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8651 - acc: 0.7824 - val_loss: 0.9152 - val_acc: 0.7690\n",
      "Epoch 710/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8656 - acc: 0.7863 - val_loss: 0.9062 - val_acc: 0.7790\n",
      "Epoch 711/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8631 - acc: 0.7883 - val_loss: 0.9058 - val_acc: 0.7800\n",
      "Epoch 712/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8639 - acc: 0.7837 - val_loss: 0.9078 - val_acc: 0.7850\n",
      "Epoch 713/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8632 - acc: 0.7872 - val_loss: 0.9063 - val_acc: 0.7760\n",
      "Epoch 714/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8636 - acc: 0.7877 - val_loss: 0.9123 - val_acc: 0.7720\n",
      "Epoch 715/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8625 - acc: 0.7891 - val_loss: 0.9069 - val_acc: 0.7810\n",
      "Epoch 716/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8627 - acc: 0.7872 - val_loss: 0.9009 - val_acc: 0.7780\n",
      "Epoch 717/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8619 - acc: 0.7859 - val_loss: 0.9141 - val_acc: 0.7800\n",
      "Epoch 718/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8618 - acc: 0.7891 - val_loss: 0.9118 - val_acc: 0.7830\n",
      "Epoch 719/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8625 - acc: 0.7860 - val_loss: 0.9163 - val_acc: 0.7820\n",
      "Epoch 720/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8619 - acc: 0.7884 - val_loss: 0.9229 - val_acc: 0.7700\n",
      "Epoch 721/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8639 - acc: 0.7848 - val_loss: 0.9157 - val_acc: 0.7820\n",
      "Epoch 722/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8603 - acc: 0.7877 - val_loss: 0.9071 - val_acc: 0.7750\n",
      "Epoch 723/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8620 - acc: 0.7841 - val_loss: 0.9586 - val_acc: 0.7580\n",
      "Epoch 724/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8621 - acc: 0.7871 - val_loss: 0.9043 - val_acc: 0.7770\n",
      "Epoch 725/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8612 - acc: 0.7848 - val_loss: 0.9146 - val_acc: 0.7660\n",
      "Epoch 726/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8613 - acc: 0.7879 - val_loss: 0.9050 - val_acc: 0.7790\n",
      "Epoch 727/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8606 - acc: 0.7893 - val_loss: 0.9062 - val_acc: 0.7850\n",
      "Epoch 728/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8599 - acc: 0.7896 - val_loss: 0.9043 - val_acc: 0.7800\n",
      "Epoch 729/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8594 - acc: 0.7864 - val_loss: 0.9014 - val_acc: 0.7750\n",
      "Epoch 730/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8598 - acc: 0.7884 - val_loss: 0.9030 - val_acc: 0.7760\n",
      "Epoch 731/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8605 - acc: 0.7879 - val_loss: 0.9228 - val_acc: 0.7720\n",
      "Epoch 732/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8596 - acc: 0.7891 - val_loss: 0.9211 - val_acc: 0.7630\n",
      "Epoch 733/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8610 - acc: 0.7891 - val_loss: 0.9118 - val_acc: 0.7820\n",
      "Epoch 734/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8582 - acc: 0.7871 - val_loss: 0.9173 - val_acc: 0.7740\n",
      "Epoch 735/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8580 - acc: 0.7889 - val_loss: 0.9023 - val_acc: 0.7760\n",
      "Epoch 736/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8603 - acc: 0.7873 - val_loss: 0.9049 - val_acc: 0.7750\n",
      "Epoch 737/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8587 - acc: 0.7888 - val_loss: 0.9101 - val_acc: 0.7690\n",
      "Epoch 738/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8591 - acc: 0.7883 - val_loss: 0.9049 - val_acc: 0.7800\n",
      "Epoch 739/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8594 - acc: 0.7877 - val_loss: 0.9029 - val_acc: 0.7760\n",
      "Epoch 740/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8581 - acc: 0.7925 - val_loss: 0.9012 - val_acc: 0.7830\n",
      "Epoch 741/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8568 - acc: 0.7908 - val_loss: 0.9128 - val_acc: 0.7810\n",
      "Epoch 742/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8572 - acc: 0.7924 - val_loss: 0.9004 - val_acc: 0.7810\n",
      "Epoch 743/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8572 - acc: 0.7912 - val_loss: 0.9030 - val_acc: 0.7770\n",
      "Epoch 744/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8566 - acc: 0.7895 - val_loss: 0.9040 - val_acc: 0.7790\n",
      "Epoch 745/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8576 - acc: 0.7909 - val_loss: 0.9078 - val_acc: 0.7820\n",
      "Epoch 746/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8570 - acc: 0.7895 - val_loss: 0.9111 - val_acc: 0.7700\n",
      "Epoch 747/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8564 - acc: 0.7895 - val_loss: 0.9131 - val_acc: 0.7720\n",
      "Epoch 748/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8561 - acc: 0.7916 - val_loss: 0.9010 - val_acc: 0.7820\n",
      "Epoch 749/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8554 - acc: 0.7911 - val_loss: 0.9012 - val_acc: 0.7730\n",
      "Epoch 750/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8563 - acc: 0.7895 - val_loss: 0.8993 - val_acc: 0.7790\n",
      "Epoch 751/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8567 - acc: 0.7893 - val_loss: 0.9190 - val_acc: 0.7770\n",
      "Epoch 752/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8562 - acc: 0.7888 - val_loss: 0.9031 - val_acc: 0.7880\n",
      "Epoch 753/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8553 - acc: 0.7897 - val_loss: 0.9022 - val_acc: 0.7770\n",
      "Epoch 754/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8554 - acc: 0.7905 - val_loss: 0.9043 - val_acc: 0.7760\n",
      "Epoch 755/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8536 - acc: 0.7904 - val_loss: 0.9001 - val_acc: 0.7790\n",
      "Epoch 756/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8551 - acc: 0.7919 - val_loss: 0.9049 - val_acc: 0.7790\n",
      "Epoch 757/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8545 - acc: 0.7948 - val_loss: 0.9089 - val_acc: 0.7830\n",
      "Epoch 758/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8532 - acc: 0.7915 - val_loss: 0.9013 - val_acc: 0.7750\n",
      "Epoch 759/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8552 - acc: 0.7909 - val_loss: 0.9169 - val_acc: 0.7790\n",
      "Epoch 760/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8544 - acc: 0.7905 - val_loss: 0.9052 - val_acc: 0.7770\n",
      "Epoch 761/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8550 - acc: 0.7903 - val_loss: 0.9012 - val_acc: 0.7720\n",
      "Epoch 762/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8547 - acc: 0.7905 - val_loss: 0.9098 - val_acc: 0.7760\n",
      "Epoch 763/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8538 - acc: 0.7920 - val_loss: 0.9089 - val_acc: 0.7770\n",
      "Epoch 764/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8519 - acc: 0.7924 - val_loss: 0.9024 - val_acc: 0.7810\n",
      "Epoch 765/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8525 - acc: 0.7921 - val_loss: 0.9024 - val_acc: 0.7730\n",
      "Epoch 766/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8521 - acc: 0.7935 - val_loss: 0.9016 - val_acc: 0.7720\n",
      "Epoch 767/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8526 - acc: 0.7927 - val_loss: 0.9040 - val_acc: 0.7700\n",
      "Epoch 768/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8520 - acc: 0.7909 - val_loss: 0.8971 - val_acc: 0.7820\n",
      "Epoch 769/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8532 - acc: 0.7951 - val_loss: 0.9003 - val_acc: 0.7750\n",
      "Epoch 770/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8523 - acc: 0.7907 - val_loss: 0.9008 - val_acc: 0.7840\n",
      "Epoch 771/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8515 - acc: 0.7912 - val_loss: 0.9039 - val_acc: 0.7770\n",
      "Epoch 772/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8523 - acc: 0.7933 - val_loss: 0.9105 - val_acc: 0.7800\n",
      "Epoch 773/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8522 - acc: 0.7913 - val_loss: 0.9141 - val_acc: 0.7780\n",
      "Epoch 774/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8535 - acc: 0.7929 - val_loss: 0.9042 - val_acc: 0.7800\n",
      "Epoch 775/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8516 - acc: 0.7939 - val_loss: 0.9000 - val_acc: 0.7820\n",
      "Epoch 776/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8508 - acc: 0.7931 - val_loss: 0.9327 - val_acc: 0.7600\n",
      "Epoch 777/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8533 - acc: 0.7907 - val_loss: 0.9147 - val_acc: 0.7810\n",
      "Epoch 778/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8522 - acc: 0.7949 - val_loss: 0.9007 - val_acc: 0.7760\n",
      "Epoch 779/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8510 - acc: 0.7944 - val_loss: 0.9065 - val_acc: 0.7800\n",
      "Epoch 780/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8502 - acc: 0.7920 - val_loss: 0.9014 - val_acc: 0.7860\n",
      "Epoch 781/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8502 - acc: 0.7929 - val_loss: 0.9167 - val_acc: 0.7740\n",
      "Epoch 782/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8509 - acc: 0.7952 - val_loss: 0.9015 - val_acc: 0.7810\n",
      "Epoch 783/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8511 - acc: 0.7921 - val_loss: 0.9012 - val_acc: 0.7780\n",
      "Epoch 784/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8503 - acc: 0.7969 - val_loss: 0.9053 - val_acc: 0.7750\n",
      "Epoch 785/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8503 - acc: 0.7909 - val_loss: 0.9085 - val_acc: 0.7760\n",
      "Epoch 786/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8479 - acc: 0.7940 - val_loss: 0.9251 - val_acc: 0.7720\n",
      "Epoch 787/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8508 - acc: 0.7927 - val_loss: 0.9022 - val_acc: 0.7760\n",
      "Epoch 788/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8493 - acc: 0.7960 - val_loss: 0.8956 - val_acc: 0.7800\n",
      "Epoch 789/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8488 - acc: 0.7952 - val_loss: 0.8960 - val_acc: 0.7800\n",
      "Epoch 790/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8491 - acc: 0.7955 - val_loss: 0.9052 - val_acc: 0.7780\n",
      "Epoch 791/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8486 - acc: 0.7949 - val_loss: 0.8946 - val_acc: 0.7820\n",
      "Epoch 792/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8472 - acc: 0.7945 - val_loss: 0.8980 - val_acc: 0.7780\n",
      "Epoch 793/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8484 - acc: 0.7943 - val_loss: 0.9092 - val_acc: 0.7770\n",
      "Epoch 794/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8475 - acc: 0.7948 - val_loss: 0.9079 - val_acc: 0.7910\n",
      "Epoch 795/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8489 - acc: 0.7932 - val_loss: 0.9002 - val_acc: 0.7880\n",
      "Epoch 796/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8482 - acc: 0.7941 - val_loss: 0.9001 - val_acc: 0.7820\n",
      "Epoch 797/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8483 - acc: 0.7927 - val_loss: 0.9062 - val_acc: 0.7830\n",
      "Epoch 798/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8478 - acc: 0.7925 - val_loss: 0.9016 - val_acc: 0.7810\n",
      "Epoch 799/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8463 - acc: 0.7951 - val_loss: 0.9008 - val_acc: 0.7770\n",
      "Epoch 800/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8471 - acc: 0.7935 - val_loss: 0.9019 - val_acc: 0.7800\n",
      "Epoch 801/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8451 - acc: 0.7960 - val_loss: 0.8990 - val_acc: 0.7780\n",
      "Epoch 802/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8460 - acc: 0.7948 - val_loss: 0.9064 - val_acc: 0.7850\n",
      "Epoch 803/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8462 - acc: 0.7980 - val_loss: 0.8947 - val_acc: 0.7820\n",
      "Epoch 804/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8455 - acc: 0.7965 - val_loss: 0.8965 - val_acc: 0.7820\n",
      "Epoch 805/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8449 - acc: 0.7940 - val_loss: 0.8939 - val_acc: 0.7790\n",
      "Epoch 806/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8462 - acc: 0.7932 - val_loss: 0.9103 - val_acc: 0.7820\n",
      "Epoch 807/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8458 - acc: 0.7968 - val_loss: 0.8945 - val_acc: 0.7880\n",
      "Epoch 808/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8440 - acc: 0.7956 - val_loss: 0.9043 - val_acc: 0.7780\n",
      "Epoch 809/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8447 - acc: 0.7959 - val_loss: 0.9105 - val_acc: 0.7800\n",
      "Epoch 810/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8457 - acc: 0.7944 - val_loss: 0.9059 - val_acc: 0.7860\n",
      "Epoch 811/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8451 - acc: 0.7960 - val_loss: 0.9061 - val_acc: 0.7830\n",
      "Epoch 812/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8452 - acc: 0.7939 - val_loss: 0.9180 - val_acc: 0.7670\n",
      "Epoch 813/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8460 - acc: 0.7945 - val_loss: 0.8953 - val_acc: 0.7790\n",
      "Epoch 814/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8436 - acc: 0.7971 - val_loss: 0.9063 - val_acc: 0.7800\n",
      "Epoch 815/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8440 - acc: 0.7980 - val_loss: 0.9131 - val_acc: 0.7660\n",
      "Epoch 816/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8444 - acc: 0.7971 - val_loss: 0.8942 - val_acc: 0.7870\n",
      "Epoch 817/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8434 - acc: 0.7983 - val_loss: 0.9108 - val_acc: 0.7770\n",
      "Epoch 818/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8437 - acc: 0.7944 - val_loss: 0.8983 - val_acc: 0.7800\n",
      "Epoch 819/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8425 - acc: 0.7964 - val_loss: 0.8969 - val_acc: 0.7850\n",
      "Epoch 820/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8433 - acc: 0.7947 - val_loss: 0.9166 - val_acc: 0.7760\n",
      "Epoch 821/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8416 - acc: 0.7948 - val_loss: 0.9038 - val_acc: 0.7720\n",
      "Epoch 822/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8420 - acc: 0.7961 - val_loss: 0.8994 - val_acc: 0.7820\n",
      "Epoch 823/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8423 - acc: 0.7947 - val_loss: 0.9016 - val_acc: 0.7790\n",
      "Epoch 824/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8411 - acc: 0.7988 - val_loss: 0.9285 - val_acc: 0.7700\n",
      "Epoch 825/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8433 - acc: 0.7953 - val_loss: 0.8967 - val_acc: 0.7800\n",
      "Epoch 826/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8441 - acc: 0.7969 - val_loss: 0.8915 - val_acc: 0.7830\n",
      "Epoch 827/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8411 - acc: 0.7980 - val_loss: 0.8914 - val_acc: 0.7840\n",
      "Epoch 828/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8420 - acc: 0.7948 - val_loss: 0.8905 - val_acc: 0.7820\n",
      "Epoch 829/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8406 - acc: 0.7975 - val_loss: 0.8933 - val_acc: 0.7810\n",
      "Epoch 830/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8402 - acc: 0.7981 - val_loss: 0.8920 - val_acc: 0.7840\n",
      "Epoch 831/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8394 - acc: 0.7975 - val_loss: 0.8961 - val_acc: 0.7820\n",
      "Epoch 832/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8402 - acc: 0.7987 - val_loss: 0.8903 - val_acc: 0.7850\n",
      "Epoch 833/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8410 - acc: 0.7951 - val_loss: 0.9004 - val_acc: 0.7840\n",
      "Epoch 834/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8401 - acc: 0.7984 - val_loss: 0.9017 - val_acc: 0.7780\n",
      "Epoch 835/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8410 - acc: 0.7960 - val_loss: 0.9295 - val_acc: 0.7720\n",
      "Epoch 836/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8432 - acc: 0.7967 - val_loss: 0.8913 - val_acc: 0.7820\n",
      "Epoch 837/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8418 - acc: 0.7997 - val_loss: 0.9016 - val_acc: 0.7880\n",
      "Epoch 838/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8393 - acc: 0.7973 - val_loss: 0.8975 - val_acc: 0.7820\n",
      "Epoch 839/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8395 - acc: 0.7981 - val_loss: 0.9024 - val_acc: 0.7840\n",
      "Epoch 840/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8399 - acc: 0.7987 - val_loss: 0.8930 - val_acc: 0.7760\n",
      "Epoch 841/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8374 - acc: 0.7984 - val_loss: 0.8996 - val_acc: 0.7820\n",
      "Epoch 842/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8414 - acc: 0.7959 - val_loss: 0.8961 - val_acc: 0.7800\n",
      "Epoch 843/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8389 - acc: 0.7999 - val_loss: 0.9104 - val_acc: 0.7650\n",
      "Epoch 844/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8394 - acc: 0.8005 - val_loss: 0.9343 - val_acc: 0.7750\n",
      "Epoch 845/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8391 - acc: 0.7997 - val_loss: 0.8954 - val_acc: 0.7860\n",
      "Epoch 846/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8395 - acc: 0.7973 - val_loss: 0.8932 - val_acc: 0.7890\n",
      "Epoch 847/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8372 - acc: 0.8005 - val_loss: 0.8930 - val_acc: 0.7850\n",
      "Epoch 848/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8372 - acc: 0.7995 - val_loss: 0.8939 - val_acc: 0.7830\n",
      "Epoch 849/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8383 - acc: 0.8001 - val_loss: 0.8922 - val_acc: 0.7830\n",
      "Epoch 850/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8369 - acc: 0.8004 - val_loss: 0.9390 - val_acc: 0.7620\n",
      "Epoch 851/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8394 - acc: 0.7985 - val_loss: 0.8952 - val_acc: 0.7850\n",
      "Epoch 852/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8356 - acc: 0.7997 - val_loss: 0.8930 - val_acc: 0.7850\n",
      "Epoch 853/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8373 - acc: 0.7967 - val_loss: 0.8963 - val_acc: 0.7850\n",
      "Epoch 854/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8363 - acc: 0.7997 - val_loss: 0.8890 - val_acc: 0.7850\n",
      "Epoch 855/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8363 - acc: 0.8005 - val_loss: 0.8945 - val_acc: 0.7850\n",
      "Epoch 856/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8365 - acc: 0.8001 - val_loss: 0.9068 - val_acc: 0.7850\n",
      "Epoch 857/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8370 - acc: 0.7999 - val_loss: 0.8895 - val_acc: 0.7910\n",
      "Epoch 858/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8353 - acc: 0.8005 - val_loss: 0.9182 - val_acc: 0.7620\n",
      "Epoch 859/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8364 - acc: 0.7993 - val_loss: 0.8929 - val_acc: 0.7880\n",
      "Epoch 860/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8356 - acc: 0.8017 - val_loss: 0.8989 - val_acc: 0.7820\n",
      "Epoch 861/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8345 - acc: 0.8019 - val_loss: 0.8934 - val_acc: 0.7880\n",
      "Epoch 862/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8343 - acc: 0.7999 - val_loss: 0.8917 - val_acc: 0.7890\n",
      "Epoch 863/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8353 - acc: 0.8005 - val_loss: 0.8953 - val_acc: 0.7810\n",
      "Epoch 864/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8334 - acc: 0.7989 - val_loss: 0.9048 - val_acc: 0.7900\n",
      "Epoch 865/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8345 - acc: 0.8007 - val_loss: 0.8924 - val_acc: 0.7880\n",
      "Epoch 866/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8341 - acc: 0.8012 - val_loss: 0.9149 - val_acc: 0.7710\n",
      "Epoch 867/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8342 - acc: 0.7993 - val_loss: 0.8878 - val_acc: 0.7870\n",
      "Epoch 868/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8348 - acc: 0.8020 - val_loss: 0.9036 - val_acc: 0.7870\n",
      "Epoch 869/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8344 - acc: 0.7999 - val_loss: 0.9170 - val_acc: 0.7780\n",
      "Epoch 870/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8358 - acc: 0.8047 - val_loss: 0.8882 - val_acc: 0.7860\n",
      "Epoch 871/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8318 - acc: 0.8041 - val_loss: 0.9039 - val_acc: 0.7870\n",
      "Epoch 872/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8327 - acc: 0.7999 - val_loss: 0.8979 - val_acc: 0.7860\n",
      "Epoch 873/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8333 - acc: 0.8008 - val_loss: 0.8983 - val_acc: 0.7900\n",
      "Epoch 874/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8316 - acc: 0.8029 - val_loss: 0.9036 - val_acc: 0.7850\n",
      "Epoch 875/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8327 - acc: 0.8017 - val_loss: 0.8985 - val_acc: 0.7860\n",
      "Epoch 876/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8334 - acc: 0.8016 - val_loss: 0.9126 - val_acc: 0.7740\n",
      "Epoch 877/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8330 - acc: 0.8027 - val_loss: 0.9013 - val_acc: 0.7890\n",
      "Epoch 878/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8313 - acc: 0.8047 - val_loss: 0.8845 - val_acc: 0.7910\n",
      "Epoch 879/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8332 - acc: 0.8016 - val_loss: 0.8851 - val_acc: 0.7870\n",
      "Epoch 880/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8335 - acc: 0.8021 - val_loss: 0.8900 - val_acc: 0.7880\n",
      "Epoch 881/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8330 - acc: 0.8033 - val_loss: 0.9257 - val_acc: 0.7750\n",
      "Epoch 882/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8336 - acc: 0.8036 - val_loss: 0.9141 - val_acc: 0.7850\n",
      "Epoch 883/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8313 - acc: 0.8056 - val_loss: 0.8986 - val_acc: 0.7840\n",
      "Epoch 884/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8300 - acc: 0.8037 - val_loss: 0.9029 - val_acc: 0.7870\n",
      "Epoch 885/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8318 - acc: 0.8023 - val_loss: 0.9027 - val_acc: 0.7830\n",
      "Epoch 886/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8313 - acc: 0.8037 - val_loss: 0.8844 - val_acc: 0.7840\n",
      "Epoch 887/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8310 - acc: 0.8017 - val_loss: 0.8894 - val_acc: 0.7900\n",
      "Epoch 888/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8306 - acc: 0.8057 - val_loss: 0.8987 - val_acc: 0.7730\n",
      "Epoch 889/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8305 - acc: 0.8023 - val_loss: 0.8919 - val_acc: 0.7830\n",
      "Epoch 890/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8297 - acc: 0.8012 - val_loss: 0.8953 - val_acc: 0.7880\n",
      "Epoch 891/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8309 - acc: 0.8005 - val_loss: 0.8856 - val_acc: 0.7880\n",
      "Epoch 892/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8287 - acc: 0.8020 - val_loss: 0.9094 - val_acc: 0.7770\n",
      "Epoch 893/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8314 - acc: 0.8029 - val_loss: 0.9026 - val_acc: 0.7900\n",
      "Epoch 894/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8298 - acc: 0.8031 - val_loss: 0.8906 - val_acc: 0.7890\n",
      "Epoch 895/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8274 - acc: 0.8064 - val_loss: 0.8868 - val_acc: 0.7870\n",
      "Epoch 896/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8293 - acc: 0.8033 - val_loss: 0.8907 - val_acc: 0.7910\n",
      "Epoch 897/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8277 - acc: 0.8031 - val_loss: 0.9231 - val_acc: 0.7740\n",
      "Epoch 898/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8300 - acc: 0.8056 - val_loss: 0.8877 - val_acc: 0.7860\n",
      "Epoch 899/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8287 - acc: 0.8051 - val_loss: 0.8886 - val_acc: 0.7870\n",
      "Epoch 900/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8276 - acc: 0.8048 - val_loss: 0.8882 - val_acc: 0.7850\n",
      "Epoch 901/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8264 - acc: 0.8075 - val_loss: 0.9002 - val_acc: 0.7870\n",
      "Epoch 902/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8285 - acc: 0.8041 - val_loss: 0.8888 - val_acc: 0.7890\n",
      "Epoch 903/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8266 - acc: 0.8067 - val_loss: 0.8964 - val_acc: 0.7870\n",
      "Epoch 904/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8274 - acc: 0.8041 - val_loss: 0.9055 - val_acc: 0.7840\n",
      "Epoch 905/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8273 - acc: 0.8043 - val_loss: 0.9015 - val_acc: 0.7720\n",
      "Epoch 906/1000\n",
      "7500/7500 [==============================] - 0s 35us/step - loss: 0.8263 - acc: 0.8048 - val_loss: 0.8881 - val_acc: 0.7860\n",
      "Epoch 907/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8269 - acc: 0.8041 - val_loss: 0.8865 - val_acc: 0.7870\n",
      "Epoch 908/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8248 - acc: 0.8037 - val_loss: 0.8847 - val_acc: 0.7880\n",
      "Epoch 909/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8258 - acc: 0.8073 - val_loss: 0.8990 - val_acc: 0.7860\n",
      "Epoch 910/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8279 - acc: 0.8051 - val_loss: 0.8913 - val_acc: 0.7920\n",
      "Epoch 911/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8268 - acc: 0.8040 - val_loss: 0.8855 - val_acc: 0.7900\n",
      "Epoch 912/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8258 - acc: 0.8043 - val_loss: 0.8864 - val_acc: 0.7900\n",
      "Epoch 913/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8236 - acc: 0.8069 - val_loss: 0.8958 - val_acc: 0.7870\n",
      "Epoch 914/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8248 - acc: 0.8049 - val_loss: 0.9032 - val_acc: 0.7820\n",
      "Epoch 915/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8278 - acc: 0.8061 - val_loss: 0.8937 - val_acc: 0.7850\n",
      "Epoch 916/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8257 - acc: 0.8075 - val_loss: 0.8873 - val_acc: 0.7970\n",
      "Epoch 917/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8251 - acc: 0.8063 - val_loss: 0.8822 - val_acc: 0.7930\n",
      "Epoch 918/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8247 - acc: 0.8071 - val_loss: 0.9167 - val_acc: 0.7800\n",
      "Epoch 919/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8262 - acc: 0.8052 - val_loss: 0.8845 - val_acc: 0.7900\n",
      "Epoch 920/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8234 - acc: 0.8056 - val_loss: 0.8865 - val_acc: 0.7920\n",
      "Epoch 921/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8256 - acc: 0.8051 - val_loss: 0.8929 - val_acc: 0.7950\n",
      "Epoch 922/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8251 - acc: 0.8059 - val_loss: 0.9265 - val_acc: 0.7660\n",
      "Epoch 923/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8231 - acc: 0.8067 - val_loss: 0.9060 - val_acc: 0.7760\n",
      "Epoch 924/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8236 - acc: 0.8069 - val_loss: 0.9074 - val_acc: 0.7890\n",
      "Epoch 925/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8245 - acc: 0.8072 - val_loss: 0.8882 - val_acc: 0.7840\n",
      "Epoch 926/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8224 - acc: 0.8060 - val_loss: 0.8817 - val_acc: 0.7930\n",
      "Epoch 927/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8236 - acc: 0.8037 - val_loss: 0.8854 - val_acc: 0.7880\n",
      "Epoch 928/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8233 - acc: 0.8083 - val_loss: 0.9014 - val_acc: 0.7800\n",
      "Epoch 929/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8265 - acc: 0.8061 - val_loss: 0.8927 - val_acc: 0.7950\n",
      "Epoch 930/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8245 - acc: 0.8029 - val_loss: 0.8871 - val_acc: 0.7920\n",
      "Epoch 931/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8217 - acc: 0.8091 - val_loss: 0.8883 - val_acc: 0.7860\n",
      "Epoch 932/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8202 - acc: 0.8057 - val_loss: 0.8945 - val_acc: 0.7870\n",
      "Epoch 933/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8220 - acc: 0.8068 - val_loss: 0.8961 - val_acc: 0.7910\n",
      "Epoch 934/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8215 - acc: 0.8080 - val_loss: 0.9131 - val_acc: 0.7800\n",
      "Epoch 935/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8242 - acc: 0.8069 - val_loss: 0.8899 - val_acc: 0.7910\n",
      "Epoch 936/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8218 - acc: 0.8100 - val_loss: 0.9408 - val_acc: 0.7570\n",
      "Epoch 937/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8248 - acc: 0.8099 - val_loss: 0.8892 - val_acc: 0.7920\n",
      "Epoch 938/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8220 - acc: 0.8079 - val_loss: 0.8839 - val_acc: 0.7900\n",
      "Epoch 939/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8197 - acc: 0.8077 - val_loss: 0.8979 - val_acc: 0.7880\n",
      "Epoch 940/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8218 - acc: 0.8063 - val_loss: 0.8983 - val_acc: 0.7870\n",
      "Epoch 941/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8195 - acc: 0.8097 - val_loss: 0.8861 - val_acc: 0.7960\n",
      "Epoch 942/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8223 - acc: 0.8059 - val_loss: 0.8990 - val_acc: 0.7940\n",
      "Epoch 943/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8210 - acc: 0.8077 - val_loss: 0.8898 - val_acc: 0.7870\n",
      "Epoch 944/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8204 - acc: 0.8105 - val_loss: 0.8862 - val_acc: 0.7910\n",
      "Epoch 945/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8194 - acc: 0.8101 - val_loss: 0.8808 - val_acc: 0.7900\n",
      "Epoch 946/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8206 - acc: 0.8077 - val_loss: 0.8946 - val_acc: 0.7900\n",
      "Epoch 947/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8197 - acc: 0.8091 - val_loss: 0.8873 - val_acc: 0.7870\n",
      "Epoch 948/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8197 - acc: 0.8081 - val_loss: 0.8816 - val_acc: 0.7930\n",
      "Epoch 949/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8190 - acc: 0.8105 - val_loss: 0.9006 - val_acc: 0.7790\n",
      "Epoch 950/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8196 - acc: 0.8059 - val_loss: 0.8845 - val_acc: 0.7880\n",
      "Epoch 951/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8204 - acc: 0.8073 - val_loss: 0.8851 - val_acc: 0.7960\n",
      "Epoch 952/1000\n",
      "7500/7500 [==============================] - 0s 19us/step - loss: 0.8180 - acc: 0.8088 - val_loss: 0.8960 - val_acc: 0.7800\n",
      "Epoch 953/1000\n",
      "7500/7500 [==============================] - 0s 17us/step - loss: 0.8195 - acc: 0.8092 - val_loss: 0.8835 - val_acc: 0.7990\n",
      "Epoch 954/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8186 - acc: 0.8111 - val_loss: 0.9085 - val_acc: 0.7850\n",
      "Epoch 955/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8187 - acc: 0.8108 - val_loss: 0.8921 - val_acc: 0.7920\n",
      "Epoch 956/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8186 - acc: 0.8100 - val_loss: 0.9092 - val_acc: 0.7850\n",
      "Epoch 957/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8190 - acc: 0.8071 - val_loss: 0.8904 - val_acc: 0.7930\n",
      "Epoch 958/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8172 - acc: 0.8083 - val_loss: 0.8860 - val_acc: 0.7970\n",
      "Epoch 959/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8174 - acc: 0.8105 - val_loss: 0.8823 - val_acc: 0.7910\n",
      "Epoch 960/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8177 - acc: 0.8071 - val_loss: 0.8849 - val_acc: 0.7970\n",
      "Epoch 961/1000\n",
      "7500/7500 [==============================] - 0s 18us/step - loss: 0.8175 - acc: 0.8085 - val_loss: 0.8848 - val_acc: 0.7990\n",
      "Epoch 962/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8159 - acc: 0.8116 - val_loss: 0.9180 - val_acc: 0.7620\n",
      "Epoch 963/1000\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.8178 - acc: 0.8112 - val_loss: 0.8826 - val_acc: 0.7910\n",
      "Epoch 964/1000\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 0.8167 - acc: 0.8075 - val_loss: 0.8839 - val_acc: 0.7920\n",
      "Epoch 965/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8164 - acc: 0.8105 - val_loss: 0.8844 - val_acc: 0.7960\n",
      "Epoch 966/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8184 - acc: 0.8093 - val_loss: 0.8846 - val_acc: 0.7960\n",
      "Epoch 967/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8179 - acc: 0.8080 - val_loss: 0.8970 - val_acc: 0.7790\n",
      "Epoch 968/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8177 - acc: 0.8108 - val_loss: 0.8871 - val_acc: 0.8000\n",
      "Epoch 969/1000\n",
      "7500/7500 [==============================] - 0s 39us/step - loss: 0.8179 - acc: 0.8101 - val_loss: 0.8850 - val_acc: 0.7990\n",
      "Epoch 970/1000\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8151 - acc: 0.8108 - val_loss: 0.8968 - val_acc: 0.7830\n",
      "Epoch 971/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8154 - acc: 0.8095 - val_loss: 0.9057 - val_acc: 0.7870\n",
      "Epoch 972/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8158 - acc: 0.8109 - val_loss: 0.8852 - val_acc: 0.7890\n",
      "Epoch 973/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8154 - acc: 0.8088 - val_loss: 0.8999 - val_acc: 0.7820\n",
      "Epoch 974/1000\n",
      "7500/7500 [==============================] - 0s 20us/step - loss: 0.8157 - acc: 0.8121 - val_loss: 0.8898 - val_acc: 0.7930\n",
      "Epoch 975/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8145 - acc: 0.8107 - val_loss: 0.8947 - val_acc: 0.7960\n",
      "Epoch 976/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8137 - acc: 0.8104 - val_loss: 0.8802 - val_acc: 0.7950\n",
      "Epoch 977/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8149 - acc: 0.8087 - val_loss: 0.8912 - val_acc: 0.7930\n",
      "Epoch 978/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8142 - acc: 0.8123 - val_loss: 0.8894 - val_acc: 0.7860\n",
      "Epoch 979/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8135 - acc: 0.8120 - val_loss: 0.8819 - val_acc: 0.7960\n",
      "Epoch 980/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8152 - acc: 0.8117 - val_loss: 0.8802 - val_acc: 0.7950\n",
      "Epoch 981/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8161 - acc: 0.8093 - val_loss: 0.8925 - val_acc: 0.8000\n",
      "Epoch 982/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8147 - acc: 0.8104 - val_loss: 0.8975 - val_acc: 0.7850\n",
      "Epoch 983/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8149 - acc: 0.8113 - val_loss: 0.8918 - val_acc: 0.7930\n",
      "Epoch 984/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8138 - acc: 0.8119 - val_loss: 0.8865 - val_acc: 0.7890\n",
      "Epoch 985/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8129 - acc: 0.8111 - val_loss: 0.9010 - val_acc: 0.7890\n",
      "Epoch 986/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8140 - acc: 0.8111 - val_loss: 0.8872 - val_acc: 0.7970\n",
      "Epoch 987/1000\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8115 - acc: 0.8075 - val_loss: 0.8845 - val_acc: 0.8000\n",
      "Epoch 988/1000\n",
      "7500/7500 [==============================] - 0s 21us/step - loss: 0.8129 - acc: 0.8117 - val_loss: 0.8895 - val_acc: 0.7890\n",
      "Epoch 989/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8124 - acc: 0.8109 - val_loss: 0.9067 - val_acc: 0.7800\n",
      "Epoch 990/1000\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8155 - acc: 0.8105 - val_loss: 0.8827 - val_acc: 0.8020\n",
      "Epoch 991/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8130 - acc: 0.8115 - val_loss: 0.8929 - val_acc: 0.7900\n",
      "Epoch 992/1000\n",
      "7500/7500 [==============================] - ETA: 0s - loss: 0.8154 - acc: 0.807 - 0s 22us/step - loss: 0.8125 - acc: 0.8099 - val_loss: 0.8788 - val_acc: 0.7980\n",
      "Epoch 993/1000\n",
      "7500/7500 [==============================] - 0s 24us/step - loss: 0.8115 - acc: 0.8127 - val_loss: 0.8837 - val_acc: 0.7980\n",
      "Epoch 994/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8140 - acc: 0.8093 - val_loss: 0.8956 - val_acc: 0.7780\n",
      "Epoch 995/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8114 - acc: 0.8133 - val_loss: 0.8891 - val_acc: 0.8040\n",
      "Epoch 996/1000\n",
      "7500/7500 [==============================] - 0s 23us/step - loss: 0.8141 - acc: 0.8100 - val_loss: 0.8849 - val_acc: 0.7970\n",
      "Epoch 997/1000\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8116 - acc: 0.8113 - val_loss: 0.8871 - val_acc: 0.7920\n",
      "Epoch 998/1000\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 0.8109 - acc: 0.8129 - val_loss: 0.8955 - val_acc: 0.7930\n",
      "Epoch 999/1000\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8122 - acc: 0.8111 - val_loss: 0.8983 - val_acc: 0.7890\n",
      "Epoch 1000/1000\n",
      "7500/7500 [==============================] - 0s 22us/step - loss: 0.8121 - acc: 0.8080 - val_loss: 0.9021 - val_acc: 0.7920\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu',kernel_regularizer=regularizers.l1(0.005), input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, kernel_regularizer=regularizers.l1(0.005), activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "L1_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=1000,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "L1_model_dict = L1_model.history\n",
    "plt.clf()\n",
    "\n",
    "acc_values = L1_model_dict['acc'] \n",
    "val_acc_values = L1_model_dict['val_acc']\n",
    "\n",
    "epochs = range(1, len(acc_values) + 1)\n",
    "plt.plot(epochs, acc_values, 'g', label='Training acc L1')\n",
    "plt.plot(epochs, val_acc_values, 'r,', label='Validation acc L1')\n",
    "plt.title('Training & validation accuracy L2 vs regular')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 36us/step\n",
      "1500/1500 [==============================] - 0s 36us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.8211118408521016, 0.7974666666666667]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9607763438224792, 0.744]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about the best we've seen so far, but we were training for quite a while! Let's see if dropout regularization can do even better and/or be more efficient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7500 samples, validate on 1000 samples\n",
      "Epoch 1/200\n",
      "7500/7500 [==============================] - 0s 66us/step - loss: 1.9862 - acc: 0.1425 - val_loss: 1.9397 - val_acc: 0.1680\n",
      "Epoch 2/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.9613 - acc: 0.1604 - val_loss: 1.9307 - val_acc: 0.1900\n",
      "Epoch 3/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.9462 - acc: 0.1647 - val_loss: 1.9244 - val_acc: 0.2190\n",
      "Epoch 4/200\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.9391 - acc: 0.1731 - val_loss: 1.9191 - val_acc: 0.2280\n",
      "Epoch 5/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.9289 - acc: 0.1848 - val_loss: 1.9133 - val_acc: 0.2280\n",
      "Epoch 6/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.9238 - acc: 0.1868 - val_loss: 1.9082 - val_acc: 0.2350\n",
      "Epoch 7/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.9152 - acc: 0.2087 - val_loss: 1.9021 - val_acc: 0.2410\n",
      "Epoch 8/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.9091 - acc: 0.2096 - val_loss: 1.8949 - val_acc: 0.2400\n",
      "Epoch 9/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.9068 - acc: 0.2128 - val_loss: 1.8877 - val_acc: 0.2490\n",
      "Epoch 10/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.8958 - acc: 0.2197 - val_loss: 1.8785 - val_acc: 0.2590\n",
      "Epoch 11/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.8867 - acc: 0.2265 - val_loss: 1.8683 - val_acc: 0.2620\n",
      "Epoch 12/200\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 1.8853 - acc: 0.2305 - val_loss: 1.8563 - val_acc: 0.2690\n",
      "Epoch 13/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.8718 - acc: 0.2397 - val_loss: 1.8430 - val_acc: 0.2820\n",
      "Epoch 14/200\n",
      "7500/7500 [==============================] - 0s 32us/step - loss: 1.8639 - acc: 0.2467 - val_loss: 1.8277 - val_acc: 0.2870\n",
      "Epoch 15/200\n",
      "7500/7500 [==============================] - 0s 31us/step - loss: 1.8496 - acc: 0.2560 - val_loss: 1.8098 - val_acc: 0.3060\n",
      "Epoch 16/200\n",
      "7500/7500 [==============================] - 0s 37us/step - loss: 1.8359 - acc: 0.2647 - val_loss: 1.7901 - val_acc: 0.3240\n",
      "Epoch 17/200\n",
      "7500/7500 [==============================] - 0s 40us/step - loss: 1.8177 - acc: 0.2737 - val_loss: 1.7680 - val_acc: 0.3300\n",
      "Epoch 18/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.8068 - acc: 0.2888 - val_loss: 1.7438 - val_acc: 0.3420\n",
      "Epoch 19/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 1.7889 - acc: 0.2952 - val_loss: 1.7204 - val_acc: 0.3730\n",
      "Epoch 20/200\n",
      "7500/7500 [==============================] - 0s 42us/step - loss: 1.7669 - acc: 0.3097 - val_loss: 1.6937 - val_acc: 0.3870\n",
      "Epoch 21/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.7511 - acc: 0.3088 - val_loss: 1.6686 - val_acc: 0.4040\n",
      "Epoch 22/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.7367 - acc: 0.3153 - val_loss: 1.6412 - val_acc: 0.4120\n",
      "Epoch 23/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.7131 - acc: 0.3319 - val_loss: 1.6151 - val_acc: 0.4410\n",
      "Epoch 24/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6972 - acc: 0.3331 - val_loss: 1.5887 - val_acc: 0.4570\n",
      "Epoch 25/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.6831 - acc: 0.3400 - val_loss: 1.5640 - val_acc: 0.4850\n",
      "Epoch 26/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.6636 - acc: 0.3527 - val_loss: 1.5401 - val_acc: 0.4930\n",
      "Epoch 27/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6478 - acc: 0.3645 - val_loss: 1.5159 - val_acc: 0.5300\n",
      "Epoch 28/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.6258 - acc: 0.3700 - val_loss: 1.4896 - val_acc: 0.5470\n",
      "Epoch 29/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.6033 - acc: 0.3815 - val_loss: 1.4655 - val_acc: 0.5580\n",
      "Epoch 30/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.5806 - acc: 0.3981 - val_loss: 1.4380 - val_acc: 0.5790\n",
      "Epoch 31/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.5641 - acc: 0.4023 - val_loss: 1.4125 - val_acc: 0.5840\n",
      "Epoch 32/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.5546 - acc: 0.4003 - val_loss: 1.3905 - val_acc: 0.6040\n",
      "Epoch 33/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.5352 - acc: 0.4184 - val_loss: 1.3661 - val_acc: 0.6040\n",
      "Epoch 34/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.5186 - acc: 0.4123 - val_loss: 1.3438 - val_acc: 0.6170\n",
      "Epoch 35/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4990 - acc: 0.4261 - val_loss: 1.3205 - val_acc: 0.6300\n",
      "Epoch 36/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4775 - acc: 0.4357 - val_loss: 1.2961 - val_acc: 0.6370\n",
      "Epoch 37/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4677 - acc: 0.4440 - val_loss: 1.2755 - val_acc: 0.6410\n",
      "Epoch 38/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4373 - acc: 0.4564 - val_loss: 1.2531 - val_acc: 0.6530\n",
      "Epoch 39/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4251 - acc: 0.4656 - val_loss: 1.2301 - val_acc: 0.6610\n",
      "Epoch 40/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.4284 - acc: 0.4520 - val_loss: 1.2121 - val_acc: 0.6670\n",
      "Epoch 41/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3899 - acc: 0.4780 - val_loss: 1.1908 - val_acc: 0.6700\n",
      "Epoch 42/200\n",
      "7500/7500 [==============================] - 0s 30us/step - loss: 1.3692 - acc: 0.4859 - val_loss: 1.1707 - val_acc: 0.6800\n",
      "Epoch 43/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3725 - acc: 0.4748 - val_loss: 1.1508 - val_acc: 0.6870\n",
      "Epoch 44/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 1.3523 - acc: 0.4928 - val_loss: 1.1337 - val_acc: 0.6850\n",
      "Epoch 45/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3325 - acc: 0.4937 - val_loss: 1.1157 - val_acc: 0.6910\n",
      "Epoch 46/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.3141 - acc: 0.5057 - val_loss: 1.0968 - val_acc: 0.6960\n",
      "Epoch 47/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.3179 - acc: 0.5077 - val_loss: 1.0820 - val_acc: 0.7010\n",
      "Epoch 48/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.3016 - acc: 0.5069 - val_loss: 1.0684 - val_acc: 0.7010\n",
      "Epoch 49/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.2919 - acc: 0.5163 - val_loss: 1.0518 - val_acc: 0.7090\n",
      "Epoch 50/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2696 - acc: 0.5309 - val_loss: 1.0394 - val_acc: 0.7080\n",
      "Epoch 51/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.2563 - acc: 0.5323 - val_loss: 1.0233 - val_acc: 0.7110\n",
      "Epoch 52/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2561 - acc: 0.5315 - val_loss: 1.0112 - val_acc: 0.7060\n",
      "Epoch 53/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2427 - acc: 0.5368 - val_loss: 0.9992 - val_acc: 0.7090\n",
      "Epoch 54/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2328 - acc: 0.5356 - val_loss: 0.9888 - val_acc: 0.7180\n",
      "Epoch 55/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.2114 - acc: 0.5437 - val_loss: 0.9733 - val_acc: 0.7200\n",
      "Epoch 56/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2052 - acc: 0.5483 - val_loss: 0.9613 - val_acc: 0.7220\n",
      "Epoch 57/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.2099 - acc: 0.5471 - val_loss: 0.9512 - val_acc: 0.7320\n",
      "Epoch 58/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1796 - acc: 0.5568 - val_loss: 0.9414 - val_acc: 0.7300\n",
      "Epoch 59/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1716 - acc: 0.5604 - val_loss: 0.9273 - val_acc: 0.7350\n",
      "Epoch 60/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1569 - acc: 0.5663 - val_loss: 0.9139 - val_acc: 0.7370\n",
      "Epoch 61/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1661 - acc: 0.5635 - val_loss: 0.9093 - val_acc: 0.7280\n",
      "Epoch 62/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1580 - acc: 0.5617 - val_loss: 0.8992 - val_acc: 0.7320\n",
      "Epoch 63/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1465 - acc: 0.5631 - val_loss: 0.8923 - val_acc: 0.7310\n",
      "Epoch 64/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.1362 - acc: 0.5696 - val_loss: 0.8823 - val_acc: 0.7340\n",
      "Epoch 65/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1420 - acc: 0.5732 - val_loss: 0.8745 - val_acc: 0.7340\n",
      "Epoch 66/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 1.1280 - acc: 0.5731 - val_loss: 0.8654 - val_acc: 0.7360\n",
      "Epoch 67/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.1369 - acc: 0.5796 - val_loss: 0.8587 - val_acc: 0.7400\n",
      "Epoch 68/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0978 - acc: 0.5929 - val_loss: 0.8499 - val_acc: 0.7480\n",
      "Epoch 69/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0943 - acc: 0.5859 - val_loss: 0.8447 - val_acc: 0.7490\n",
      "Epoch 70/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0911 - acc: 0.5907 - val_loss: 0.8388 - val_acc: 0.7500\n",
      "Epoch 71/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0979 - acc: 0.5896 - val_loss: 0.8311 - val_acc: 0.7440\n",
      "Epoch 72/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0891 - acc: 0.5969 - val_loss: 0.8250 - val_acc: 0.7460\n",
      "Epoch 73/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0780 - acc: 0.5923 - val_loss: 0.8205 - val_acc: 0.7460\n",
      "Epoch 74/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 1.0688 - acc: 0.6092 - val_loss: 0.8108 - val_acc: 0.7490\n",
      "Epoch 75/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0651 - acc: 0.5947 - val_loss: 0.8035 - val_acc: 0.7490\n",
      "Epoch 76/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0612 - acc: 0.6007 - val_loss: 0.7981 - val_acc: 0.7540\n",
      "Epoch 77/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0636 - acc: 0.6071 - val_loss: 0.7977 - val_acc: 0.7560\n",
      "Epoch 78/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0535 - acc: 0.6128 - val_loss: 0.7914 - val_acc: 0.7510\n",
      "Epoch 79/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0477 - acc: 0.6049 - val_loss: 0.7868 - val_acc: 0.7540\n",
      "Epoch 80/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0295 - acc: 0.6128 - val_loss: 0.7793 - val_acc: 0.7560\n",
      "Epoch 81/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0407 - acc: 0.6015 - val_loss: 0.7765 - val_acc: 0.7530\n",
      "Epoch 82/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0384 - acc: 0.6140 - val_loss: 0.7710 - val_acc: 0.7570\n",
      "Epoch 83/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0299 - acc: 0.6119 - val_loss: 0.7696 - val_acc: 0.7530\n",
      "Epoch 84/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0331 - acc: 0.6088 - val_loss: 0.7645 - val_acc: 0.7590\n",
      "Epoch 85/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0183 - acc: 0.6180 - val_loss: 0.7602 - val_acc: 0.7590\n",
      "Epoch 86/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 1.0115 - acc: 0.6264 - val_loss: 0.7551 - val_acc: 0.7600\n",
      "Epoch 87/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0072 - acc: 0.6275 - val_loss: 0.7514 - val_acc: 0.7610\n",
      "Epoch 88/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0039 - acc: 0.6280 - val_loss: 0.7474 - val_acc: 0.7580\n",
      "Epoch 89/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 1.0017 - acc: 0.6287 - val_loss: 0.7443 - val_acc: 0.7660\n",
      "Epoch 90/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9930 - acc: 0.6281 - val_loss: 0.7426 - val_acc: 0.7590\n",
      "Epoch 91/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9801 - acc: 0.6380 - val_loss: 0.7363 - val_acc: 0.7690\n",
      "Epoch 92/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9852 - acc: 0.6363 - val_loss: 0.7317 - val_acc: 0.7670\n",
      "Epoch 93/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9917 - acc: 0.6316 - val_loss: 0.7298 - val_acc: 0.7650\n",
      "Epoch 94/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9630 - acc: 0.6417 - val_loss: 0.7242 - val_acc: 0.7620\n",
      "Epoch 95/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9688 - acc: 0.6387 - val_loss: 0.7204 - val_acc: 0.7700\n",
      "Epoch 96/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9813 - acc: 0.6281 - val_loss: 0.7179 - val_acc: 0.7680\n",
      "Epoch 97/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9537 - acc: 0.6467 - val_loss: 0.7144 - val_acc: 0.7740\n",
      "Epoch 98/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9536 - acc: 0.6416 - val_loss: 0.7107 - val_acc: 0.7740\n",
      "Epoch 99/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9691 - acc: 0.6401 - val_loss: 0.7114 - val_acc: 0.7730\n",
      "Epoch 100/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9611 - acc: 0.6359 - val_loss: 0.7090 - val_acc: 0.7730\n",
      "Epoch 101/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9472 - acc: 0.6488 - val_loss: 0.7023 - val_acc: 0.7750\n",
      "Epoch 102/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9366 - acc: 0.6537 - val_loss: 0.6977 - val_acc: 0.7740\n",
      "Epoch 103/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9311 - acc: 0.6539 - val_loss: 0.6971 - val_acc: 0.7780\n",
      "Epoch 104/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9405 - acc: 0.6560 - val_loss: 0.6941 - val_acc: 0.7780\n",
      "Epoch 105/200\n",
      "7500/7500 [==============================] - 0s 29us/step - loss: 0.9295 - acc: 0.6564 - val_loss: 0.6931 - val_acc: 0.7710\n",
      "Epoch 106/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.9295 - acc: 0.6515 - val_loss: 0.6906 - val_acc: 0.7760\n",
      "Epoch 107/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9204 - acc: 0.6512 - val_loss: 0.6870 - val_acc: 0.7720\n",
      "Epoch 108/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9219 - acc: 0.6585 - val_loss: 0.6877 - val_acc: 0.7770\n",
      "Epoch 109/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9379 - acc: 0.6524 - val_loss: 0.6855 - val_acc: 0.7790\n",
      "Epoch 110/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9193 - acc: 0.6515 - val_loss: 0.6804 - val_acc: 0.7750\n",
      "Epoch 111/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9183 - acc: 0.6564 - val_loss: 0.6773 - val_acc: 0.7740\n",
      "Epoch 112/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9057 - acc: 0.6619 - val_loss: 0.6782 - val_acc: 0.7810\n",
      "Epoch 113/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9059 - acc: 0.6655 - val_loss: 0.6738 - val_acc: 0.7820\n",
      "Epoch 114/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9032 - acc: 0.6655 - val_loss: 0.6724 - val_acc: 0.7800\n",
      "Epoch 115/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8997 - acc: 0.6648 - val_loss: 0.6684 - val_acc: 0.7780\n",
      "Epoch 116/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.9027 - acc: 0.6628 - val_loss: 0.6714 - val_acc: 0.7810\n",
      "Epoch 117/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8979 - acc: 0.6691 - val_loss: 0.6686 - val_acc: 0.7800\n",
      "Epoch 118/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9041 - acc: 0.6615 - val_loss: 0.6667 - val_acc: 0.7840\n",
      "Epoch 119/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.9065 - acc: 0.6651 - val_loss: 0.6664 - val_acc: 0.7850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 120/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8857 - acc: 0.6667 - val_loss: 0.6648 - val_acc: 0.7800\n",
      "Epoch 121/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8988 - acc: 0.6649 - val_loss: 0.6637 - val_acc: 0.7810\n",
      "Epoch 122/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8996 - acc: 0.6639 - val_loss: 0.6595 - val_acc: 0.7820\n",
      "Epoch 123/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8696 - acc: 0.6773 - val_loss: 0.6569 - val_acc: 0.7790\n",
      "Epoch 124/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8790 - acc: 0.6708 - val_loss: 0.6531 - val_acc: 0.7880\n",
      "Epoch 125/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8810 - acc: 0.6696 - val_loss: 0.6515 - val_acc: 0.7890\n",
      "Epoch 126/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8687 - acc: 0.6763 - val_loss: 0.6507 - val_acc: 0.7830\n",
      "Epoch 127/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8758 - acc: 0.6800 - val_loss: 0.6501 - val_acc: 0.7870\n",
      "Epoch 128/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8783 - acc: 0.6687 - val_loss: 0.6487 - val_acc: 0.7860\n",
      "Epoch 129/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8679 - acc: 0.6764 - val_loss: 0.6471 - val_acc: 0.7810\n",
      "Epoch 130/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8694 - acc: 0.6772 - val_loss: 0.6454 - val_acc: 0.7820\n",
      "Epoch 131/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8603 - acc: 0.6768 - val_loss: 0.6451 - val_acc: 0.7820\n",
      "Epoch 132/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8533 - acc: 0.6856 - val_loss: 0.6444 - val_acc: 0.7780\n",
      "Epoch 133/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8466 - acc: 0.6844 - val_loss: 0.6391 - val_acc: 0.7830\n",
      "Epoch 134/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8637 - acc: 0.6801 - val_loss: 0.6387 - val_acc: 0.7830\n",
      "Epoch 135/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8616 - acc: 0.6788 - val_loss: 0.6420 - val_acc: 0.7780\n",
      "Epoch 136/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8349 - acc: 0.6900 - val_loss: 0.6358 - val_acc: 0.7800\n",
      "Epoch 137/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8478 - acc: 0.6891 - val_loss: 0.6356 - val_acc: 0.7750\n",
      "Epoch 138/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8441 - acc: 0.6839 - val_loss: 0.6335 - val_acc: 0.7860\n",
      "Epoch 139/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8434 - acc: 0.6855 - val_loss: 0.6324 - val_acc: 0.7840\n",
      "Epoch 140/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8349 - acc: 0.6875 - val_loss: 0.6309 - val_acc: 0.7880\n",
      "Epoch 141/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8356 - acc: 0.6896 - val_loss: 0.6296 - val_acc: 0.7810\n",
      "Epoch 142/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8357 - acc: 0.6928 - val_loss: 0.6276 - val_acc: 0.7840\n",
      "Epoch 143/200\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8302 - acc: 0.6852 - val_loss: 0.6304 - val_acc: 0.7870\n",
      "Epoch 144/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8360 - acc: 0.6844 - val_loss: 0.6282 - val_acc: 0.7860\n",
      "Epoch 145/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8317 - acc: 0.6876 - val_loss: 0.6244 - val_acc: 0.7890\n",
      "Epoch 146/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8256 - acc: 0.6925 - val_loss: 0.6254 - val_acc: 0.7780\n",
      "Epoch 147/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8335 - acc: 0.6899 - val_loss: 0.6247 - val_acc: 0.7810\n",
      "Epoch 148/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8325 - acc: 0.6929 - val_loss: 0.6223 - val_acc: 0.7900\n",
      "Epoch 149/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8171 - acc: 0.7027 - val_loss: 0.6218 - val_acc: 0.7850\n",
      "Epoch 150/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8122 - acc: 0.6876 - val_loss: 0.6207 - val_acc: 0.7830\n",
      "Epoch 151/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8217 - acc: 0.6879 - val_loss: 0.6190 - val_acc: 0.7870\n",
      "Epoch 152/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8251 - acc: 0.6895 - val_loss: 0.6189 - val_acc: 0.7830\n",
      "Epoch 153/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.8037 - acc: 0.6987 - val_loss: 0.6172 - val_acc: 0.7850\n",
      "Epoch 154/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8076 - acc: 0.6983 - val_loss: 0.6145 - val_acc: 0.7880\n",
      "Epoch 155/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8091 - acc: 0.7019 - val_loss: 0.6136 - val_acc: 0.7840\n",
      "Epoch 156/200\n",
      "7500/7500 [==============================] - 0s 43us/step - loss: 0.8021 - acc: 0.6995 - val_loss: 0.6131 - val_acc: 0.7840\n",
      "Epoch 157/200\n",
      "7500/7500 [==============================] - 0s 41us/step - loss: 0.7984 - acc: 0.6992 - val_loss: 0.6117 - val_acc: 0.7900\n",
      "Epoch 158/200\n",
      "7500/7500 [==============================] - 0s 33us/step - loss: 0.8022 - acc: 0.6983 - val_loss: 0.6145 - val_acc: 0.7870\n",
      "Epoch 159/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.8055 - acc: 0.6959 - val_loss: 0.6121 - val_acc: 0.7870\n",
      "Epoch 160/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.8151 - acc: 0.6999 - val_loss: 0.6125 - val_acc: 0.7900\n",
      "Epoch 161/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7985 - acc: 0.7033 - val_loss: 0.6102 - val_acc: 0.7840\n",
      "Epoch 162/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.8005 - acc: 0.7007 - val_loss: 0.6093 - val_acc: 0.7910\n",
      "Epoch 163/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7935 - acc: 0.6993 - val_loss: 0.6062 - val_acc: 0.7850\n",
      "Epoch 164/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7761 - acc: 0.7111 - val_loss: 0.6064 - val_acc: 0.7890\n",
      "Epoch 165/200\n",
      "7500/7500 [==============================] - 0s 27us/step - loss: 0.7929 - acc: 0.6983 - val_loss: 0.6050 - val_acc: 0.7810\n",
      "Epoch 166/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7857 - acc: 0.6972 - val_loss: 0.6038 - val_acc: 0.7890\n",
      "Epoch 167/200\n",
      "7500/7500 [==============================] - 0s 38us/step - loss: 0.7910 - acc: 0.7055 - val_loss: 0.6041 - val_acc: 0.7910\n",
      "Epoch 168/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7907 - acc: 0.7052 - val_loss: 0.6035 - val_acc: 0.7940\n",
      "Epoch 169/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7869 - acc: 0.7036 - val_loss: 0.6034 - val_acc: 0.7910\n",
      "Epoch 170/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7820 - acc: 0.7067 - val_loss: 0.6015 - val_acc: 0.7920\n",
      "Epoch 171/200\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7659 - acc: 0.7103 - val_loss: 0.6006 - val_acc: 0.7890\n",
      "Epoch 172/200\n",
      "7500/7500 [==============================] - 0s 34us/step - loss: 0.7796 - acc: 0.7131 - val_loss: 0.5993 - val_acc: 0.7830\n",
      "Epoch 173/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7780 - acc: 0.7055 - val_loss: 0.5986 - val_acc: 0.7860\n",
      "Epoch 174/200\n",
      "7500/7500 [==============================] - 0s 28us/step - loss: 0.7712 - acc: 0.7159 - val_loss: 0.5963 - val_acc: 0.7890\n",
      "Epoch 175/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7716 - acc: 0.7088 - val_loss: 0.5987 - val_acc: 0.7890\n",
      "Epoch 176/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7638 - acc: 0.7095 - val_loss: 0.5966 - val_acc: 0.7850\n",
      "Epoch 177/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7775 - acc: 0.7115 - val_loss: 0.5961 - val_acc: 0.7910\n",
      "Epoch 178/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7736 - acc: 0.7057 - val_loss: 0.5951 - val_acc: 0.7830\n",
      "Epoch 179/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7705 - acc: 0.7141 - val_loss: 0.5938 - val_acc: 0.7870\n",
      "Epoch 180/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7679 - acc: 0.7080 - val_loss: 0.5936 - val_acc: 0.7880\n",
      "Epoch 181/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7697 - acc: 0.7141 - val_loss: 0.5953 - val_acc: 0.7900\n",
      "Epoch 182/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7665 - acc: 0.7160 - val_loss: 0.5931 - val_acc: 0.7850\n",
      "Epoch 183/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7665 - acc: 0.7160 - val_loss: 0.5942 - val_acc: 0.7860\n",
      "Epoch 184/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7621 - acc: 0.7189 - val_loss: 0.5935 - val_acc: 0.7870\n",
      "Epoch 185/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7596 - acc: 0.7155 - val_loss: 0.5930 - val_acc: 0.7860\n",
      "Epoch 186/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7481 - acc: 0.7228 - val_loss: 0.5922 - val_acc: 0.7900\n",
      "Epoch 187/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7639 - acc: 0.7159 - val_loss: 0.5916 - val_acc: 0.7880\n",
      "Epoch 188/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7599 - acc: 0.7117 - val_loss: 0.5909 - val_acc: 0.7880\n",
      "Epoch 189/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7520 - acc: 0.7177 - val_loss: 0.5884 - val_acc: 0.7950\n",
      "Epoch 190/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7618 - acc: 0.7120 - val_loss: 0.5899 - val_acc: 0.7900\n",
      "Epoch 191/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7377 - acc: 0.7205 - val_loss: 0.5896 - val_acc: 0.7850\n",
      "Epoch 192/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7409 - acc: 0.7212 - val_loss: 0.5879 - val_acc: 0.7880\n",
      "Epoch 193/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7520 - acc: 0.7227 - val_loss: 0.5865 - val_acc: 0.7960\n",
      "Epoch 194/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7455 - acc: 0.7207 - val_loss: 0.5864 - val_acc: 0.7920\n",
      "Epoch 195/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7548 - acc: 0.7184 - val_loss: 0.5877 - val_acc: 0.7910\n",
      "Epoch 196/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7423 - acc: 0.7179 - val_loss: 0.5831 - val_acc: 0.7940\n",
      "Epoch 197/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7377 - acc: 0.7204 - val_loss: 0.5841 - val_acc: 0.7950\n",
      "Epoch 198/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7420 - acc: 0.7285 - val_loss: 0.5846 - val_acc: 0.7930\n",
      "Epoch 199/200\n",
      "7500/7500 [==============================] - 0s 25us/step - loss: 0.7422 - acc: 0.7208 - val_loss: 0.5826 - val_acc: 0.7950\n",
      "Epoch 200/200\n",
      "7500/7500 [==============================] - 0s 26us/step - loss: 0.7506 - acc: 0.7227 - val_loss: 0.5843 - val_acc: 0.7950\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dropout(0.3, input_shape=(2000,)))\n",
    "model.add(layers.Dense(50, activation='relu')) #2 hidden layers\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dropout(0.3))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "dropout_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=200,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7500/7500 [==============================] - 0s 38us/step\n",
      "1500/1500 [==============================] - 0s 38us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4582416109402974, 0.8493333333651225]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6518971514701843, 0.7606666671435038]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see here that the validation performance has improved again! the variance did become higher again compared to L1-regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigger Data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the lecture, one of the solutions to high variance was just getting more data. We actually *have* more data, but took a subset of 10,000 units before. Let's now quadruple our data set, and see what happens. Note that we are really just lucky here, and getting more data isn't always possible, but this is a useful exercise in order to understand the power of big data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Bank_complaints.csv')\n",
    "random.seed(123)\n",
    "df = df.sample(40000)\n",
    "df.index = range(40000)\n",
    "product = df[\"Product\"]\n",
    "complaints = df[\"Consumer complaint narrative\"]\n",
    "\n",
    "#one-hot encoding of the complaints\n",
    "tokenizer = Tokenizer(num_words=2000)\n",
    "tokenizer.fit_on_texts(complaints)\n",
    "sequences = tokenizer.texts_to_sequences(complaints)\n",
    "one_hot_results= tokenizer.texts_to_matrix(complaints, mode='binary')\n",
    "word_index = tokenizer.word_index\n",
    "np.shape(one_hot_results)\n",
    "\n",
    "#one-hot encoding of products\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(product)\n",
    "list(le.classes_)\n",
    "product_cat = le.transform(product) \n",
    "product_onehot = to_categorical(product_cat)\n",
    "\n",
    "# train test split\n",
    "test_index = random.sample(range(1,40000), 4000)\n",
    "test = one_hot_results[test_index]\n",
    "train = np.delete(one_hot_results, test_index, 0)\n",
    "label_test = product_onehot[test_index]\n",
    "label_train = np.delete(product_onehot, test_index, 0)\n",
    "\n",
    "#Validation set\n",
    "random.seed(123)\n",
    "val = train[:3000]\n",
    "train_final = train[3000:]\n",
    "label_val = label_train[:3000]\n",
    "label_train_final = label_train[3000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 33000 samples, validate on 3000 samples\n",
      "Epoch 1/120\n",
      "33000/33000 [==============================] - 1s 23us/step - loss: 1.9226 - acc: 0.1845 - val_loss: 1.8913 - val_acc: 0.2377\n",
      "Epoch 2/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 1.8440 - acc: 0.2671 - val_loss: 1.7904 - val_acc: 0.3143\n",
      "Epoch 3/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 1.7170 - acc: 0.3737 - val_loss: 1.6300 - val_acc: 0.4463\n",
      "Epoch 4/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 1.5355 - acc: 0.5047 - val_loss: 1.4297 - val_acc: 0.5647\n",
      "Epoch 5/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 1.3289 - acc: 0.6037 - val_loss: 1.2280 - val_acc: 0.6220\n",
      "Epoch 6/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 1.1368 - acc: 0.6585 - val_loss: 1.0611 - val_acc: 0.6733\n",
      "Epoch 7/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.9864 - acc: 0.6908 - val_loss: 0.9376 - val_acc: 0.6933\n",
      "Epoch 8/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.8802 - acc: 0.7125 - val_loss: 0.8558 - val_acc: 0.7057\n",
      "Epoch 9/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.8071 - acc: 0.7274 - val_loss: 0.8004 - val_acc: 0.7223\n",
      "Epoch 10/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.7548 - acc: 0.7389 - val_loss: 0.7600 - val_acc: 0.7293\n",
      "Epoch 11/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.7163 - acc: 0.7478 - val_loss: 0.7302 - val_acc: 0.7330\n",
      "Epoch 12/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.6864 - acc: 0.7544 - val_loss: 0.7077 - val_acc: 0.7407\n",
      "Epoch 13/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.6622 - acc: 0.7617 - val_loss: 0.6901 - val_acc: 0.7470\n",
      "Epoch 14/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.6423 - acc: 0.7676 - val_loss: 0.6764 - val_acc: 0.7520\n",
      "Epoch 15/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.6251 - acc: 0.7728 - val_loss: 0.6644 - val_acc: 0.7550\n",
      "Epoch 16/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.6103 - acc: 0.7781 - val_loss: 0.6560 - val_acc: 0.7590\n",
      "Epoch 17/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5975 - acc: 0.7811 - val_loss: 0.6486 - val_acc: 0.7570\n",
      "Epoch 18/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.5860 - acc: 0.7853 - val_loss: 0.6394 - val_acc: 0.7633\n",
      "Epoch 19/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.5754 - acc: 0.7887 - val_loss: 0.6324 - val_acc: 0.7643\n",
      "Epoch 20/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.5660 - acc: 0.7926 - val_loss: 0.6286 - val_acc: 0.7643\n",
      "Epoch 21/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.5571 - acc: 0.7944 - val_loss: 0.6212 - val_acc: 0.7690\n",
      "Epoch 22/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.5488 - acc: 0.7985 - val_loss: 0.6178 - val_acc: 0.7720\n",
      "Epoch 23/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.5412 - acc: 0.8013 - val_loss: 0.6154 - val_acc: 0.7737\n",
      "Epoch 24/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.5339 - acc: 0.8048 - val_loss: 0.6095 - val_acc: 0.7747\n",
      "Epoch 25/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.5272 - acc: 0.8063 - val_loss: 0.6080 - val_acc: 0.7727\n",
      "Epoch 26/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.5210 - acc: 0.8094 - val_loss: 0.6030 - val_acc: 0.7730\n",
      "Epoch 27/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.5148 - acc: 0.8118 - val_loss: 0.5994 - val_acc: 0.7773\n",
      "Epoch 28/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.5088 - acc: 0.8146 - val_loss: 0.5973 - val_acc: 0.7773\n",
      "Epoch 29/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.5035 - acc: 0.8172 - val_loss: 0.5955 - val_acc: 0.7777\n",
      "Epoch 30/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4982 - acc: 0.8190 - val_loss: 0.5917 - val_acc: 0.7780\n",
      "Epoch 31/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4927 - acc: 0.8205 - val_loss: 0.5915 - val_acc: 0.7797\n",
      "Epoch 32/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4881 - acc: 0.8224 - val_loss: 0.5876 - val_acc: 0.7810\n",
      "Epoch 33/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4836 - acc: 0.8243 - val_loss: 0.5850 - val_acc: 0.7820\n",
      "Epoch 34/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4790 - acc: 0.8263 - val_loss: 0.5845 - val_acc: 0.7833\n",
      "Epoch 35/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4745 - acc: 0.8288 - val_loss: 0.5849 - val_acc: 0.7823\n",
      "Epoch 36/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4704 - acc: 0.8296 - val_loss: 0.5843 - val_acc: 0.7857\n",
      "Epoch 37/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4664 - acc: 0.8312 - val_loss: 0.5802 - val_acc: 0.7887\n",
      "Epoch 38/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4623 - acc: 0.8328 - val_loss: 0.5788 - val_acc: 0.7887\n",
      "Epoch 39/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4586 - acc: 0.8348 - val_loss: 0.5756 - val_acc: 0.7897\n",
      "Epoch 40/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4549 - acc: 0.8359 - val_loss: 0.5775 - val_acc: 0.7890\n",
      "Epoch 41/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4510 - acc: 0.8378 - val_loss: 0.5782 - val_acc: 0.7870\n",
      "Epoch 42/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4475 - acc: 0.8388 - val_loss: 0.5748 - val_acc: 0.7940\n",
      "Epoch 43/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4441 - acc: 0.8398 - val_loss: 0.5786 - val_acc: 0.7910\n",
      "Epoch 44/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4412 - acc: 0.8411 - val_loss: 0.5764 - val_acc: 0.7923\n",
      "Epoch 45/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4376 - acc: 0.8429 - val_loss: 0.5767 - val_acc: 0.7900\n",
      "Epoch 46/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4347 - acc: 0.8446 - val_loss: 0.5754 - val_acc: 0.7920\n",
      "Epoch 47/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4320 - acc: 0.8454 - val_loss: 0.5716 - val_acc: 0.7957\n",
      "Epoch 48/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4289 - acc: 0.8463 - val_loss: 0.5727 - val_acc: 0.7923\n",
      "Epoch 49/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4258 - acc: 0.8479 - val_loss: 0.5713 - val_acc: 0.7937\n",
      "Epoch 50/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4232 - acc: 0.8487 - val_loss: 0.5712 - val_acc: 0.7943\n",
      "Epoch 51/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4202 - acc: 0.8508 - val_loss: 0.5693 - val_acc: 0.7953\n",
      "Epoch 52/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4171 - acc: 0.8512 - val_loss: 0.5729 - val_acc: 0.7937\n",
      "Epoch 53/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4148 - acc: 0.8528 - val_loss: 0.5699 - val_acc: 0.7967\n",
      "Epoch 54/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4122 - acc: 0.8536 - val_loss: 0.5706 - val_acc: 0.7947\n",
      "Epoch 55/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4098 - acc: 0.8544 - val_loss: 0.5737 - val_acc: 0.7930\n",
      "Epoch 56/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4076 - acc: 0.8551 - val_loss: 0.5681 - val_acc: 0.7983\n",
      "Epoch 57/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4050 - acc: 0.8550 - val_loss: 0.5695 - val_acc: 0.7943\n",
      "Epoch 58/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.4024 - acc: 0.8583 - val_loss: 0.5729 - val_acc: 0.7967\n",
      "Epoch 59/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.4003 - acc: 0.8573 - val_loss: 0.5744 - val_acc: 0.7917\n",
      "Epoch 60/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3981 - acc: 0.8589 - val_loss: 0.5728 - val_acc: 0.7940\n",
      "Epoch 61/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3961 - acc: 0.8589 - val_loss: 0.5740 - val_acc: 0.7943\n",
      "Epoch 62/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3939 - acc: 0.8602 - val_loss: 0.5698 - val_acc: 0.7970\n",
      "Epoch 63/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3917 - acc: 0.8613 - val_loss: 0.5721 - val_acc: 0.7963\n",
      "Epoch 64/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3895 - acc: 0.8616 - val_loss: 0.5731 - val_acc: 0.7943\n",
      "Epoch 65/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3874 - acc: 0.8624 - val_loss: 0.5736 - val_acc: 0.7943\n",
      "Epoch 66/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3855 - acc: 0.8632 - val_loss: 0.5701 - val_acc: 0.7990\n",
      "Epoch 67/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3835 - acc: 0.8645 - val_loss: 0.5719 - val_acc: 0.7973\n",
      "Epoch 68/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3814 - acc: 0.8654 - val_loss: 0.5734 - val_acc: 0.7953\n",
      "Epoch 69/120\n",
      "33000/33000 [==============================] - 1s 19us/step - loss: 0.3794 - acc: 0.8662 - val_loss: 0.5724 - val_acc: 0.7993\n",
      "Epoch 70/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3778 - acc: 0.8673 - val_loss: 0.5727 - val_acc: 0.7980\n",
      "Epoch 71/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3757 - acc: 0.8676 - val_loss: 0.5732 - val_acc: 0.7967\n",
      "Epoch 72/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3738 - acc: 0.8683 - val_loss: 0.5742 - val_acc: 0.7943\n",
      "Epoch 73/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3723 - acc: 0.8684 - val_loss: 0.5767 - val_acc: 0.7990\n",
      "Epoch 74/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3703 - acc: 0.8695 - val_loss: 0.5778 - val_acc: 0.7943\n",
      "Epoch 75/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3683 - acc: 0.8705 - val_loss: 0.5768 - val_acc: 0.7960\n",
      "Epoch 76/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3668 - acc: 0.8710 - val_loss: 0.5755 - val_acc: 0.7977\n",
      "Epoch 77/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3650 - acc: 0.8718 - val_loss: 0.5772 - val_acc: 0.7970\n",
      "Epoch 78/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3630 - acc: 0.8729 - val_loss: 0.5810 - val_acc: 0.7957\n",
      "Epoch 79/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3614 - acc: 0.8725 - val_loss: 0.5812 - val_acc: 0.7973\n",
      "Epoch 80/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3600 - acc: 0.8723 - val_loss: 0.5781 - val_acc: 0.7957\n",
      "Epoch 81/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3583 - acc: 0.8747 - val_loss: 0.5819 - val_acc: 0.7977\n",
      "Epoch 82/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3569 - acc: 0.8748 - val_loss: 0.5826 - val_acc: 0.7967\n",
      "Epoch 83/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3555 - acc: 0.8745 - val_loss: 0.5801 - val_acc: 0.7970\n",
      "Epoch 84/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3542 - acc: 0.8752 - val_loss: 0.5805 - val_acc: 0.7973\n",
      "Epoch 85/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3523 - acc: 0.8763 - val_loss: 0.5816 - val_acc: 0.7930\n",
      "Epoch 86/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3506 - acc: 0.8769 - val_loss: 0.5826 - val_acc: 0.7973\n",
      "Epoch 87/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3492 - acc: 0.8773 - val_loss: 0.5861 - val_acc: 0.7990\n",
      "Epoch 88/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3479 - acc: 0.8783 - val_loss: 0.5870 - val_acc: 0.7917\n",
      "Epoch 89/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3464 - acc: 0.8789 - val_loss: 0.5858 - val_acc: 0.7957\n",
      "Epoch 90/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3448 - acc: 0.8795 - val_loss: 0.5876 - val_acc: 0.7997\n",
      "Epoch 91/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3434 - acc: 0.8803 - val_loss: 0.5875 - val_acc: 0.7950\n",
      "Epoch 92/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3420 - acc: 0.8801 - val_loss: 0.5898 - val_acc: 0.7960\n",
      "Epoch 93/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3410 - acc: 0.8808 - val_loss: 0.5871 - val_acc: 0.7960\n",
      "Epoch 94/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3393 - acc: 0.8819 - val_loss: 0.5887 - val_acc: 0.7960\n",
      "Epoch 95/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3378 - acc: 0.8816 - val_loss: 0.5919 - val_acc: 0.7980\n",
      "Epoch 96/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3365 - acc: 0.8827 - val_loss: 0.5922 - val_acc: 0.8013\n",
      "Epoch 97/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3352 - acc: 0.8832 - val_loss: 0.5916 - val_acc: 0.7987\n",
      "Epoch 98/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3338 - acc: 0.8836 - val_loss: 0.5914 - val_acc: 0.8007\n",
      "Epoch 99/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3327 - acc: 0.8836 - val_loss: 0.5920 - val_acc: 0.8000\n",
      "Epoch 100/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3310 - acc: 0.8846 - val_loss: 0.5942 - val_acc: 0.7943\n",
      "Epoch 101/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3300 - acc: 0.8846 - val_loss: 0.5932 - val_acc: 0.7963\n",
      "Epoch 102/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3289 - acc: 0.8858 - val_loss: 0.5940 - val_acc: 0.8000\n",
      "Epoch 103/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3276 - acc: 0.8853 - val_loss: 0.5974 - val_acc: 0.7980\n",
      "Epoch 104/120\n",
      "33000/33000 [==============================] - 1s 16us/step - loss: 0.3262 - acc: 0.8865 - val_loss: 0.6002 - val_acc: 0.7973\n",
      "Epoch 105/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3248 - acc: 0.8868 - val_loss: 0.5994 - val_acc: 0.7980\n",
      "Epoch 106/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3238 - acc: 0.8870 - val_loss: 0.5980 - val_acc: 0.7993\n",
      "Epoch 107/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3227 - acc: 0.8878 - val_loss: 0.6001 - val_acc: 0.7990\n",
      "Epoch 108/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3215 - acc: 0.8879 - val_loss: 0.6040 - val_acc: 0.7957\n",
      "Epoch 109/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3203 - acc: 0.8878 - val_loss: 0.6088 - val_acc: 0.7977\n",
      "Epoch 110/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3190 - acc: 0.8882 - val_loss: 0.6040 - val_acc: 0.7957\n",
      "Epoch 111/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3183 - acc: 0.8890 - val_loss: 0.6033 - val_acc: 0.7997\n",
      "Epoch 112/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3172 - acc: 0.8894 - val_loss: 0.6040 - val_acc: 0.7967\n",
      "Epoch 113/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3157 - acc: 0.8897 - val_loss: 0.6051 - val_acc: 0.7990\n",
      "Epoch 114/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3143 - acc: 0.8905 - val_loss: 0.6110 - val_acc: 0.7947\n",
      "Epoch 115/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3134 - acc: 0.8909 - val_loss: 0.6085 - val_acc: 0.8023\n",
      "Epoch 116/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3119 - acc: 0.8909 - val_loss: 0.6172 - val_acc: 0.7973\n",
      "Epoch 117/120\n",
      "33000/33000 [==============================] - 1s 18us/step - loss: 0.3112 - acc: 0.8924 - val_loss: 0.6094 - val_acc: 0.7973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3101 - acc: 0.8917 - val_loss: 0.6093 - val_acc: 0.8013\n",
      "Epoch 119/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3086 - acc: 0.8925 - val_loss: 0.6155 - val_acc: 0.7993\n",
      "Epoch 120/120\n",
      "33000/33000 [==============================] - 1s 17us/step - loss: 0.3077 - acc: 0.8925 - val_loss: 0.6165 - val_acc: 0.7987\n"
     ]
    }
   ],
   "source": [
    "random.seed(123)\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(50, activation='relu', input_shape=(2000,))) #2 hidden layers\n",
    "model.add(layers.Dense(25, activation='relu'))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "moredata_model = model.fit(train_final,\n",
    "                    label_train_final,\n",
    "                    epochs=120,\n",
    "                    batch_size=256,\n",
    "                    validation_data=(val, label_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33000/33000 [==============================] - 1s 21us/step\n",
      "4000/4000 [==============================] - 0s 22us/step\n"
     ]
    }
   ],
   "source": [
    "results_train = model.evaluate(train_final, label_train_final)\n",
    "results_test = model.evaluate(test, label_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4582416109402974, 0.8493333333651225]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6518971514701843, 0.7606666671435038]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the same amount of epochs, we were able to get a fairly similar validation accuracy of 89.67 (compared to 88.55 in obtained in the first model in this lab). Our test set accuracy went up from 75.8 to a staggering 80.225% though, without any other regularization technique. You can still consider early stopping, L1, L2 and dropout here. It's clear that having more data has a strong impact on model performance!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "* https://github.com/susanli2016/Machine-Learning-with-Python/blob/master/Consumer_complaints.ipynb\n",
    "* https://machinelearningmastery.com/dropout-regularization-deep-learning-models-keras/\n",
    "* https://catalog.data.gov/dataset/consumer-complaint-database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary  \n",
    "\n",
    "In this lesson, we not only built an initial deep-learning model, we then used a validation set to tune our model using various types of regularization. From here, we'll continue to describe more practice and theory regarding tuning and optimizing deep-learning networks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
